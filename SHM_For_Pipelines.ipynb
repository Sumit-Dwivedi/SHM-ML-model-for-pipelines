{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyi/u5N6No5V+XAeNogwNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sumit-Dwivedi/SHM-ML-model-for-pipelines/blob/main/SHM_For_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Added for visualization\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler # Keep StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define category mapping for clarity and visualization\n",
        "DAMAGE_CATEGORIES = {\n",
        "    0: 'Normal',\n",
        "    1: 'Micro',\n",
        "    2: 'Minor',\n",
        "    3: 'Major'\n",
        "}\n",
        "\n",
        "def load_frequency_data(file_path, selected_sheets):\n",
        "    \"\"\"\n",
        "    Load data from multiple sheets in a single Excel file.\n",
        "    \"\"\"\n",
        "    frequency_data = {}\n",
        "    try:\n",
        "        xls = pd.ExcelFile(file_path)\n",
        "        # Assume sheet names directly map or adjust mapping if needed\n",
        "        frequency_mapping = {name: name for name in xls.sheet_names if name in selected_sheets}\n",
        "\n",
        "        for sheet_name in selected_sheets:\n",
        "            if sheet_name in frequency_mapping:\n",
        "                df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "                freq = frequency_mapping[sheet_name]\n",
        "                print(f\"Loaded {freq} data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "                frequency_data[freq] = df\n",
        "            else:\n",
        "                 print(f\"Warning: Selected sheet '{sheet_name}' not found in the Excel file.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "    return frequency_data\n",
        "\n",
        "def add_acquisition_info(dataframes, acquisition_ranges):\n",
        "    \"\"\"\n",
        "    Add acquisition number and damage category label (0-3) to dataframes.\n",
        "\n",
        "    Categorization Logic (based on acquisition number 'i'):\n",
        "    - Normal (0): i < damage_start\n",
        "    - Micro (1): damage_start <= i <= micro_boundary\n",
        "    - Minor (2): micro_boundary < i <= minor_boundary\n",
        "    - Major (3): i > minor_boundary\n",
        "    \"\"\"\n",
        "    updated_dfs = {}\n",
        "\n",
        "    # --- Define category boundaries based on acquisition_ranges ---\n",
        "    damage_start = acquisition_ranges['damage_start']\n",
        "    acq_end = acquisition_ranges['end']\n",
        "    total_damaged_acqs = acq_end - damage_start + 1\n",
        "\n",
        "    if total_damaged_acqs <= 0:\n",
        "        print(\"Warning: No damaged acquisitions based on provided range.\")\n",
        "        micro_boundary = damage_start - 1 # No micro/minor/major range\n",
        "        minor_boundary = damage_start - 1\n",
        "    elif total_damaged_acqs < 3:\n",
        "         print(\"Warning: Very few damaged acquisitions (<3), assigning all to 'Micro'.\")\n",
        "         micro_boundary = acq_end # All damaged fall into micro category (up to end)\n",
        "         minor_boundary = acq_end # No distinct minor/major range\n",
        "    else:\n",
        "        # Divide damaged range into three roughly equal parts using floor division\n",
        "        # Boundary is the *last* acquisition number in the category\n",
        "        acqs_per_cat = total_damaged_acqs // 3\n",
        "        micro_boundary = damage_start + acqs_per_cat - 1\n",
        "        minor_boundary = micro_boundary + acqs_per_cat\n",
        "        # Major starts after minor_boundary\n",
        "\n",
        "    print(f\"Categorization Ranges: Normal(<{damage_start}), Micro({damage_start}-{micro_boundary}), Minor({micro_boundary+1}-{minor_boundary}), Major(>{minor_boundary})\")\n",
        "    # --- End Category Definition ---\n",
        "\n",
        "\n",
        "    for freq, df in dataframes.items():\n",
        "        # Assuming each acquisition might have slightly different numbers of measurements\n",
        "        # Estimate based on total rows and total acquisitions for array pre-allocation\n",
        "        total_rows_in_df = df.shape[0]\n",
        "        total_acquisitions_in_range = acquisition_ranges['end'] - acquisition_ranges['start'] + 1\n",
        "        if total_acquisitions_in_range <= 0:\n",
        "            print(f\"Error for {freq}: Invalid acquisition range.\")\n",
        "            continue\n",
        "        # Use floating point division for average, then ceiling for initial estimate\n",
        "        avg_measurements = total_rows_in_df / total_acquisitions_in_range\n",
        "        est_measurements_per_acq = int(np.ceil(avg_measurements)) if avg_measurements > 0 else 1\n",
        "        print(f\"Estimating ~{est_measurements_per_acq} measurements per acquisition for {freq}.\")\n",
        "\n",
        "\n",
        "        all_acquisitions = []\n",
        "        # *** MODIFICATION START: Use all_damage_categories ***\n",
        "        all_damage_categories = []\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "        acq_row_counts = {} # Track actual rows per acq if needed later\n",
        "\n",
        "        current_row_index = 0\n",
        "        for i in range(acquisition_ranges['start'], acquisition_ranges['end'] + 1):\n",
        "\n",
        "            # --- Determine Damage Category for acquisition 'i' ---\n",
        "            if i < damage_start:\n",
        "                category = 0  # Normal\n",
        "            elif i <= micro_boundary:\n",
        "                category = 1  # Micro\n",
        "            elif i <= minor_boundary:\n",
        "                category = 2  # Minor\n",
        "            else:\n",
        "                category = 3  # Major\n",
        "            # --- End Damage Category Determination ---\n",
        "\n",
        "            # Find rows corresponding to the current acquisition number 'i'\n",
        "            # This assumes the original Excel data might be ordered or requires search\n",
        "            # For simplicity based on previous runs, assume contiguous blocks based on estimated size.\n",
        "            # THIS IS A POTENTIAL SOURCE OF ERROR if measurements_per_acquisition varies significantly.\n",
        "            # A more robust way would be to pre-process the Excel to *have* an acquisition column\n",
        "            # or use the grouping logic from extract_features here.\n",
        "            # Using estimated block size for now:\n",
        "            start_index = current_row_index\n",
        "            # Make sure end_index doesn't exceed total rows\n",
        "            end_index = min(start_index + est_measurements_per_acq, total_rows_in_df)\n",
        "            num_measurements_this_acq = end_index - start_index\n",
        "\n",
        "            if num_measurements_this_acq <= 0 and current_row_index < total_rows_in_df:\n",
        "                 # If estimation leads to zero measurements, take at least one if rows remain\n",
        "                 num_measurements_this_acq = 1\n",
        "                 end_index = start_index + 1\n",
        "\n",
        "            # Repeat the acquisition number and category for the determined number of measurements\n",
        "            all_acquisitions.extend([i] * num_measurements_this_acq)\n",
        "            # *** MODIFICATION START: Extend the correct list ***\n",
        "            all_damage_categories.extend([category] * num_measurements_this_acq)\n",
        "            # *** MODIFICATION END ***\n",
        "\n",
        "            current_row_index = end_index # Move to the next block start\n",
        "            acq_row_counts[i] = num_measurements_this_acq # Store count\n",
        "\n",
        "            if current_row_index >= total_rows_in_df:\n",
        "                 if i < acquisition_ranges['end']:\n",
        "                      print(f\"Warning for {freq}: Reached end of DataFrame rows ({total_rows_in_df}) before processing all expected acquisitions (up to {acquisition_ranges['end']}). Stopping assignment at acq {i}.\")\n",
        "                 break # Stop if we've assigned labels for all rows\n",
        "\n",
        "        # --- Adjust lists if total length doesn't match DataFrame ---\n",
        "        # This part might be less necessary if the block estimation above is accurate enough,\n",
        "        # but keep it as a safety net.\n",
        "        current_len = len(all_acquisitions)\n",
        "        if current_len != total_rows_in_df:\n",
        "            print(f\"Warning for {freq}: Final assigned rows ({current_len}) don't match DataFrame rows ({total_rows_in_df}). Adjusting lists.\")\n",
        "            if current_len > total_rows_in_df:\n",
        "                all_acquisitions = all_acquisitions[:total_rows_in_df]\n",
        "                # *** MODIFICATION START: Adjust correct list ***\n",
        "                all_damage_categories = all_damage_categories[:total_rows_in_df]\n",
        "                # *** MODIFICATION END ***\n",
        "            else: # current_len < total_rows_in_df\n",
        "                remaining = total_rows_in_df - current_len\n",
        "                last_acq = all_acquisitions[-1] if all_acquisitions else acquisition_ranges['end'] # Use last assigned or end\n",
        "                # Determine category for the last acquisition\n",
        "                last_cat = 0\n",
        "                if last_acq > minor_boundary: last_cat = 3\n",
        "                elif last_acq > micro_boundary: last_cat = 2\n",
        "                elif last_acq >= damage_start: last_cat = 1\n",
        "\n",
        "                all_acquisitions.extend([last_acq] * remaining)\n",
        "                # *** MODIFICATION START: Extend correct list ***\n",
        "                all_damage_categories.extend([last_cat] * remaining)\n",
        "                # *** MODIFICATION END ***\n",
        "        # --- End Adjustment ---\n",
        "\n",
        "\n",
        "        # --- Add new columns ---\n",
        "        df_copy = df.copy()\n",
        "        df_copy['acquisition_number'] = all_acquisitions\n",
        "        # *** MODIFICATION START: Use new column name ***\n",
        "        df_copy['damage_category'] = all_damage_categories\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "        updated_dfs[freq] = df_copy\n",
        "        # *** MODIFICATION START: Print new category distribution ***\n",
        "        print(f\" -> Added info for {freq}. Category distribution: {df_copy['damage_category'].value_counts().sort_index()}\")\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "    return updated_dfs\n",
        "\n",
        "def extract_features(dataframes):\n",
        "    \"\"\"\n",
        "    Extract relevant features from the ultrasonic guided wave data.\n",
        "    \"\"\"\n",
        "    feature_dfs = {}\n",
        "\n",
        "    for freq, df in dataframes.items():\n",
        "        # *** MODIFICATION START: Check for new column name ***\n",
        "        if not {'acquisition_number', 'damage_category'}.issubset(df.columns):\n",
        "             print(f\"Error: Missing 'acquisition_number' or 'damage_category' in DataFrame for {freq}. Skipping.\")\n",
        "             continue\n",
        "        # *** MODIFICATION END ***\n",
        "        if df.shape[1] < 5: # Dist, Tors, Flex, Ind?, Acq, Cat\n",
        "             print(f\"Warning: DataFrame for {freq} has fewer columns than expected. Check column indexing.\")\n",
        "\n",
        "        grouped = df.groupby('acquisition_number')\n",
        "        features = []\n",
        "\n",
        "        for acq_num, group in grouped:\n",
        "            if group.empty: continue\n",
        "            # Assuming columns are [distance?, torsional, flexural, indicator?, acq, cat]\n",
        "            torsional_data = group.iloc[:, 1].values  # Torsional column\n",
        "            flexural_data = group.iloc[:, 2].values   # Flexural column\n",
        "\n",
        "            feature_dict = {\n",
        "                'acquisition_number': acq_num,\n",
        "                # *** MODIFICATION START: Use new column name ***\n",
        "                'damage_category': group['damage_category'].iloc[0],\n",
        "                # *** MODIFICATION END ***\n",
        "\n",
        "                # Torsional wave features\n",
        "                'torsional_mean': np.mean(torsional_data),\n",
        "                'torsional_std': np.std(torsional_data),\n",
        "                'torsional_max': np.max(torsional_data),\n",
        "                'torsional_min': np.min(torsional_data),\n",
        "                'torsional_peak_to_peak': np.ptp(torsional_data),\n",
        "                'torsional_rms': np.sqrt(np.mean(np.square(torsional_data))),\n",
        "                'torsional_kurtosis': pd.Series(torsional_data).kurtosis(), # Use pandas\n",
        "\n",
        "                # Flexural wave features\n",
        "                'flexural_mean': np.mean(flexural_data),\n",
        "                'flexural_std': np.std(flexural_data),\n",
        "                'flexural_max': np.max(flexural_data),\n",
        "                'flexural_min': np.min(flexural_data),\n",
        "                'flexural_peak_to_peak': np.ptp(flexural_data),\n",
        "                'flexural_rms': np.sqrt(np.mean(np.square(flexural_data))),\n",
        "                'flexural_kurtosis': pd.Series(flexural_data).kurtosis(), # Use pandas\n",
        "\n",
        "                # Signal energy features\n",
        "                'torsional_energy': np.sum(np.square(torsional_data)),\n",
        "                'flexural_energy': np.sum(np.square(flexural_data)),\n",
        "                'energy_ratio': (np.sum(np.square(torsional_data)) /\n",
        "                                (np.sum(np.square(flexural_data)) + 1e-9)) # Add epsilon\n",
        "            }\n",
        "            features.append(feature_dict)\n",
        "\n",
        "        feature_df = pd.DataFrame(features) if features else pd.DataFrame()\n",
        "        feature_dfs[freq] = feature_df\n",
        "        if not feature_df.empty:\n",
        "            print(f\" -> Extracted features for {freq}: {feature_df.shape[0]} acquisitions.\")\n",
        "        else:\n",
        "            print(f\" -> No features extracted for {freq}.\")\n",
        "\n",
        "\n",
        "    return feature_dfs\n",
        "\n",
        "def normalize_features(feature_dfs):\n",
        "    \"\"\"\n",
        "    Normalize features using StandardScaler.\n",
        "    \"\"\"\n",
        "    normalized_dfs = {}\n",
        "    scalers = {}\n",
        "\n",
        "    for freq, df in feature_dfs.items():\n",
        "        if df.empty:\n",
        "             print(f\"Skipping normalization for empty DataFrame {freq}\")\n",
        "             normalized_dfs[freq] = df\n",
        "             scalers[freq] = None\n",
        "             continue\n",
        "        # *** MODIFICATION START: Use new column name ***\n",
        "        labels = df[['acquisition_number', 'damage_category']]\n",
        "        features = df.drop(['acquisition_number', 'damage_category'], axis=1)\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "        if features.empty:\n",
        "             print(f\"No features to normalize for {freq}. Returning only labels.\")\n",
        "             normalized_dfs[freq] = labels.reset_index(drop=True)\n",
        "             scalers[freq] = None\n",
        "             continue\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        try:\n",
        "            normalized_features = scaler.fit_transform(features)\n",
        "        except ValueError as e:\n",
        "             print(f\"Error scaling features for {freq}: {e}. Check data.\")\n",
        "             normalized_dfs[freq] = df # Return unnormalized as fallback\n",
        "             scalers[freq] = None\n",
        "             continue\n",
        "\n",
        "\n",
        "        normalized_df = pd.DataFrame(normalized_features, columns=features.columns, index=features.index) # Keep index\n",
        "\n",
        "        # *** MODIFICATION START: Align using index ***\n",
        "        normalized_df = pd.concat([labels, normalized_df], axis=1)\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "        normalized_dfs[freq] = normalized_df\n",
        "        scalers[freq] = scaler\n",
        "        print(f\" -> Normalized features for {freq}.\")\n",
        "\n",
        "    return normalized_dfs, scalers\n",
        "\n",
        "def split_data(feature_dfs, test_size=0.2, validation_size=0.15): # Renamed arg\n",
        "    \"\"\"\n",
        "    Split data into training, validation, and test sets, stratified by damage category.\n",
        "    \"\"\"\n",
        "    split_data_dict = {}\n",
        "\n",
        "    for freq, df in feature_dfs.items():\n",
        "        if df.empty:\n",
        "             print(f\"Skipping split for empty DataFrame {freq}\")\n",
        "             split_data_dict[freq] = {'train': pd.DataFrame(), 'val': pd.DataFrame(), 'test': pd.DataFrame()}\n",
        "             continue\n",
        "\n",
        "        # *** MODIFICATION START: Use new column name ***\n",
        "        y = df['damage_category']\n",
        "        # Check for sufficient samples for stratification\n",
        "        min_samples = y.value_counts().min()\n",
        "        n_splits_needed = 2 # For train/test and then train/val\n",
        "        if min_samples < n_splits_needed:\n",
        "            print(f\"Warning for {freq}: Cannot stratify split (min samples={min_samples} < {n_splits_needed}). Using non-stratified split.\")\n",
        "            stratify_y = None\n",
        "        else:\n",
        "            stratify_y = y\n",
        "\n",
        "        X = df.drop(['damage_category'], axis=1) # Keep acq number for now if needed\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "\n",
        "        # First split: separate test set\n",
        "        try:\n",
        "            X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "                X, y, test_size=test_size, random_state=42, stratify=stratify_y\n",
        "            )\n",
        "        except ValueError as e:\n",
        "            print(f\"Initial split error (stratify={stratify_y is not None}) for {freq}: {e}. Retrying without stratification.\")\n",
        "            X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "                X, y, test_size=test_size, random_state=42, stratify=None\n",
        "            )\n",
        "\n",
        "        # Second split: separate validation set\n",
        "        # Check stratification for validation split\n",
        "        if stratify_y is not None:\n",
        "             min_samples_tv = y_train_val.value_counts().min()\n",
        "             if min_samples_tv < n_splits_needed:\n",
        "                  print(f\"Warning for {freq}: Cannot stratify validation split (min samples={min_samples_tv} < {n_splits_needed}). Using non-stratified split.\")\n",
        "                  stratify_y_val = None\n",
        "             else:\n",
        "                  stratify_y_val = y_train_val\n",
        "        else:\n",
        "             stratify_y_val = None\n",
        "\n",
        "        val_ratio = validation_size / (1.0 - test_size)  # Adjusted validation ratio\n",
        "        try:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X_train_val, y_train_val, test_size=val_ratio, random_state=42, stratify=stratify_y_val\n",
        "            )\n",
        "        except ValueError as e:\n",
        "            print(f\"Validation split error (stratify={stratify_y_val is not None}) for {freq}: {e}. Retrying without stratification.\")\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X_train_val, y_train_val, test_size=val_ratio, random_state=42, stratify=None\n",
        "            )\n",
        "\n",
        "\n",
        "        # Restore the target variable and acquisition number\n",
        "        # *** MODIFICATION START: Use new column name ***\n",
        "        train_df = X_train.copy()\n",
        "        train_df['damage_category'] = y_train\n",
        "\n",
        "        val_df = X_val.copy()\n",
        "        val_df['damage_category'] = y_val\n",
        "\n",
        "        test_df = X_test.copy()\n",
        "        test_df['damage_category'] = y_test\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "        # Re-add acquisition number if it was dropped earlier (it was kept in X here)\n",
        "        # If X included 'acquisition_number', it's already in train_df, val_df, test_df\n",
        "\n",
        "        split_data_dict[freq] = {\n",
        "            'train': train_df,\n",
        "            'val': val_df,\n",
        "            'test': test_df\n",
        "        }\n",
        "        # *** MODIFICATION START: Print new category distribution ***\n",
        "        print(f\" -> Data split for {freq}: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
        "        if not train_df.empty: print(f\"    Train distribution: {train_df['damage_category'].value_counts().sort_index()}\")\n",
        "        if not val_df.empty: print(f\"    Val distribution:   {val_df['damage_category'].value_counts().sort_index()}\")\n",
        "        if not test_df.empty: print(f\"    Test distribution:  {test_df['damage_category'].value_counts().sort_index()}\")\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "    return split_data_dict\n",
        "\n",
        "def create_time_windows(dataframes, window_size=5, step=1):\n",
        "    \"\"\"\n",
        "    Create time windows for sequential data analysis using the split feature DataFrames.\n",
        "    \"\"\"\n",
        "    windowed_data = {}\n",
        "\n",
        "    for freq, df_dict in dataframes.items():\n",
        "        windowed_data[freq] = {} # Initialize frequency entry\n",
        "        for split_name, df in df_dict.items():\n",
        "             # Ensure df is not empty and has the required columns before processing\n",
        "             if df.empty or not {'acquisition_number', 'damage_category'}.issubset(df.columns):\n",
        "                 print(f\"Skipping window creation for {freq} - {split_name}: DataFrame empty or missing required columns.\")\n",
        "                 windowed_data[freq][split_name] = (np.array([]), np.array([])) # Assign empty arrays\n",
        "                 continue\n",
        "\n",
        "             windowed_result = create_windows_for_dataset(df, window_size, step)\n",
        "             windowed_data[freq][split_name] = windowed_result\n",
        "             # *** MODIFICATION START: Print shapes ***\n",
        "             if windowed_result[0].size > 0:\n",
        "                 print(f\" -> Created windows for {freq} - {split_name}: X shape {windowed_result[0].shape}, y shape {windowed_result[1].shape}\")\n",
        "             else:\n",
        "                 print(f\" -> No windows created for {freq} - {split_name}.\")\n",
        "            # *** MODIFICATION END ***\n",
        "\n",
        "    return windowed_data\n",
        "\n",
        "\n",
        "def create_windows_for_dataset(df, window_size, step):\n",
        "    \"\"\"\n",
        "    Helper function to create windows for a single dataset (train, val, or test).\n",
        "    \"\"\"\n",
        "    # Ensure df is not empty and has required columns\n",
        "    if df.empty or not {'acquisition_number', 'damage_category'}.issubset(df.columns):\n",
        "         return np.array([]), np.array([])\n",
        "\n",
        "    # Sort by acquisition number first is crucial for time windows\n",
        "    df = df.sort_values(by='acquisition_number').reset_index(drop=True)\n",
        "\n",
        "    # Get unique acquisition numbers in sorted order\n",
        "    acquisitions = df['acquisition_number'].unique()\n",
        "    acquisitions.sort()\n",
        "\n",
        "    # Features to include in the window\n",
        "    # *** MODIFICATION START: Use new column name ***\n",
        "    feature_cols = df.drop(['acquisition_number', 'damage_category'], axis=1).columns\n",
        "    # *** MODIFICATION END ***\n",
        "    if not list(feature_cols): # Check if feature_cols is empty\n",
        "        print(\"Warning: No feature columns found for window creation.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    X_windows = []\n",
        "    y_windows = []\n",
        "\n",
        "    # Iterate through unique acquisitions to define window start points\n",
        "    for i in range(0, len(acquisitions) - window_size + 1, step):\n",
        "        window_acquisitions = acquisitions[i : i + window_size]\n",
        "\n",
        "        # Select rows from the dataframe corresponding to these acquisitions\n",
        "        window_data = df[df['acquisition_number'].isin(window_acquisitions)]\n",
        "\n",
        "        # Ensure data is sorted by acquisition within the window slice\n",
        "        window_data = window_data.sort_values(by='acquisition_number')\n",
        "\n",
        "        # Check if we have exactly one row per acquisition number in the window\n",
        "        if len(window_data) != window_size:\n",
        "            # This indicates missing acquisitions or duplicate entries within the expected range\n",
        "            # print(f\"Skipping window at acq {acquisitions[i]}: Expected {window_size} rows, found {len(window_data)}.\")\n",
        "            continue\n",
        "\n",
        "        # Extract features for the window\n",
        "        features = window_data[feature_cols].values\n",
        "\n",
        "        # Get the label of the last acquisition in the window\n",
        "        last_acq = window_acquisitions[-1]\n",
        "        try:\n",
        "            # *** MODIFICATION START: Use new column name ***\n",
        "            label = window_data[window_data['acquisition_number'] == last_acq]['damage_category'].iloc[0]\n",
        "            # *** MODIFICATION END ***\n",
        "        except IndexError:\n",
        "            # Should not happen if len(window_data) == window_size, but safety check\n",
        "            # print(f\"Could not find label for last acq {last_acq} in window {i}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        X_windows.append(features)\n",
        "        y_windows.append(label)\n",
        "\n",
        "    X_windows_np = np.array(X_windows)\n",
        "    y_windows_np = np.array(y_windows)\n",
        "\n",
        "    # Handle case where no windows were created\n",
        "    if X_windows_np.size == 0:\n",
        "         return np.array([]), np.array([])\n",
        "\n",
        "    return X_windows_np, y_windows_np\n",
        "\n",
        "def visualize_data(raw_data, feature_data, split_data):\n",
        "    \"\"\"\n",
        "    Create visualizations of the data, updated for 4 categories.\n",
        "    \"\"\"\n",
        "    # --- Create visualizations directory ---\n",
        "    viz_dir = './visualizations'\n",
        "    os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "    # --- Check if data exists ---\n",
        "    if not raw_data:\n",
        "        print(\"Cannot visualize: Raw data is empty.\")\n",
        "        return\n",
        "    freq = list(raw_data.keys())[0] # Use the first frequency for examples\n",
        "    if freq not in raw_data or raw_data[freq].empty:\n",
        "        print(f\"Cannot visualize: Raw data for {freq} is empty.\")\n",
        "        return\n",
        "\n",
        "    # --- 1. Plot sample raw signals ---\n",
        "    sample_df = raw_data[freq]\n",
        "    plt.figure(figsize=(20, 5)) # Wider figure for 4 plots\n",
        "    plt.suptitle(f'Raw Signal Examples - {freq}', fontsize=16)\n",
        "\n",
        "    for cat_code, cat_name in DAMAGE_CATEGORIES.items():\n",
        "        ax = plt.subplot(1, len(DAMAGE_CATEGORIES), cat_code + 1)\n",
        "        # *** MODIFICATION START: Use new column name ***\n",
        "        cat_df_sample = sample_df[sample_df['damage_category'] == cat_code]\n",
        "        # *** MODIFICATION END ***\n",
        "        # Plot first N points if data exists\n",
        "        plot_points = cat_df_sample.head(1000) # Sample first 1000 points of the category\n",
        "\n",
        "        if not plot_points.empty and plot_points.shape[1] >= 3: # Check columns exist\n",
        "            # Assuming columns 0=distance/index, 1=torsional, 2=flexural\n",
        "            ax.plot(plot_points.iloc[:, 0], plot_points.iloc[:, 1], label='Torsional', alpha=0.8)\n",
        "            ax.plot(plot_points.iloc[:, 0], plot_points.iloc[:, 2], label='Flexural', alpha=0.8)\n",
        "            ax.set_title(cat_name)\n",
        "            ax.set_xlabel('Index/Distance')\n",
        "            if cat_code == 0: # Add Y label only to first plot\n",
        "                ax.set_ylabel('Amplitude')\n",
        "            ax.legend()\n",
        "        else:\n",
        "            ax.set_title(f\"{cat_name}\\n(No Data/Fewer Cols)\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.savefig(os.path.join(viz_dir, 'raw_signals_comparison_4cat.png'))\n",
        "    plt.close() # Close the figure\n",
        "\n",
        "\n",
        "    # --- 2. Plot feature distributions per category ---\n",
        "    if freq not in feature_data or feature_data[freq].empty:\n",
        "        print(f\"Skipping feature distribution plot for {freq}: No features.\")\n",
        "        return\n",
        "    feature_df = feature_data[freq]\n",
        "\n",
        "    plt.figure(figsize=(18, 12))\n",
        "    plt.suptitle(f'Feature Distributions by Damage Category - {freq}', fontsize=16)\n",
        "    # *** MODIFICATION START: Use new column name ***\n",
        "    features_to_plot = [col for col in feature_df.columns if col not in ['acquisition_number', 'damage_category']]\n",
        "    # *** MODIFICATION END ***\n",
        "    features_to_plot = features_to_plot[:min(len(features_to_plot), 9)] # Plot max 9\n",
        "\n",
        "    num_plots = len(features_to_plot)\n",
        "    if num_plots == 0:\n",
        "         print(f\"No features found to plot distributions for {freq}.\")\n",
        "         plt.close()\n",
        "         return\n",
        "    num_cols = 3\n",
        "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(DAMAGE_CATEGORIES))) # Color map\n",
        "\n",
        "    for i, feature in enumerate(features_to_plot):\n",
        "        ax = plt.subplot(num_rows, num_cols, i + 1)\n",
        "        for cat_code, cat_name in DAMAGE_CATEGORIES.items():\n",
        "            # *** MODIFICATION START: Use new column name ***\n",
        "            subset = feature_df[feature_df['damage_category'] == cat_code][feature]\n",
        "            # *** MODIFICATION END ***\n",
        "            if not subset.empty:\n",
        "                # Use seaborn histplot for better density estimation\n",
        "                sns.histplot(subset, kde=True, label=cat_name, ax=ax, color=colors[cat_code], stat=\"density\", common_norm=False, element=\"step\")\n",
        "        ax.set_title(f'Distribution of {feature}')\n",
        "        ax.set_xlabel(feature)\n",
        "        ax.set_ylabel('Density')\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.savefig(os.path.join(viz_dir, 'feature_distributions_4cat.png'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    # --- 3. Plot class distribution in train/val/test sets ---\n",
        "    if freq not in split_data or not all(s in split_data[freq] for s in ['train', 'val', 'test']):\n",
        "         print(f\"Skipping split distribution plot for {freq}: Split data incomplete.\")\n",
        "         return\n",
        "\n",
        "    all_counts = {}\n",
        "    splits_exist = False\n",
        "    for name, df in [('Train', split_data[freq]['train']), ('Validation', split_data[freq]['val']), ('Test', split_data[freq]['test'])]:\n",
        "        # *** MODIFICATION START: Use new column name ***\n",
        "        if not df.empty and 'damage_category' in df.columns:\n",
        "            all_counts[name] = df['damage_category'].value_counts()\n",
        "            splits_exist = True\n",
        "        else:\n",
        "            all_counts[name] = pd.Series(dtype=int)\n",
        "        # *** MODIFICATION END ***\n",
        "\n",
        "    if not splits_exist:\n",
        "        print(f\"Skipping split distribution plot for {freq}: No valid split data.\")\n",
        "        return\n",
        "\n",
        "    # Create DataFrame from counts for easier plotting\n",
        "    count_df = pd.DataFrame(all_counts).fillna(0).astype(int)\n",
        "    # Ensure all categories (0-3) are present as rows, map index to names\n",
        "    count_df = count_df.reindex(list(DAMAGE_CATEGORIES.keys()), fill_value=0)\n",
        "    count_df.index = count_df.index.map(DAMAGE_CATEGORIES) # Use names\n",
        "\n",
        "    # Plotting\n",
        "    count_df.plot(kind='bar', figsize=(12, 7), width=0.8)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel('Damage Category')\n",
        "    plt.title(f'Class Distribution in Train/Val/Test Sets - {freq}')\n",
        "    plt.xticks(rotation=0) # Keep category names horizontal\n",
        "    plt.legend(title='Dataset Split')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(viz_dir, 'class_distribution_split_4cat.png'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the data preparation pipeline.\n",
        "    \"\"\"\n",
        "    # Define file paths\n",
        "    file_path = \"/content/Merged_Pipeline_Data.xlsx\" # Make sure this path is correct\n",
        "\n",
        "    # Select which frequency sheets to process\n",
        "    selected_sheets = [\"Frequency_1\", \"Frequency_2\",\"Frequency_3\", \"Frequency_4\",\"Frequency_5\"] # Example\n",
        "\n",
        "    # Define acquisition ranges (USE THE NUMBERS FROM YOUR DATA)\n",
        "    # Example using the numbers from your previous successful run's output\n",
        "    acquisition_info = {\n",
        "        'start': 22,           # Corresponds to original #1622?\n",
        "        'end': 330,            # Corresponds to original #1930?\n",
        "        'damage_start': 302,   # Corresponds to original #1902?\n",
        "        'total_acquisitions': 309 # end - start + 1\n",
        "    }\n",
        "    # OR if using original numbers directly:\n",
        "    # acquisition_info = {\n",
        "    #     'start': 1622,\n",
        "    #     'end': 1930,\n",
        "    #     'damage_start': 1902,\n",
        "    #     'total_acquisitions': 1930 - 1622 + 1 # Calculate total\n",
        "    # }\n",
        "    # Choose ONE version of acquisition_info based on how your data is structured\n",
        "\n",
        "    # Create output directories\n",
        "    proc_dir = './processed_data'\n",
        "    viz_dir = './visualizations'\n",
        "    os.makedirs(proc_dir, exist_ok=True)\n",
        "    os.makedirs(viz_dir, exist_ok=True) # Also ensure viz dir exists\n",
        "\n",
        "    # 1. Load data\n",
        "    print(\"--- Loading Data ---\")\n",
        "    raw_data = load_frequency_data(file_path, selected_sheets)\n",
        "    if raw_data is None: return # Exit if loading fails\n",
        "\n",
        "    # 2. Add acquisition information and damage CATEGORY labels (0-3)\n",
        "    print(\"\\n--- Adding Acquisition Info & Damage Categories ---\")\n",
        "    labeled_data = add_acquisition_info(raw_data, acquisition_info)\n",
        "\n",
        "    # 3. Extract features\n",
        "    print(\"\\n--- Extracting Features ---\")\n",
        "    feature_data = extract_features(labeled_data)\n",
        "\n",
        "    # 4. Normalize features\n",
        "    print(\"\\n--- Normalizing Features ---\")\n",
        "    normalized_data, scalers = normalize_features(feature_data)\n",
        "\n",
        "    # 5. Split data\n",
        "    print(\"\\n--- Splitting Data ---\")\n",
        "    split_data_dict = split_data(normalized_data, test_size=0.2, validation_size=0.15) # Pass validation_size\n",
        "\n",
        "    # 6. Create time windows\n",
        "    print(\"\\n--- Creating Time Windows ---\")\n",
        "    windowed_data = create_time_windows(split_data_dict, window_size=5, step=1) # Pass step\n",
        "\n",
        "    # 7. Visualize the data\n",
        "    print(\"\\n--- Creating Visualizations ---\")\n",
        "    visualize_data(labeled_data, feature_data, split_data_dict)\n",
        "\n",
        "    # 8. Save processed data\n",
        "    print(\"\\n--- Saving Processed Data ---\")\n",
        "    for freq, data_dict in split_data_dict.items():\n",
        "        # Save split features (CSV) - keeping acquisition number is useful\n",
        "        if not data_dict['train'].empty:\n",
        "            data_dict['train'].to_csv(f'{proc_dir}/{freq}_train_features.csv', index=False)\n",
        "        if not data_dict['val'].empty:\n",
        "            data_dict['val'].to_csv(f'{proc_dir}/{freq}_val_features.csv', index=False)\n",
        "        if not data_dict['test'].empty:\n",
        "            data_dict['test'].to_csv(f'{proc_dir}/{freq}_test_features.csv', index=False)\n",
        "\n",
        "    for freq, window_dict in windowed_data.items():\n",
        "        # Save windowed features (NPY) - X includes features, y includes category\n",
        "        # Check if arrays are not empty before saving\n",
        "        if window_dict.get('train') and window_dict['train'][0].size > 0:\n",
        "            np.save(f'{proc_dir}/{freq}_train_windows_X.npy', window_dict['train'][0])\n",
        "            np.save(f'{proc_dir}/{freq}_train_windows_y.npy', window_dict['train'][1])\n",
        "        if window_dict.get('val') and window_dict['val'][0].size > 0:\n",
        "            np.save(f'{proc_dir}/{freq}_val_windows_X.npy', window_dict['val'][0])\n",
        "            np.save(f'{proc_dir}/{freq}_val_windows_y.npy', window_dict['val'][1])\n",
        "        if window_dict.get('test') and window_dict['test'][0].size > 0:\n",
        "            np.save(f'{proc_dir}/{freq}_test_windows_X.npy', window_dict['test'][0])\n",
        "            np.save(f'{proc_dir}/{freq}_test_windows_y.npy', window_dict['test'][1])\n",
        "\n",
        "    print(\"\\n--- Data Preparation Complete! ---\")\n",
        "    print(f\"Saved processed features (CSV) and windowed data (NPY) to '{proc_dir}'\")\n",
        "    print(f\"Saved visualizations to '{viz_dir}'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "esNfKP0acZVH",
        "outputId": "e11ba0fb-09c2-40b6-bc8d-5ce8a8dabcc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data ---\n",
            "Loaded Frequency_1 data: 591652 rows, 4 columns\n",
            "Loaded Frequency_2 data: 591652 rows, 4 columns\n",
            "Loaded Frequency_3 data: 591652 rows, 4 columns\n",
            "Loaded Frequency_4 data: 591652 rows, 4 columns\n",
            "Loaded Frequency_5 data: 591652 rows, 4 columns\n",
            "\n",
            "--- Adding Acquisition Info & Damage Categories ---\n",
            "Categorization Ranges: Normal(<302), Micro(302-310), Minor(311-319), Major(>319)\n",
            "Estimating ~1915 measurements per acquisition for Frequency_1.\n",
            " -> Added info for Frequency_1. Category distribution: damage_category\n",
            "0    536200\n",
            "1     17235\n",
            "2     17235\n",
            "3     20982\n",
            "Name: count, dtype: int64\n",
            "Estimating ~1915 measurements per acquisition for Frequency_2.\n",
            " -> Added info for Frequency_2. Category distribution: damage_category\n",
            "0    536200\n",
            "1     17235\n",
            "2     17235\n",
            "3     20982\n",
            "Name: count, dtype: int64\n",
            "Estimating ~1915 measurements per acquisition for Frequency_3.\n",
            " -> Added info for Frequency_3. Category distribution: damage_category\n",
            "0    536200\n",
            "1     17235\n",
            "2     17235\n",
            "3     20982\n",
            "Name: count, dtype: int64\n",
            "Estimating ~1915 measurements per acquisition for Frequency_4.\n",
            " -> Added info for Frequency_4. Category distribution: damage_category\n",
            "0    536200\n",
            "1     17235\n",
            "2     17235\n",
            "3     20982\n",
            "Name: count, dtype: int64\n",
            "Estimating ~1915 measurements per acquisition for Frequency_5.\n",
            " -> Added info for Frequency_5. Category distribution: damage_category\n",
            "0    536200\n",
            "1     17235\n",
            "2     17235\n",
            "3     20982\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Extracting Features ---\n",
            " -> Extracted features for Frequency_1: 309 acquisitions.\n",
            " -> Extracted features for Frequency_2: 309 acquisitions.\n",
            " -> Extracted features for Frequency_3: 309 acquisitions.\n",
            " -> Extracted features for Frequency_4: 309 acquisitions.\n",
            " -> Extracted features for Frequency_5: 309 acquisitions.\n",
            "\n",
            "--- Normalizing Features ---\n",
            " -> Normalized features for Frequency_1.\n",
            " -> Normalized features for Frequency_2.\n",
            " -> Normalized features for Frequency_3.\n",
            " -> Normalized features for Frequency_4.\n",
            " -> Normalized features for Frequency_5.\n",
            "\n",
            "--- Splitting Data ---\n",
            " -> Data split for Frequency_1: Train(200), Val(47), Test(62)\n",
            "    Train distribution: damage_category\n",
            "0    181\n",
            "1      6\n",
            "2      6\n",
            "3      7\n",
            "Name: count, dtype: int64\n",
            "    Val distribution:   damage_category\n",
            "0    43\n",
            "1     1\n",
            "2     1\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            "    Test distribution:  damage_category\n",
            "0    56\n",
            "1     2\n",
            "2     2\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            " -> Data split for Frequency_2: Train(200), Val(47), Test(62)\n",
            "    Train distribution: damage_category\n",
            "0    181\n",
            "1      6\n",
            "2      6\n",
            "3      7\n",
            "Name: count, dtype: int64\n",
            "    Val distribution:   damage_category\n",
            "0    43\n",
            "1     1\n",
            "2     1\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            "    Test distribution:  damage_category\n",
            "0    56\n",
            "1     2\n",
            "2     2\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            " -> Data split for Frequency_3: Train(200), Val(47), Test(62)\n",
            "    Train distribution: damage_category\n",
            "0    181\n",
            "1      6\n",
            "2      6\n",
            "3      7\n",
            "Name: count, dtype: int64\n",
            "    Val distribution:   damage_category\n",
            "0    43\n",
            "1     1\n",
            "2     1\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            "    Test distribution:  damage_category\n",
            "0    56\n",
            "1     2\n",
            "2     2\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            " -> Data split for Frequency_4: Train(200), Val(47), Test(62)\n",
            "    Train distribution: damage_category\n",
            "0    181\n",
            "1      6\n",
            "2      6\n",
            "3      7\n",
            "Name: count, dtype: int64\n",
            "    Val distribution:   damage_category\n",
            "0    43\n",
            "1     1\n",
            "2     1\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            "    Test distribution:  damage_category\n",
            "0    56\n",
            "1     2\n",
            "2     2\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            " -> Data split for Frequency_5: Train(200), Val(47), Test(62)\n",
            "    Train distribution: damage_category\n",
            "0    181\n",
            "1      6\n",
            "2      6\n",
            "3      7\n",
            "Name: count, dtype: int64\n",
            "    Val distribution:   damage_category\n",
            "0    43\n",
            "1     1\n",
            "2     1\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            "    Test distribution:  damage_category\n",
            "0    56\n",
            "1     2\n",
            "2     2\n",
            "3     2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Creating Time Windows ---\n",
            " -> Created windows for Frequency_1 - train: X shape (196, 5, 17), y shape (196,)\n",
            " -> Created windows for Frequency_1 - val: X shape (43, 5, 17), y shape (43,)\n",
            " -> Created windows for Frequency_1 - test: X shape (58, 5, 17), y shape (58,)\n",
            " -> Created windows for Frequency_2 - train: X shape (196, 5, 17), y shape (196,)\n",
            " -> Created windows for Frequency_2 - val: X shape (43, 5, 17), y shape (43,)\n",
            " -> Created windows for Frequency_2 - test: X shape (58, 5, 17), y shape (58,)\n",
            " -> Created windows for Frequency_3 - train: X shape (196, 5, 17), y shape (196,)\n",
            " -> Created windows for Frequency_3 - val: X shape (43, 5, 17), y shape (43,)\n",
            " -> Created windows for Frequency_3 - test: X shape (58, 5, 17), y shape (58,)\n",
            " -> Created windows for Frequency_4 - train: X shape (196, 5, 17), y shape (196,)\n",
            " -> Created windows for Frequency_4 - val: X shape (43, 5, 17), y shape (43,)\n",
            " -> Created windows for Frequency_4 - test: X shape (58, 5, 17), y shape (58,)\n",
            " -> Created windows for Frequency_5 - train: X shape (196, 5, 17), y shape (196,)\n",
            " -> Created windows for Frequency_5 - val: X shape (43, 5, 17), y shape (43,)\n",
            " -> Created windows for Frequency_5 - test: X shape (58, 5, 17), y shape (58,)\n",
            "\n",
            "--- Creating Visualizations ---\n",
            "\n",
            "--- Saving Processed Data ---\n",
            "\n",
            "--- Data Preparation Complete! ---\n",
            "Saved processed features (CSV) and windowed data (NPY) to './processed_data'\n",
            "Saved visualizations to './visualizations'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost==1.7.6"
      ],
      "metadata": {
        "id": "ZnGC8C3XdUod",
        "outputId": "0d1715f6-47e3-448d-aa3e-24ac490a28a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==1.7.6\n",
            "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.6) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.6) (1.14.1)\n",
            "Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.4\n",
            "    Uninstalling xgboost-2.1.4:\n",
            "      Successfully uninstalled xgboost-2.1.4\n",
            "Successfully installed xgboost-1.7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lifelines"
      ],
      "metadata": {
        "id": "ttZfq96xgpek",
        "outputId": "9621cd59-5a93-4116-e04c-9f404375bc43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lifelines\n",
            "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.14.1)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.7.0)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines)\n",
            "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
            "Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=3caf5723e200e3372b4d60cdc3e648da3c31e2579f4e59f9a55d08b02e05a0bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Added\n",
        "import os\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import classification_report, mean_absolute_error, confusion_matrix, accuracy_score, roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# TensorFlow / Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# XGBoost import\n",
        "import xgboost as xgb\n",
        "\n",
        "# +++ Lifelines Import +++\n",
        "from lifelines import CoxPHFitter\n",
        "from lifelines.utils import concordance_index\n",
        "# +++ End Lifelines Import +++"
      ],
      "metadata": {
        "id": "foXIdr_ymIgs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define directories\n",
        "MODEL_SAVE_DIR = './saved_models/'\n",
        "SCALER_SAVE_DIR = './saved_scalers/'\n",
        "VIZ_SAVE_DIR = './visualizations/' # Added\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(SCALER_SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(VIZ_SAVE_DIR, exist_ok=True) # Added\n",
        "\n",
        "class PipelineCrackAnalysisPipeline:\n",
        "    \"\"\"\n",
        "    Updated pipeline including visualization and basic survival analysis.\n",
        "    Assumes pre-processed data available:\n",
        "    - Window sequences: *_windows_X.npy\n",
        "    - Window labels (0-3): *_windows_y.npy\n",
        "    - Extracted features (with acquisition_number): *_features.csv\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir='./processed_data/', frequencies=['Frequency_1', 'Frequency_2']):\n",
        "        self.data_dir = data_dir\n",
        "        self.frequencies = frequencies\n",
        "        self.data = {}\n",
        "        self.models = {\n",
        "            'anomaly_detector': {},\n",
        "            'crack_classifier': {},\n",
        "            'lifetime_predictor': {} # Will store RUL regressors AND Cox model\n",
        "        }\n",
        "        self.scalers = {}\n",
        "        self.feature_importances = {}\n",
        "        self.evaluation_results = {\n",
        "             'anomaly_detection': {},\n",
        "             'crack_classification': {},\n",
        "             'lifetime_prediction': {} # Will store RUL and Survival results\n",
        "        }\n",
        "        self.crack_categories = {0: 'Normal', 1: 'Micro', 2: 'Minor', 3: 'Major'}\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Load processed windowed data (NPY) and extracted feature data (CSV).\n",
        "        Assumes _windows_y.npy and CSV 'damage_category' contain labels 0-3.\n",
        "        Assumes *_features.csv contains 'acquisition_number'.\n",
        "        \"\"\"\n",
        "        # --- This function remains the same ---\n",
        "        print(\"--- Loading Data ---\")\n",
        "        self.data = {freq: {} for freq in self.frequencies}\n",
        "        found_all_files = True\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"Loading data for {freq}...\")\n",
        "            try:\n",
        "                self.data[freq]['train_windows_X'] = np.load(f'{self.data_dir}/{freq}_train_windows_X.npy', allow_pickle=True)\n",
        "                self.data[freq]['train_windows_y'] = np.load(f'{self.data_dir}/{freq}_train_windows_y.npy', allow_pickle=True)\n",
        "                self.data[freq]['val_windows_X'] = np.load(f'{self.data_dir}/{freq}_val_windows_X.npy', allow_pickle=True)\n",
        "                self.data[freq]['val_windows_y'] = np.load(f'{self.data_dir}/{freq}_val_windows_y.npy', allow_pickle=True)\n",
        "                self.data[freq]['test_windows_X'] = np.load(f'{self.data_dir}/{freq}_test_windows_X.npy', allow_pickle=True)\n",
        "                self.data[freq]['test_windows_y'] = np.load(f'{self.data_dir}/{freq}_test_windows_y.npy', allow_pickle=True)\n",
        "                print(\" -> Loaded windowed NPY data.\")\n",
        "                self.data[freq]['train_features_df'] = pd.read_csv(f'{self.data_dir}/{freq}_train_features.csv')\n",
        "                self.data[freq]['val_features_df'] = pd.read_csv(f'{self.data_dir}/{freq}_val_features.csv')\n",
        "                self.data[freq]['test_features_df'] = pd.read_csv(f'{self.data_dir}/{freq}_test_features.csv')\n",
        "                print(\" -> Loaded feature CSV data.\")\n",
        "                for split in ['train', 'val', 'test']:\n",
        "                     df_key = f'{split}_features_df'; required_cols = {'acquisition_number', 'damage_category'}\n",
        "                     if df_key in self.data[freq] and not required_cols.issubset(self.data[freq][df_key].columns):\n",
        "                          raise ValueError(f\"Missing required columns in {df_key} for {freq}\")\n",
        "                print(f\" -> Windowed data shapes (Train X/y, Val X/y, Test X/y):\")\n",
        "                print(f\"    {self.data[freq]['train_windows_X'].shape}, {self.data[freq]['train_windows_y'].shape}\")\n",
        "                # ... [rest of print statements as before] ...\n",
        "            except FileNotFoundError as e: print(f\"Error: {e}\"); found_all_files=False; raise\n",
        "            except Exception as e: print(f\"Error: {e}\"); found_all_files=False; raise\n",
        "        if found_all_files: print(\"Data loaded successfully.\")\n",
        "        else: print(\"Data loading incomplete.\")\n",
        "\n",
        "    def _scale_windowed_data(self, freq):\n",
        "        \"\"\"Scales the windowed feature data (X) using StandardScaler.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(f\"Scaling windowed data for {freq}...\")\n",
        "        scaler = StandardScaler()\n",
        "        if self.data[freq]['train_windows_X'].size == 0:\n",
        "             print(f\" -> Skipping scaling for {freq}: Train X data is empty.\")\n",
        "             self.scalers[f\"{freq}_windows\"] = None\n",
        "             self.data[freq]['train_windows_X_scaled'] = np.array([])\n",
        "             self.data[freq]['val_windows_X_scaled'] = np.array([])\n",
        "             self.data[freq]['test_windows_X_scaled'] = np.array([])\n",
        "             return\n",
        "        train_shape = self.data[freq]['train_windows_X'].shape\n",
        "        val_shape = self.data[freq]['val_windows_X'].shape\n",
        "        test_shape = self.data[freq]['test_windows_X'].shape\n",
        "        try:\n",
        "            if len(train_shape) < 3 or train_shape[-1] == 0: raise ValueError(f\"Invalid shape: {train_shape}\")\n",
        "            train_reshaped = self.data[freq]['train_windows_X'].reshape(-1, train_shape[-1])\n",
        "            scaler.fit(train_reshaped)\n",
        "            self.data[freq]['train_windows_X_scaled'] = scaler.transform(train_reshaped).reshape(train_shape)\n",
        "            if self.data[freq]['val_windows_X'].size > 0:\n",
        "                 if len(val_shape) < 3 or val_shape[-1] != train_shape[-1]: raise ValueError(f\"Val X shape mismatch: {val_shape}\")\n",
        "                 val_reshaped = self.data[freq]['val_windows_X'].reshape(-1, val_shape[-1])\n",
        "                 self.data[freq]['val_windows_X_scaled'] = scaler.transform(val_reshaped).reshape(val_shape)\n",
        "            else: self.data[freq]['val_windows_X_scaled'] = np.array([])\n",
        "            if self.data[freq]['test_windows_X'].size > 0:\n",
        "                 if len(test_shape) < 3 or test_shape[-1] != train_shape[-1]: raise ValueError(f\"Test X shape mismatch: {test_shape}\")\n",
        "                 test_reshaped = self.data[freq]['test_windows_X'].reshape(-1, test_shape[-1])\n",
        "                 self.data[freq]['test_windows_X_scaled'] = scaler.transform(test_reshaped).reshape(test_shape)\n",
        "            else: self.data[freq]['test_windows_X_scaled'] = np.array([])\n",
        "            self.scalers[f\"{freq}_windows\"] = scaler\n",
        "            print(f\" -> Windowed data scaled for {freq}.\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Error scaling windowed data for {freq}: {e}\")\n",
        "             self.scalers[f\"{freq}_windows\"] = None\n",
        "             self.data[freq]['train_windows_X_scaled'] = np.array([])\n",
        "             self.data[freq]['val_windows_X_scaled'] = np.array([])\n",
        "             self.data[freq]['test_windows_X_scaled'] = np.array([])\n",
        "\n",
        "    def _determine_threshold(self, model, data_normal, strategy='percentile', percentile=95):\n",
        "        \"\"\"\n",
        "        Helper to determine anomaly threshold based on scores/errors from normal data.\n",
        "        Args:\n",
        "            model: The fitted anomaly detection model.\n",
        "            data_normal: Normal data (NumPy array or Pandas DataFrame).\n",
        "            strategy (str): Method ('percentile').\n",
        "            percentile (int): Percentile value.\n",
        "        Returns: float: Anomaly threshold.\n",
        "        \"\"\"\n",
        "        if isinstance(data_normal, pd.DataFrame) and data_normal.empty: data_normal = np.array([])\n",
        "        elif not isinstance(data_normal, (np.ndarray, pd.DataFrame)): raise TypeError(\"data_normal must be NumPy array or Pandas DataFrame\")\n",
        "        if data_normal.size == 0: print(\"Warning: Cannot determine threshold from empty normal data.\"); return np.inf\n",
        "\n",
        "        print(f\"Determining threshold using {model.__class__.__name__} on data shape {data_normal.shape}\")\n",
        "        data_normal_np = data_normal.values if isinstance(data_normal, pd.DataFrame) else data_normal\n",
        "\n",
        "        # --- Determine expected data dimensionality based on model type ---\n",
        "        if model.__class__.__name__ in ['Functional', 'Sequential']: # Keras model\n",
        "            is_lstm_ae = False\n",
        "            if model.__class__.__name__ == 'Sequential' and len(model.layers) > 0 and isinstance(model.layers[0], LSTM):\n",
        "                 is_lstm_ae = True # Assuming LSTM AE is Sequential starting with LSTM\n",
        "\n",
        "            expected_data_dims = 3 if is_lstm_ae else 2\n",
        "\n",
        "            # --- CORRECTED Dimension Check ---\n",
        "            if len(data_normal_np.shape) != expected_data_dims:\n",
        "                 raise ValueError(f\"Input data dimension mismatch for Keras model {model.name}. Expected {expected_data_dims}D, got {len(data_normal_np.shape)}D (shape {data_normal_np.shape}).\")\n",
        "\n",
        "            # Calculate reconstruction error\n",
        "            reconstructions = model.predict(data_normal_np, verbose=0)\n",
        "            if expected_data_dims == 3: # LSTM AE\n",
        "                errors = np.mean(np.mean(np.square(data_normal_np - reconstructions), axis=2), axis=1)\n",
        "            else: # Dense AE\n",
        "                errors = np.mean(np.square(data_normal_np - reconstructions), axis=1)\n",
        "\n",
        "        elif model.__class__.__name__ == 'IsolationForest':\n",
        "             errors = -model.decision_function(data_normal_np)\n",
        "        elif model.__class__.__name__ == 'OneClassSVM':\n",
        "              errors = -model.decision_function(data_normal_np)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type '{model.__class__.__name__}' for threshold determination.\")\n",
        "\n",
        "        # --- Calculate Threshold ---\n",
        "        if strategy == 'percentile':\n",
        "            if errors.size == 0: print(\"Warning: No errors calculated.\"); return np.inf\n",
        "            if np.all(errors == errors[0]): print(f\"Warning: All errors identical ({errors[0]:.4f}).\"); return errors[0] # Return the value itself\n",
        "            return np.percentile(errors, percentile)\n",
        "        else:\n",
        "             raise ValueError(f\"Unsupported thresholding strategy: {strategy}\")\n",
        "\n",
        "    def _extract_signal_features(self, freq):\n",
        "        \"\"\"Creates scaled feature sets from loaded CSVs.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(f\"Creating scaled feature set for {freq}...\")\n",
        "        for dataset in ['train', 'val', 'test']:\n",
        "             df_key = f'{dataset}_features_df'; scaled_df_key = f'{dataset}_features_scaled'\n",
        "             if df_key not in self.data[freq] or self.data[freq][df_key].empty:\n",
        "                 print(f\" -> Skipping feature scaling for {freq}-{dataset}: Source CSV missing or empty.\")\n",
        "                 self.data[freq][scaled_df_key] = pd.DataFrame()\n",
        "                 if dataset == 'train': self.scalers[f\"{freq}_features\"] = None\n",
        "                 continue\n",
        "             df_unscaled = self.data[freq][df_key]\n",
        "             ids_labels = df_unscaled[['acquisition_number', 'damage_category']]\n",
        "             features_unscaled = df_unscaled.drop(['acquisition_number', 'damage_category'], axis=1)\n",
        "             if features_unscaled.empty:\n",
        "                  print(f\" -> No features to scale for {freq}-{dataset}.\")\n",
        "                  self.data[freq][scaled_df_key] = ids_labels.copy()\n",
        "                  if dataset == 'train': self.scalers[f\"{freq}_features\"] = None\n",
        "                  continue\n",
        "             if dataset == 'train':\n",
        "                  try:\n",
        "                       feature_scaler = StandardScaler(); feature_scaler.fit(features_unscaled)\n",
        "                       self.scalers[f\"{freq}_features\"] = feature_scaler\n",
        "                       print(f\" -> Fitted feature scaler for {freq} on training data.\")\n",
        "                  except ValueError as e: print(f\"Error fitting feature scaler for {freq}: {e}. Skipping.\"); self.scalers[f\"{freq}_features\"] = None; break\n",
        "             scaler = self.scalers.get(f\"{freq}_features\")\n",
        "             if scaler is None:\n",
        "                  print(f\" -> Skipping scaling transform for {freq}-{dataset}: Scaler not available.\")\n",
        "                  self.data[freq][scaled_df_key] = df_unscaled.copy(); continue\n",
        "             try:\n",
        "                 scaled_features_array = scaler.transform(features_unscaled)\n",
        "                 scaled_features_df = pd.DataFrame(scaled_features_array, columns=features_unscaled.columns, index=features_unscaled.index)\n",
        "                 self.data[freq][scaled_df_key] = pd.concat([ids_labels, scaled_features_df], axis=1)\n",
        "                 print(f\" -> Scaled features created for {freq}-{dataset}.\")\n",
        "             except ValueError as e: print(f\"Error transforming features for {freq}-{dataset}: {e}. Using unscaled.\"); self.data[freq][scaled_df_key] = df_unscaled.copy()\n",
        "        print(f\" -> Feature scaling preparation done for {freq}.\")\n",
        "\n",
        "    def preprocess_features(self):\n",
        "        \"\"\"Orchestrates preprocessing.\"\"\"\n",
        "        # --- Same as previous version ---\n",
        "        print(\"\\n--- Feature Preprocessing ---\")\n",
        "        for freq in self.frequencies:\n",
        "            self._extract_signal_features(freq)\n",
        "            self._scale_windowed_data(freq)\n",
        "        print(\"Feature preprocessing completed for all frequencies.\")\n",
        "\n",
        "    def build_anomaly_detector(self):\n",
        "        \"\"\"Build anomaly detection models (Stage 1).\"\"\"\n",
        "        print(\"\\n--- Building Anomaly Detection Models (Stage 1) ---\")\n",
        "        for freq in self.frequencies:\n",
        "             print(f\"\\nTraining anomaly detectors for {freq}...\")\n",
        "             # Check data existence\n",
        "             if 'train_features_scaled' not in self.data[freq] or self.data[freq]['train_features_scaled'].empty:\n",
        "                 print(f\" -> Skipping anomaly detection for {freq}: Missing or empty scaled features.\")\n",
        "                 continue\n",
        "\n",
        "             # Prepare normal data\n",
        "             train_features_normal = self.data[freq]['train_features_scaled'][\n",
        "                 self.data[freq]['train_features_scaled']['damage_category'] == 0\n",
        "             ].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore') # Drop all non-feature cols\n",
        "\n",
        "             if 'train_windows_X_scaled' not in self.data[freq] or self.data[freq]['train_windows_X_scaled'].size == 0:\n",
        "                 train_windows_normal_scaled = np.array([])\n",
        "                 print(f\"Warning: Scaled window data missing/empty for {freq}. LSTM AE cannot be trained.\")\n",
        "             else:\n",
        "                 train_windows_normal_scaled = self.data[freq]['train_windows_X_scaled'][\n",
        "                     self.data[freq]['train_windows_y'] == 0 ]\n",
        "\n",
        "             # --- Train Feature-Based Anomaly Detectors ---\n",
        "             if not train_features_normal.empty:\n",
        "                 # 1. Isolation Forest\n",
        "                 print(\"Training Isolation Forest...\"); iso_forest = IsolationForest(n_estimators=150, contamination='auto', max_samples='auto', random_state=42, n_jobs=-1)\n",
        "                 iso_forest.fit(train_features_normal); self.models['anomaly_detector'][f'{freq}_isolation_forest'] = iso_forest; print(\" -> IF trained.\")\n",
        "                 # 2. One-Class SVM\n",
        "                 print(\"Training One-Class SVM...\"); ocsvm = OneClassSVM(nu=0.05, kernel='rbf', gamma='scale')\n",
        "                 ocsvm.fit(train_features_normal); self.models['anomaly_detector'][f'{freq}_ocsvm'] = ocsvm; print(\" -> OCSVM trained.\")\n",
        "                 # 3. Dense Autoencoder\n",
        "                 print(\"Training Dense Autoencoder...\"); input_dim = train_features_normal.shape[1]\n",
        "                 if input_dim > 0:\n",
        "                      encoding_dim = max(8, input_dim // 8); input_layer = Input(shape=(input_dim,)); encoded = Dense(input_dim // 2, activation='relu')(input_layer); encoded = Dropout(0.1)(encoded); encoded = Dense(input_dim // 4, activation='relu')(encoded); encoded = Dense(encoding_dim, activation='relu')(encoded); decoded = Dense(input_dim // 4, activation='relu')(encoded); decoded = Dropout(0.1)(decoded); decoded = Dense(input_dim // 2, activation='relu')(decoded); decoded = Dense(input_dim, activation='linear')(decoded); autoencoder = Model(input_layer, decoded)\n",
        "                      autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "                      # Prep validation data\n",
        "                      val_features_normal = self.data[freq].get('val_features_scaled', pd.DataFrame())\n",
        "                      if not val_features_normal.empty: val_features_normal = val_features_normal[val_features_normal['damage_category'] == 0].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "                      val_data_ae = (val_features_normal, val_features_normal) if not val_features_normal.empty else None\n",
        "                      callbacks_ae = [EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, mode='min')] if val_data_ae else None\n",
        "                      autoencoder.fit(train_features_normal, train_features_normal, epochs=150, batch_size=64, shuffle=True, validation_data=val_data_ae, callbacks=callbacks_ae, verbose=0)\n",
        "                      self.models['anomaly_detector'][f'{freq}_autoencoder'] = autoencoder; print(\" -> Dense AE trained.\")\n",
        "                 else: print(\" -> Skipping Dense AE: No features.\")\n",
        "             else: print(\"Skipping feature-based AD due to no normal feature data.\")\n",
        "\n",
        "             # --- Train Sequence-Based Anomaly Detector ---\n",
        "             if train_windows_normal_scaled.size > 0:\n",
        "                 print(\"Training LSTM Autoencoder...\"); timesteps, features = train_windows_normal_scaled.shape[1:]\n",
        "                 if features > 0:\n",
        "                      lstm_encoding_dim = max(16, features // 2)\n",
        "                      lstm_autoencoder = Sequential([ LSTM(128, activation='relu', input_shape=(timesteps, features), return_sequences=True), Dropout(0.2), LSTM(lstm_encoding_dim, activation='relu', return_sequences=False), RepeatVector(timesteps), LSTM(lstm_encoding_dim, activation='relu', return_sequences=True), Dropout(0.2), LSTM(128, activation='relu', return_sequences=True), TimeDistributed(Dense(features, activation='linear')) ])\n",
        "                      lstm_autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "                      # Prep validation data\n",
        "                      val_windows_normal_scaled = np.array([])\n",
        "                      if 'val_windows_X_scaled' in self.data[freq] and self.data[freq]['val_windows_X_scaled'].size > 0:\n",
        "                           val_windows_normal_scaled = self.data[freq]['val_windows_X_scaled'][self.data[freq]['val_windows_y'] == 0]\n",
        "                      val_data_lstm = (val_windows_normal_scaled, val_windows_normal_scaled) if val_windows_normal_scaled.size > 0 else None\n",
        "                      callbacks_lstm = [EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, mode='min')] if val_data_lstm else None\n",
        "                      lstm_autoencoder.fit(train_windows_normal_scaled, train_windows_normal_scaled, epochs=75, batch_size=64, shuffle=True, validation_data=val_data_lstm, callbacks=callbacks_lstm, verbose=0)\n",
        "                      self.models['anomaly_detector'][f'{freq}_lstm_autoencoder'] = lstm_autoencoder; print(\" -> LSTM AE trained.\")\n",
        "                 else: print(\" -> Skipping LSTM AE: No features in windowed data.\")\n",
        "             else: print(\"Skipping LSTM AE due to no normal window data.\")\n",
        "        print(\"\\nAnomaly detection model building completed.\")\n",
        "\n",
        "\n",
        "    def evaluate_anomaly_detector(self, threshold_percentile=95):\n",
        "        \"\"\"Evaluate anomaly detection models (Stage 1).\"\"\"\n",
        "        print(\"\\n--- Evaluating Anomaly Detection Models ---\")\n",
        "        results = {}; anomaly_thresholds = {}\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"\\nEvaluating anomaly detectors for {freq}...\"); results[freq] = {}; anomaly_thresholds[freq] = {}\n",
        "            # Check data\n",
        "            if 'test_features_scaled' not in self.data[freq] or self.data[freq]['test_features_scaled'].empty:\n",
        "                print(f\" -> Skipping anomaly evaluation for {freq}: Test features missing/empty.\"); continue\n",
        "            test_features = self.data[freq]['test_features_scaled'].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "            true_labels_features = (self.data[freq]['test_features_scaled']['damage_category'] > 0).astype(int)\n",
        "            if 'test_windows_X_scaled' in self.data[freq] and self.data[freq]['test_windows_X_scaled'].size > 0:\n",
        "                 test_windows_scaled = self.data[freq]['test_windows_X_scaled']\n",
        "                 true_labels_windows = (self.data[freq]['test_windows_y'] > 0).astype(int)\n",
        "            else: test_windows_scaled = np.array([]); true_labels_windows = np.array([])\n",
        "\n",
        "            # Get normal validation data for thresholding\n",
        "            val_features_normal = self.data[freq].get('val_features_scaled', pd.DataFrame())\n",
        "            if not val_features_normal.empty: val_features_normal = val_features_normal[val_features_normal['damage_category'] == 0].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "            val_windows_normal_scaled = np.array([])\n",
        "            if 'val_windows_X_scaled' in self.data[freq] and self.data[freq]['val_windows_X_scaled'].size > 0:\n",
        "                 val_windows_normal_scaled = self.data[freq]['val_windows_X_scaled'][self.data[freq]['val_windows_y'] == 0]\n",
        "\n",
        "            # Evaluate IF\n",
        "            if f'{freq}_isolation_forest' in self.models['anomaly_detector']:\n",
        "                print(\"Evaluating Isolation Forest...\"); iso_forest = self.models['anomaly_detector'][f'{freq}_isolation_forest']\n",
        "                iso_scores = iso_forest.decision_function(test_features); iso_predictions = (iso_forest.predict(test_features) == -1).astype(int)\n",
        "                try: auc = roc_auc_score(true_labels_features, -iso_scores)\n",
        "                except ValueError: auc = float('nan')\n",
        "                results[freq]['Isolation Forest'] = {'Accuracy': accuracy_score(true_labels_features, iso_predictions), 'AUC': auc, 'Confusion Matrix': confusion_matrix(true_labels_features, iso_predictions).tolist(), 'Classification Report': classification_report(true_labels_features, iso_predictions, output_dict=True, zero_division=0)}\n",
        "                print(f\" -> IF Accuracy: {results[freq]['Isolation Forest']['Accuracy']:.4f}, AUC: {results[freq]['Isolation Forest']['AUC']:.4f}\")\n",
        "            # Evaluate OCSVM\n",
        "            if f'{freq}_ocsvm' in self.models['anomaly_detector']:\n",
        "                 print(\"Evaluating One-Class SVM...\"); ocsvm = self.models['anomaly_detector'][f'{freq}_ocsvm']\n",
        "                 ocsvm_scores = ocsvm.decision_function(test_features); ocsvm_predictions = (ocsvm.predict(test_features) == -1).astype(int)\n",
        "                 try: auc = roc_auc_score(true_labels_features, -ocsvm_scores)\n",
        "                 except ValueError: auc = float('nan')\n",
        "                 results[freq]['One-Class SVM'] = {'Accuracy': accuracy_score(true_labels_features, ocsvm_predictions), 'AUC': auc, 'Confusion Matrix': confusion_matrix(true_labels_features, ocsvm_predictions).tolist(), 'Classification Report': classification_report(true_labels_features, ocsvm_predictions, output_dict=True, zero_division=0)}\n",
        "                 print(f\" -> OCSVM Accuracy: {results[freq]['One-Class SVM']['Accuracy']:.4f}, AUC: {results[freq]['One-Class SVM']['AUC']:.4f}\")\n",
        "            # Evaluate Dense AE\n",
        "            if f'{freq}_autoencoder' in self.models['anomaly_detector']:\n",
        "                 print(\"Evaluating Dense Autoencoder...\"); autoencoder = self.models['anomaly_detector'][f'{freq}_autoencoder']\n",
        "                 ae_threshold = np.inf # Default if no normal data\n",
        "                 if not val_features_normal.empty: ae_threshold = self._determine_threshold(autoencoder, val_features_normal, percentile=threshold_percentile)\n",
        "                 else: print(\" -> Warning: Cannot determine AE threshold.\");\n",
        "                 anomaly_thresholds[freq]['dense_autoencoder'] = ae_threshold\n",
        "                 ae_reconstructions = autoencoder.predict(test_features, verbose=0)\n",
        "                 ae_mse = np.mean(np.square(test_features.values - ae_reconstructions), axis=1)\n",
        "                 ae_predictions = (ae_mse > ae_threshold).astype(int)\n",
        "                 try: auc = roc_auc_score(true_labels_features, ae_mse)\n",
        "                 except ValueError: auc = float('nan')\n",
        "                 results[freq]['Dense Autoencoder'] = {'Accuracy': accuracy_score(true_labels_features, ae_predictions), 'AUC': auc, 'Threshold': ae_threshold, 'Confusion Matrix': confusion_matrix(true_labels_features, ae_predictions).tolist(), 'Classification Report': classification_report(true_labels_features, ae_predictions, output_dict=True, zero_division=0), 'reconstruction_error': ae_mse} # Added errors\n",
        "                 print(f\" -> Dense AE Accuracy: {results[freq]['Dense Autoencoder']['Accuracy']:.4f}, AUC: {results[freq]['Dense Autoencoder']['AUC']:.4f} (Thresh: {ae_threshold:.4f})\")\n",
        "            # Evaluate LSTM AE\n",
        "            if f'{freq}_lstm_autoencoder' in self.models['anomaly_detector']:\n",
        "                 print(\"Evaluating LSTM Autoencoder...\")\n",
        "                 lstm_ae = self.models['anomaly_detector'][f'{freq}_lstm_autoencoder']\n",
        "                 lstm_threshold = np.inf\n",
        "                 if val_windows_normal_scaled.size > 0: lstm_threshold = self._determine_threshold(lstm_ae, val_windows_normal_scaled, percentile=threshold_percentile)\n",
        "                 else: print(\" -> Warning: Cannot determine LSTM AE threshold.\")\n",
        "                 anomaly_thresholds[freq]['lstm_autoencoder'] = lstm_threshold\n",
        "                 if test_windows_scaled.size > 0:\n",
        "                      lstm_reconstructions = lstm_ae.predict(test_windows_scaled, verbose=0)\n",
        "                      lstm_mse = np.mean(np.mean(np.square(test_windows_scaled - lstm_reconstructions), axis=2), axis=1)\n",
        "                      lstm_predictions = (lstm_mse > lstm_threshold).astype(int)\n",
        "                      try: auc = roc_auc_score(true_labels_windows, lstm_mse)\n",
        "                      except ValueError: auc = float('nan')\n",
        "                      results[freq]['LSTM Autoencoder'] = {'Accuracy': accuracy_score(true_labels_windows, lstm_predictions), 'AUC': auc, 'Threshold': lstm_threshold, 'Confusion Matrix': confusion_matrix(true_labels_windows, lstm_predictions).tolist(), 'Classification Report': classification_report(true_labels_windows, lstm_predictions, output_dict=True, zero_division=0), 'reconstruction_error': lstm_mse} # Added errors\n",
        "                      print(f\" -> LSTM AE Accuracy: {results[freq]['LSTM Autoencoder']['Accuracy']:.4f}, AUC: {results[freq]['LSTM Autoencoder']['AUC']:.4f} (Thresh: {lstm_threshold:.4f})\")\n",
        "                 else: print(\" -> Skipping LSTM AE eval: No test window data.\")\n",
        "\n",
        "        self.evaluation_results['anomaly_detection'] = results\n",
        "        self.evaluation_results['anomaly_thresholds'] = anomaly_thresholds\n",
        "        print(\"\\nAnomaly detection evaluation completed.\")\n",
        "        return results\n",
        "\n",
        "\n",
        "    def build_crack_classifier(self):\n",
        "        \"\"\"Build classification models (Stage 2) using labels 0-3.\"\"\"\n",
        "        # --- Function code filled in (same as previous correct 4-cat version) ---\n",
        "        print(\"\\n--- Building Crack Classification Models (Stage 2) ---\")\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"\\nTraining crack classifiers for {freq}...\")\n",
        "            if 'train_features_scaled' not in self.data[freq] or self.data[freq]['train_features_scaled'].empty:\n",
        "                 print(f\" -> Skipping classifier training for {freq}: Features missing/empty.\"); continue\n",
        "            X_train = self.data[freq]['train_features_scaled'].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "            y_train_cat = self.data[freq]['train_features_scaled']['damage_category'].values\n",
        "            X_val = pd.DataFrame(); y_val_cat = np.array([]); val_set_for_xgb = None; val_data_for_nn = None\n",
        "            if 'val_features_scaled' in self.data[freq] and not self.data[freq]['val_features_scaled'].empty:\n",
        "                 X_val = self.data[freq]['val_features_scaled'].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "                 y_val_cat = self.data[freq]['val_features_scaled']['damage_category'].values\n",
        "                 val_set_for_xgb = [(X_val, y_val_cat)] if not X_val.empty else None\n",
        "                 val_data_for_nn = (X_val, y_val_cat) if not X_val.empty else None\n",
        "            else: print(f\" -> Warning: Validation data missing/empty for {freq}.\")\n",
        "            num_classes = len(self.crack_categories)\n",
        "            print(f\"Training label distribution: {np.bincount(y_train_cat, minlength=num_classes)}\")\n",
        "            if y_val_cat.size > 0: print(f\"Validation label distribution: {np.bincount(y_val_cat, minlength=num_classes)}\")\n",
        "            unique_train_labels = np.unique(y_train_cat); print(f\"Unique training labels found: {unique_train_labels}\")\n",
        "            if len(unique_train_labels) < num_classes: print(f\"Warning: Training data only contains {len(unique_train_labels)}/{num_classes} categories.\")\n",
        "            if X_train.empty: print(\" -> Skipping: Training data is empty.\"); continue\n",
        "\n",
        "            # Train RF\n",
        "            print(\"Training Random Forest Classifier...\"); rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=3, class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "            rf_classifier.fit(X_train, y_train_cat); self.models['crack_classifier'][f'{freq}_random_forest'] = rf_classifier\n",
        "            if not X_val.empty: val_preds_rf = rf_classifier.predict(X_val); val_acc_rf = accuracy_score(y_val_cat, val_preds_rf); print(f\" -> RF Validation Accuracy: {val_acc_rf:.4f}\")\n",
        "            else: print(\" -> RF Validation skipped.\");\n",
        "            if hasattr(X_train, 'columns'): self.feature_importances[f'{freq}_crack_classification_rf'] = pd.Series(rf_classifier.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "            # Train XGBoost\n",
        "            print(\"Training XGBoost Classifier...\"); xgb_classifier = xgb.XGBClassifier(n_estimators=150, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, objective='multi:softmax', num_class=num_classes, random_state=42, n_jobs=-1)\n",
        "            try:\n",
        "                 xgb_classifier.fit(X_train, y_train_cat, eval_set=val_set_for_xgb, eval_metric='mlogloss', early_stopping_rounds=10 if val_set_for_xgb else None, verbose=False)\n",
        "                 self.models['crack_classifier'][f'{freq}_xgboost'] = xgb_classifier\n",
        "                 if val_set_for_xgb: val_preds_xgb = xgb_classifier.predict(X_val); val_acc_xgb = accuracy_score(y_val_cat, val_preds_xgb); print(f\" -> XGB Validation Accuracy: {val_acc_xgb:.4f}\")\n",
        "                 else: print(\" -> XGB Validation skipped.\")\n",
        "            except ValueError as e: print(f\"Error training XGBoost for {freq}: {e}\")\n",
        "\n",
        "            # Train NN\n",
        "            print(\"Training Dense Neural Network Classifier...\"); input_dim = X_train.shape[1]\n",
        "            if input_dim > 0:\n",
        "                 nn_classifier = Sequential([ Dense(128, activation='relu', input_shape=(input_dim,)), Dropout(0.4), Dense(64, activation='relu'), Dropout(0.3), Dense(32, activation='relu'), Dense(num_classes, activation='softmax') ])\n",
        "                 nn_classifier.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "                 callbacks_nn = [EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max')] if val_data_for_nn else None\n",
        "                 nn_classifier.fit(X_train, y_train_cat, epochs=150, batch_size=64, validation_data=val_data_for_nn, callbacks=callbacks_nn, verbose=0)\n",
        "                 self.models['crack_classifier'][f'{freq}_neural_network'] = nn_classifier\n",
        "                 if val_data_for_nn: val_loss_nn, val_acc_nn = nn_classifier.evaluate(X_val, y_val_cat, verbose=0); print(f\" -> NN Validation Accuracy: {val_acc_nn:.4f}\")\n",
        "                 else: print(\" -> NN Validation skipped.\")\n",
        "            else: print(\" -> Skipping NN: No input features.\")\n",
        "        print(\"\\nCrack classification model building completed.\")\n",
        "\n",
        "\n",
        "    def evaluate_crack_classifier(self):\n",
        "        \"\"\"Evaluate classification models (Stage 2) using labels 0-3.\"\"\"\n",
        "        # --- Function code filled in (same as previous correct 4-cat version) ---\n",
        "        print(\"\\n--- Evaluating Crack Classification Models ---\")\n",
        "        results = {}\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"\\nEvaluating crack classifiers for {freq}...\"); results[freq] = {}\n",
        "            if 'test_features_scaled' not in self.data[freq] or self.data[freq]['test_features_scaled'].empty:\n",
        "                 print(f\" -> Skipping classifier evaluation for {freq}: Test features missing/empty.\"); continue\n",
        "            X_test = self.data[freq]['test_features_scaled'].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "            y_test_cat = self.data[freq]['test_features_scaled']['damage_category'].values\n",
        "            target_names = [self.crack_categories[i] for i in sorted(self.crack_categories.keys())]\n",
        "            labels_present = sorted(np.unique(y_test_cat)); target_names_present = [self.crack_categories[i] for i in labels_present]\n",
        "            print(f\"Test label distribution: {np.bincount(y_test_cat, minlength=len(self.crack_categories))}\")\n",
        "            if X_test.empty: print(\" -> Skipping eval: Test data features are empty.\"); continue\n",
        "\n",
        "            # Evaluate RF\n",
        "            if f'{freq}_random_forest' in self.models['crack_classifier']:\n",
        "                 print(\"Evaluating Random Forest Classifier...\"); rf_classifier = self.models['crack_classifier'][f'{freq}_random_forest']\n",
        "                 rf_preds = rf_classifier.predict(X_test); results[freq]['Random Forest'] = {'Accuracy': accuracy_score(y_test_cat, rf_preds), 'Confusion Matrix': confusion_matrix(y_test_cat, rf_preds, labels=labels_present).tolist(), 'Confusion Matrix Labels': labels_present, 'Classification Report': classification_report(y_test_cat, rf_preds, labels=labels_present, target_names=target_names_present, output_dict=True, zero_division=0)}\n",
        "                 print(f\" -> RF Test Accuracy: {results[freq]['Random Forest']['Accuracy']:.4f}\")\n",
        "            # Evaluate XGB\n",
        "            if f'{freq}_xgboost' in self.models['crack_classifier']:\n",
        "                 print(\"Evaluating XGBoost Classifier...\"); xgb_classifier = self.models['crack_classifier'][f'{freq}_xgboost']\n",
        "                 xgb_preds = xgb_classifier.predict(X_test); results[freq]['XGBoost'] = {'Accuracy': accuracy_score(y_test_cat, xgb_preds), 'Confusion Matrix': confusion_matrix(y_test_cat, xgb_preds, labels=labels_present).tolist(), 'Confusion Matrix Labels': labels_present, 'Classification Report': classification_report(y_test_cat, xgb_preds, labels=labels_present, target_names=target_names_present, output_dict=True, zero_division=0)}\n",
        "                 print(f\" -> XGB Test Accuracy: {results[freq]['XGBoost']['Accuracy']:.4f}\")\n",
        "            # Evaluate NN\n",
        "            if f'{freq}_neural_network' in self.models['crack_classifier']:\n",
        "                 print(\"Evaluating Neural Network Classifier...\"); nn_classifier = self.models['crack_classifier'][f'{freq}_neural_network']\n",
        "                 nn_pred_probs = nn_classifier.predict(X_test, verbose=0); nn_preds = np.argmax(nn_pred_probs, axis=1)\n",
        "                 results[freq]['Neural Network'] = {'Accuracy': accuracy_score(y_test_cat, nn_preds), 'Confusion Matrix': confusion_matrix(y_test_cat, nn_preds, labels=labels_present).tolist(), 'Confusion Matrix Labels': labels_present, 'Classification Report': classification_report(y_test_cat, nn_preds, labels=labels_present, target_names=target_names_present, output_dict=True, zero_division=0)}\n",
        "                 print(f\" -> NN Test Accuracy: {results[freq]['Neural Network']['Accuracy']:.4f}\")\n",
        "        self.evaluation_results['crack_classification'] = results\n",
        "        print(\"\\nCrack classification evaluation completed.\")\n",
        "        return results\n",
        "\n",
        "\n",
        "    def _generate_synthetic_rul(self, labels, **kwargs): # Removed time_proxy\n",
        "        \"\"\"Generates synthetic RUL based on labels (0-3).\"\"\"\n",
        "        # --- Function code filled in ---\n",
        "        print(\"Generating synthetic RUL (higher category -> lower RUL)...\")\n",
        "        max_rul=kwargs.get('max_rul', 1000)\n",
        "        damage_impact_factor=kwargs.get('damage_impact_factor', 100)\n",
        "        rul = np.zeros_like(labels, dtype=float)\n",
        "        for i, label in enumerate(labels):\n",
        "            if label == 0: rul[i] = max_rul\n",
        "            else: rul[i] = max(0, max_rul - damage_impact_factor * label)\n",
        "        return rul\n",
        "\n",
        "    def build_lifetime_predictor(self, max_rul=1000, damage_impact=100):\n",
        "        \"\"\"Build RUL regression models (Stage 3).\"\"\"\n",
        "        # --- Function code filled in ---\n",
        "        print(\"\\n--- Building Lifetime Prediction Models (Stage 3 - RUL Regression) ---\")\n",
        "        print(f\"NOTE: Using synthetic RUL (max={max_rul}, damage_impact={damage_impact}) for demonstration.\")\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"\\nTraining RUL predictors for {freq}...\")\n",
        "            # Check data\n",
        "            if 'train_features_scaled' not in self.data[freq] or self.data[freq]['train_features_scaled'].empty:\n",
        "                 print(f\" -> Skipping RUL training for {freq}: Features missing/empty.\"); continue\n",
        "            X_train_feat = self.data[freq]['train_features_scaled'].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "            y_train_labels = self.data[freq]['train_features_scaled']['damage_category'].values\n",
        "            X_val_feat = pd.DataFrame(); y_val_rul = np.array([])\n",
        "            if 'val_features_scaled' in self.data[freq] and not self.data[freq]['val_features_scaled'].empty:\n",
        "                 X_val_feat = self.data[freq]['val_features_scaled'].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "                 y_val_labels = self.data[freq]['val_features_scaled']['damage_category'].values\n",
        "                 y_val_rul = self._generate_synthetic_rul(y_val_labels, max_rul=max_rul, damage_impact_factor=damage_impact)\n",
        "            else: print(f\" -> Warning: Validation data missing/empty for {freq}. RUL training without validation.\")\n",
        "            if X_train_feat.empty: print(\" -> Skipping: Training features empty.\"); continue\n",
        "            y_train_rul = self._generate_synthetic_rul(y_train_labels, max_rul=max_rul, damage_impact_factor=damage_impact)\n",
        "\n",
        "            # Train RF Regressor\n",
        "            print(\"Training Random Forest Regressor...\"); rf_regressor = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=3, random_state=42, n_jobs=-1)\n",
        "            rf_regressor.fit(X_train_feat, y_train_rul); self.models['lifetime_predictor'][f'{freq}_random_forest'] = rf_regressor\n",
        "            if not X_val_feat.empty: val_preds_rf = rf_regressor.predict(X_val_feat); val_mae_rf = mean_absolute_error(y_val_rul, val_preds_rf); print(f\" -> RF Regressor Validation MAE: {val_mae_rf:.2f}\")\n",
        "            else: print(\" -> RF Validation skipped.\")\n",
        "\n",
        "            # Train XGB Regressor\n",
        "            print(\"Training XGBoost Regressor...\"); xgb_regressor = xgb.XGBRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
        "            val_set_xgb_rul = [(X_val_feat, y_val_rul)] if not X_val_feat.empty else None\n",
        "            xgb_regressor.fit(X_train_feat, y_train_rul, eval_set=val_set_xgb_rul, early_stopping_rounds=10 if val_set_xgb_rul else None, verbose=False)\n",
        "            self.models['lifetime_predictor'][f'{freq}_xgboost'] = xgb_regressor\n",
        "            if val_set_xgb_rul: val_preds_xgb = xgb_regressor.predict(X_val_feat); val_mae_xgb = mean_absolute_error(y_val_rul, val_preds_xgb); print(f\" -> XGB Regressor Validation MAE: {val_mae_xgb:.2f}\")\n",
        "            else: print(\" -> XGB Validation skipped.\")\n",
        "\n",
        "            # Train LSTM Regressor\n",
        "            if 'train_windows_X_scaled' in self.data[freq] and self.data[freq]['train_windows_X_scaled'].size > 0:\n",
        "                 print(\"Training LSTM Regressor...\"); X_train_win = self.data[freq]['train_windows_X_scaled']; y_train_win_labels = self.data[freq]['train_windows_y']\n",
        "                 timesteps, n_features = X_train_win.shape[1:]\n",
        "                 if n_features > 0:\n",
        "                      y_train_win_rul = self._generate_synthetic_rul(y_train_win_labels, max_rul=max_rul, damage_impact_factor=damage_impact)\n",
        "                      lstm_regressor = Sequential([ LSTM(128, activation='relu', input_shape=(timesteps, n_features), return_sequences=True), Dropout(0.3), LSTM(64, activation='relu', return_sequences=False), Dropout(0.3), Dense(32, activation='relu'), Dense(1, activation='linear') ])\n",
        "                      lstm_regressor.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "                      # Prep validation data\n",
        "                      val_data_lstm_rul = None; callbacks_lstm_rul = None\n",
        "                      if 'val_windows_X_scaled' in self.data[freq] and self.data[freq]['val_windows_X_scaled'].size > 0:\n",
        "                           X_val_win = self.data[freq]['val_windows_X_scaled']; y_val_win_labels = self.data[freq]['val_windows_y']\n",
        "                           y_val_win_rul = self._generate_synthetic_rul(y_val_win_labels, max_rul=max_rul, damage_impact_factor=damage_impact)\n",
        "                           val_data_lstm_rul = (X_val_win, y_val_win_rul)\n",
        "                           callbacks_lstm_rul = [EarlyStopping(monitor='val_mae', patience=20, restore_best_weights=True, mode='min')]\n",
        "                      lstm_regressor.fit(X_train_win, y_train_win_rul, epochs=100, batch_size=64, validation_data=val_data_lstm_rul, callbacks=callbacks_lstm_rul, verbose=0)\n",
        "                      self.models['lifetime_predictor'][f'{freq}_lstm'] = lstm_regressor\n",
        "                      if val_data_lstm_rul: val_loss_lstm, val_mae_lstm = lstm_regressor.evaluate(X_val_win, y_val_win_rul, verbose=0); print(f\" -> LSTM Regressor Validation MAE: {val_mae_lstm:.2f}\")\n",
        "                      else: print(\" -> LSTM Validation skipped.\")\n",
        "                 else: print(\" -> Skipping LSTM Regressor: No features in windowed data.\")\n",
        "            else: print(\"Skipping LSTM Regressor due to missing/empty window data.\")\n",
        "        print(\"\\nLifetime prediction model building completed (RUL Regression).\")\n",
        "\n",
        "\n",
        "    def evaluate_lifetime_predictor(self, max_rul=1000, damage_impact=100):\n",
        "        \"\"\"Evaluate RUL regression models (Stage 3).\"\"\"\n",
        "        # --- Function code filled in ---\n",
        "        print(\"\\n--- Evaluating Lifetime Prediction Models (RUL Regression) ---\")\n",
        "        results = {}\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"\\nEvaluating RUL predictors for {freq}...\"); results[freq] = {}\n",
        "            # Check data\n",
        "            if 'test_features_scaled' not in self.data[freq] or self.data[freq]['test_features_scaled'].empty:\n",
        "                 print(f\" -> Skipping RUL evaluation for {freq}: Test features missing/empty.\"); continue\n",
        "            X_test_feat = self.data[freq]['test_features_scaled'].drop(['damage_category','acquisition_number','damage_category'], axis=1, errors='ignore')\n",
        "            y_test_labels_feat = self.data[freq]['test_features_scaled']['damage_category'].values\n",
        "            y_test_rul_feat = self._generate_synthetic_rul(y_test_labels_feat, max_rul=max_rul, damage_impact_factor=damage_impact)\n",
        "            if X_test_feat.empty: print(\" -> Skipping eval: Test features empty.\"); continue\n",
        "\n",
        "            # Evaluate RF Regressor\n",
        "            if f'{freq}_random_forest' in self.models['lifetime_predictor']:\n",
        "                print(\"Evaluating Random Forest Regressor...\"); rf_regressor = self.models['lifetime_predictor'][f'{freq}_random_forest']\n",
        "                rf_preds = rf_regressor.predict(X_test_feat)\n",
        "                results[freq]['Random Forest'] = {'MAE': mean_absolute_error(y_test_rul_feat, rf_preds), 'MSE': mean_squared_error(y_test_rul_feat, rf_preds), 'R2': r2_score(y_test_rul_feat, rf_preds), 'Predictions': rf_preds.tolist()}\n",
        "                print(f\" -> RF Regressor Test MAE: {results[freq]['Random Forest']['MAE']:.2f}, R2: {results[freq]['Random Forest']['R2']:.4f}\")\n",
        "            # Evaluate XGB Regressor\n",
        "            if f'{freq}_xgboost' in self.models['lifetime_predictor']:\n",
        "                print(\"Evaluating XGBoost Regressor...\"); xgb_regressor = self.models['lifetime_predictor'][f'{freq}_xgboost']\n",
        "                xgb_preds = xgb_regressor.predict(X_test_feat)\n",
        "                results[freq]['XGBoost'] = {'MAE': mean_absolute_error(y_test_rul_feat, xgb_preds), 'MSE': mean_squared_error(y_test_rul_feat, xgb_preds), 'R2': r2_score(y_test_rul_feat, xgb_preds), 'Predictions': xgb_preds.tolist()}\n",
        "                print(f\" -> XGB Regressor Test MAE: {results[freq]['XGBoost']['MAE']:.2f}, R2: {results[freq]['XGBoost']['R2']:.4f}\")\n",
        "            # Evaluate LSTM Regressor\n",
        "            if f'{freq}_lstm' in self.models['lifetime_predictor']:\n",
        "                 print(\"Evaluating LSTM Regressor...\")\n",
        "                 lstm_regressor = self.models['lifetime_predictor'][f'{freq}_lstm']\n",
        "                 if 'test_windows_X_scaled' in self.data[freq] and self.data[freq]['test_windows_X_scaled'].size > 0:\n",
        "                     X_test_win = self.data[freq]['test_windows_X_scaled']; y_test_labels_win = self.data[freq]['test_windows_y']\n",
        "                     y_test_rul_win = self._generate_synthetic_rul(y_test_labels_win, max_rul=max_rul, damage_impact_factor=damage_impact)\n",
        "                     lstm_preds = lstm_regressor.predict(X_test_win, verbose=0).flatten()\n",
        "                     results[freq]['LSTM'] = {'MAE': mean_absolute_error(y_test_rul_win, lstm_preds), 'MSE': mean_squared_error(y_test_rul_win, lstm_preds), 'R2': r2_score(y_test_rul_win, lstm_preds), 'Predictions': lstm_preds.tolist()}\n",
        "                     print(f\" -> LSTM Regressor Test MAE: {results[freq]['LSTM']['MAE']:.2f}, R2: {results[freq]['LSTM']['R2']:.4f}\")\n",
        "                 else: print(\" -> Skipping LSTM RUL eval: Test window data missing/empty.\")\n",
        "\n",
        "        # Store RUL results (excluding CoxPH results which are handled separately)\n",
        "        rul_eval_results = {freq: {k: v for k, v in res.items() if k != 'CoxPH'} for freq, res in results.items()}\n",
        "        if 'RUL' not in self.evaluation_results['lifetime_prediction']:\n",
        "            self.evaluation_results['lifetime_prediction']['RUL'] = {}\n",
        "        self.evaluation_results['lifetime_prediction']['RUL'].update(rul_eval_results)\n",
        "        print(\"\\nLifetime prediction evaluation completed (RUL Regression).\")\n",
        "        return rul_eval_results # Return only RUL results from this function\n",
        "\n",
        "\n",
        "    # --- Survival Analysis methods unchanged ---\n",
        "    def build_survival_model(self):\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Building Survival Analysis Model (Stage 3 - CoxPH) ---\")\n",
        "        print(\"CAUTION: Using simplified survival data (window acquisition as time, category>0 as event).\")\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"\\nTraining CoxPH model for {freq}...\")\n",
        "            df_key = 'train_features_scaled'\n",
        "            if df_key not in self.data[freq] or self.data[freq][df_key].empty: print(f\" -> Skipping CoxPH training for {freq}: Features missing/empty.\"); continue\n",
        "            survival_df = self.data[freq][df_key].copy()\n",
        "            survival_df['duration'] = survival_df['acquisition_number']\n",
        "            survival_df['event'] = (survival_df['damage_category'] > 0).astype(int)\n",
        "            covariate_cols = [col for col in survival_df.columns if col not in ['acquisition_number', 'damage_category', 'duration', 'event', 'damage_category']]\n",
        "            if not covariate_cols: print(f\" -> Skipping CoxPH training for {freq}: No covariates.\"); continue\n",
        "            if survival_df['duration'].nunique() < 2 or survival_df['event'].nunique() < 2: print(f\" -> Skipping CoxPH training for {freq}: Not enough variability.\"); continue\n",
        "            survival_df = survival_df[['duration', 'event'] + covariate_cols].replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
        "            if survival_df.empty: print(f\" -> Skipping CoxPH training for {freq}: Data empty after dropna.\"); continue\n",
        "            cph = CoxPHFitter(penalizer=0.1)\n",
        "            try: cph.fit(survival_df, duration_col='duration', event_col='event'); self.models['lifetime_predictor'][f'{freq}_coxph'] = cph; print(f\" -> CoxPH model trained for {freq}.\"); cph.print_summary(decimals=3)\n",
        "            except Exception as e: print(f\"Error training CoxPH model for {freq}: {e}\")\n",
        "        print(\"\\nSurvival Analysis model building completed.\")\n",
        "\n",
        "    def evaluate_survival_model(self):\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Evaluating Survival Analysis Model (CoxPH) ---\")\n",
        "        results_survival = {}\n",
        "        for freq in self.frequencies:\n",
        "             print(f\"\\nEvaluating CoxPH model for {freq}...\"); model_key = f'{freq}_coxph'; df_key = 'test_features_scaled'\n",
        "             if model_key not in self.models['lifetime_predictor']: print(f\" -> Skipping CoxPH eval for {freq}: Model not found.\"); continue\n",
        "             if df_key not in self.data[freq] or self.data[freq][df_key].empty: print(f\" -> Skipping CoxPH eval for {freq}: Test features missing/empty.\"); continue\n",
        "             cph = self.models['lifetime_predictor'][model_key]; test_survival_df = self.data[freq][df_key].copy()\n",
        "             test_survival_df['duration'] = test_survival_df['acquisition_number']; test_survival_df['event'] = (test_survival_df['damage_category'] > 0).astype(int)\n",
        "             covariate_cols = [col for col in test_survival_df.columns if col not in ['acquisition_number', 'damage_category', 'duration', 'event', 'damage_category']]\n",
        "             if not covariate_cols: print(f\" -> Skipping CoxPH eval for {freq}: No covariates.\"); continue\n",
        "             test_survival_df = test_survival_df[['duration', 'event'] + covariate_cols].replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
        "             if test_survival_df.empty: print(f\" -> Skipping CoxPH eval for {freq}: Test data empty after dropna.\"); continue\n",
        "             try: c_index = cph.score(test_survival_df, scoring_method=\"concordance_index\"); results_survival[freq] = {'Concordance Index': c_index}; print(f\" -> CoxPH Concordance Index for {freq}: {c_index:.4f}\")\n",
        "             except Exception as e: print(f\"Error evaluating CoxPH model for {freq}: {e}\"); results_survival[freq] = {'Concordance Index': np.nan}\n",
        "        if 'CoxPH' not in self.evaluation_results['lifetime_prediction']: self.evaluation_results['lifetime_prediction']['CoxPH'] = {}\n",
        "        self.evaluation_results['lifetime_prediction']['CoxPH'].update(results_survival)\n",
        "        print(\"\\nSurvival Analysis model evaluation completed.\")\n",
        "        return results_survival\n",
        "\n",
        "\n",
        "    # --- Visualization Methods (Filled in) ---\n",
        "    def visualize_data(self):\n",
        "        \"\"\"Generates standard visualizations (feature/split distributions).\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Creating Visualizations ---\")\n",
        "        os.makedirs(VIZ_SAVE_DIR, exist_ok=True)\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"\\nGenerating standard plots for {freq}...\")\n",
        "            # Plot feature distributions\n",
        "            df_key = 'train_features_df'; label_col = 'damage_category'\n",
        "            if df_key not in self.data[freq] or self.data[freq][df_key].empty: print(f\" -> Skipping feature dist plot for {freq}: Data missing/empty.\")\n",
        "            else:\n",
        "                 feature_df = self.data[freq][df_key]; plt.figure(figsize=(18, 12)); plt.suptitle(f'Feature Distributions - {freq} (Train Set, Unscaled)', fontsize=16)\n",
        "                 features_to_plot = [col for col in feature_df.columns if col not in ['acquisition_number', label_col]][:9]\n",
        "                 num_plots = len(features_to_plot)\n",
        "                 if num_plots > 0:\n",
        "                      num_cols = 3; num_rows = (num_plots + num_cols - 1) // num_cols; colors = plt.cm.viridis(np.linspace(0, 1, len(self.crack_categories)))\n",
        "                      for i, feature in enumerate(features_to_plot):\n",
        "                           ax = plt.subplot(num_rows, num_cols, i + 1)\n",
        "                           for cat_code, cat_name in self.crack_categories.items():\n",
        "                                subset = feature_df[feature_df[label_col] == cat_code][feature]\n",
        "                                if not subset.empty: sns.histplot(subset, kde=True, label=cat_name, ax=ax, color=colors[cat_code], stat=\"density\", common_norm=False, element=\"step\")\n",
        "                           ax.set_title(f'{feature}'); ax.set_xlabel(''); ax.set_ylabel('Density'); ax.legend()\n",
        "                      plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_feature_distributions_4cat.png')); print(f\" -> Saved feature dist plot for {freq}.\")\n",
        "                 else: print(f\" -> No features to plot distributions for {freq}.\")\n",
        "                 plt.close()\n",
        "            # Plot class distribution in splits\n",
        "            all_counts = {}; splits_exist = False\n",
        "            for split_name in ['train', 'val', 'test']:\n",
        "                 df_key = f'{split_name}_features_df'\n",
        "                 if df_key in self.data[freq] and not self.data[freq][df_key].empty and 'damage_category' in self.data[freq][df_key].columns:\n",
        "                      all_counts[split_name.capitalize()] = self.data[freq][df_key]['damage_category'].value_counts(); splits_exist = True\n",
        "                 else: all_counts[split_name.capitalize()] = pd.Series(dtype=int)\n",
        "            if not splits_exist: print(f\" -> Skipping split dist plot for {freq}: No split data.\")\n",
        "            else:\n",
        "                 count_df = pd.DataFrame(all_counts).fillna(0).astype(int); count_df = count_df.reindex(list(self.crack_categories.keys()), fill_value=0)\n",
        "                 count_df.index = count_df.index.map(self.crack_categories)\n",
        "                 count_df.plot(kind='bar', figsize=(12, 7), width=0.8); plt.ylabel('Count'); plt.xlabel('Damage Category'); plt.title(f'Class Distribution in Splits - {freq}')\n",
        "                 plt.xticks(rotation=0); plt.legend(title='Dataset Split'); plt.tight_layout(); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_class_distribution_split_4cat.png')); print(f\" -> Saved class dist plot for {freq}.\")\n",
        "                 plt.close()\n",
        "        print(\"Standard visualization generation finished.\")\n",
        "\n",
        "    def visualize_feature_importance(self):\n",
        "        \"\"\"Plots feature importances from the Random Forest classifier.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Generating Feature Importance Plots ---\")\n",
        "        for freq in self.frequencies:\n",
        "            imp_key = f'{freq}_crack_classification_rf'\n",
        "            if imp_key in self.feature_importances:\n",
        "                print(f\"Plotting feature importance for {freq}...\")\n",
        "                importances = self.feature_importances[imp_key]\n",
        "                if isinstance(importances, pd.Series) and not importances.empty:\n",
        "                    plt.figure(figsize=(10, 8)); importances.nlargest(20).plot(kind='barh'); plt.title(f'Top 20 Feature Importances (RF Classifier) - {freq}'); plt.xlabel('Importance'); plt.gca().invert_yaxis(); plt.tight_layout(); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_feature_importance.png')); plt.close()\n",
        "                    print(f\" -> Saved feature importance plot for {freq}.\")\n",
        "                else: print(f\" -> Skipping importance plot for {freq}: Data invalid.\")\n",
        "            else: print(f\" -> Skipping importance plot for {freq}: Data not found.\")\n",
        "\n",
        "    def visualize_anomalies(self):\n",
        "        \"\"\"Visualizes anomalies detected on the test set.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Generating Anomaly Detection Plots ---\")\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"Plotting anomalies for {freq}...\")\n",
        "            if freq not in self.evaluation_results.get('anomaly_detection', {}) or 'test_features_df' not in self.data[freq] or self.data[freq]['test_features_df'].empty: print(f\" -> Skipping anomaly plot for {freq}: Missing results/data.\"); continue\n",
        "            results_ad = self.evaluation_results['anomaly_detection'][freq]; test_df_orig = self.data[freq]['test_features_df']\n",
        "            # Plot IF predictions\n",
        "            model_name_if = 'Isolation Forest'\n",
        "            if model_name_if in results_ad and 'predictions' in results_ad[model_name_if]:\n",
        "                if len(results_ad[model_name_if]['predictions']) == len(test_df_orig):\n",
        "                    test_df = test_df_orig.copy(); test_df['anomaly_pred_if'] = results_ad[model_name_if]['predictions']; plt.figure(figsize=(15, 6))\n",
        "                    feature_to_plot = 'torsional_peak_to_peak'; # Example feature\n",
        "                    if feature_to_plot not in test_df.columns: feature_to_plot = test_df.drop(['acquisition_number', 'damage_category', 'anomaly_pred_if'], axis=1).columns[0]\n",
        "                    sns.scatterplot(data=test_df, x='acquisition_number', y=feature_to_plot, hue='anomaly_pred_if', style='damage_category', palette={0: 'blue', 1: 'red'}); plt.title(f'Anomaly Detection ({model_name_if}) vs {feature_to_plot} - {freq}'); plt.xlabel('Acquisition Number'); plt.ylabel(f'{feature_to_plot} (Unscaled)'); plt.legend(title=f'Anomaly (IF) | True Cat'); plt.tight_layout(); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_anomalies_{model_name_if}.png')); plt.close(); print(f\" -> Saved anomaly plot ({model_name_if}) for {freq}.\")\n",
        "                else: print(f\" -> Skipping IF anomaly plot for {freq}: Length mismatch.\")\n",
        "            else: print(f\" -> Skipping IF anomaly plot for {freq}: Predictions not found.\")\n",
        "            # Plot AE Error\n",
        "            model_name_ae = 'Dense Autoencoder'\n",
        "            if model_name_ae in results_ad and 'reconstruction_error' in results_ad[model_name_ae]:\n",
        "                 if len(results_ad[model_name_ae]['reconstruction_error']) == len(test_df_orig):\n",
        "                      test_df = test_df_orig.copy(); test_df['ae_error'] = results_ad[model_name_ae]['reconstruction_error']; threshold = results_ad[model_name_ae].get('Threshold', None)\n",
        "                      plt.figure(figsize=(15, 6)); sns.scatterplot(data=test_df, x='acquisition_number', y='ae_error', hue='damage_category', palette='viridis')\n",
        "                      if threshold is not None: plt.axhline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
        "                      plt.title(f'AE Reconstruction Error - {freq}'); plt.xlabel('Acquisition Number'); plt.ylabel('MSE'); plt.legend(); plt.tight_layout(); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_anomaly_ae_error.png')); plt.close(); print(f\" -> Saved AE error plot for {freq}.\")\n",
        "                 else: print(f\" -> Skipping AE error plot for {freq}: Length mismatch.\")\n",
        "            else: print(f\" -> Skipping AE error plot for {freq}: Errors not found.\")\n",
        "\n",
        "\n",
        "    def visualize_classification_results(self):\n",
        "        \"\"\"Plots confusion matrices for the 4-category classification.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Generating Classification Result Plots ---\")\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"Plotting classification results for {freq}...\")\n",
        "            if freq not in self.evaluation_results.get('crack_classification', {}): print(f\" -> Skipping classification plots for {freq}: Results not found.\"); continue\n",
        "            results_cls = self.evaluation_results['crack_classification'][freq]; labels_present = list(range(len(self.crack_categories))); target_names_present = [self.crack_categories.get(i, str(i)) for i in labels_present]\n",
        "            # Try to get actual labels present from one of the reports if available\n",
        "            if 'Random Forest' in results_cls and isinstance(results_cls['Random Forest'].get('Confusion Matrix Labels'), list): labels_present = results_cls['Random Forest']['Confusion Matrix Labels']; target_names_present = [self.crack_categories.get(i, str(i)) for i in labels_present]\n",
        "            for model_name, results in results_cls.items():\n",
        "                if 'Confusion Matrix' in results and isinstance(results['Confusion Matrix'], list):\n",
        "                    cm = np.array(results['Confusion Matrix']); # Check if list is nested correctly\n",
        "                    if cm.ndim == 2 and cm.size > 0:\n",
        "                         plt.figure(figsize=(8, 6)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_present, yticklabels=target_names_present)\n",
        "                         plt.title(f'Confusion Matrix - {model_name} - {freq}'); plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.tight_layout(); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_classification_CM_{model_name}.png')); plt.close(); print(f\" -> Saved CM plot ({model_name}) for {freq}.\")\n",
        "                    else: print(f\" -> Skipping CM plot for {freq} - {model_name}: Matrix invalid.\")\n",
        "                else: print(f\" -> Skipping CM plot for {freq} - {model_name}: Matrix not found.\")\n",
        "\n",
        "\n",
        "    def visualize_rul_predictions(self):\n",
        "        \"\"\"Plots actual vs predicted RUL over acquisition number for the test set.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Generating RUL Prediction Plots ---\")\n",
        "        for freq in self.frequencies:\n",
        "            print(f\"Plotting RUL predictions for {freq}...\")\n",
        "            if freq not in self.evaluation_results.get('lifetime_prediction', {}).get('RUL', {}) or 'test_features_df' not in self.data[freq] or self.data[freq]['test_features_df'].empty: print(f\" -> Skipping RUL plot for {freq}: Missing results/data.\"); continue\n",
        "            results_rul = self.evaluation_results['lifetime_prediction']['RUL'][freq]; test_df = self.data[freq]['test_features_df'].copy()\n",
        "            test_labels = test_df['damage_category'].values; true_rul = self._generate_synthetic_rul(test_labels); test_df['true_rul'] = true_rul\n",
        "            plt.figure(figsize=(15, 7)); plt.plot(test_df['acquisition_number'], test_df['true_rul'], label='True Synthetic RUL', color='black', linestyle='--', linewidth=2, zorder=1)\n",
        "            plot_exists = False\n",
        "            for model_name, results in results_rul.items():\n",
        "                 if isinstance(results, dict) and 'Predictions' in results:\n",
        "                      predictions = results['Predictions']\n",
        "                      if len(predictions) == len(test_df): plt.scatter(test_df['acquisition_number'], predictions, label=f'Predicted RUL ({model_name})', alpha=0.6, s=10, zorder=2); plot_exists = True\n",
        "                      else: print(f\" -> Warning: RUL prediction length mismatch for {model_name} - {freq}.\")\n",
        "            if not plot_exists: print(f\" -> No RUL predictions found to plot for {freq}.\"); plt.close(); continue\n",
        "            plt.title(f'True vs Predicted Synthetic RUL - {freq}'); plt.xlabel('Acquisition Number'); plt.ylabel('Remaining Useful Life (Synthetic)'); plt.legend(); plt.grid(True, linestyle=':', alpha=0.6); plt.tight_layout(); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_RUL_predictions.png')); plt.close(); print(f\" -> Saved RUL prediction plot for {freq}.\")\n",
        "\n",
        "    def visualize_survival_analysis(self):\n",
        "        \"\"\"Creates visualizations for the Cox Proportional Hazards model.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Creating Survival Analysis Visualizations ---\")\n",
        "        os.makedirs(VIZ_SAVE_DIR, exist_ok=True)\n",
        "        for freq in self.frequencies:\n",
        "             model_key = f'{freq}_coxph'\n",
        "             if model_key not in self.models['lifetime_predictor']: print(f\" -> Skipping survival viz for {freq}: Model not found.\"); continue\n",
        "             print(f\"Generating survival plots for {freq}...\"); cph = self.models['lifetime_predictor'][model_key]\n",
        "             # Plot Coefficients\n",
        "             try: plt.figure(figsize=(10, 6)); cph.plot(); plt.title(f'CoxPH Coefficients - {freq}'); plt.tight_layout(); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_coxph_coefficients.png')); plt.close(); print(f\" -> Saved CoxPH coefficients plot for {freq}.\")\n",
        "             except Exception as e: print(f\"Error plotting CoxPH coeffs for {freq}: {e}\"); plt.close()\n",
        "             # Plot Partial Effects\n",
        "             try:\n",
        "                  summary_df = cph.summary; significant_features = summary_df[summary_df['p'] < 0.05].sort_values(by='coef', key=abs, ascending=False).index.tolist()\n",
        "                  if significant_features:\n",
        "                      num_partial_plots = min(len(significant_features), 3); print(f\"Plotting partial effects for: {significant_features[:num_partial_plots]}\")\n",
        "                      plt.figure(figsize=(7 * num_partial_plots, 6)); cph.plot_partial_effects_on_outcome(covariates=significant_features[:num_partial_plots], values=np.percentile(cph.data[significant_features[:num_partial_plots]], [10, 50, 90], axis=0).T, plot_baseline=True)\n",
        "                      plt.suptitle(f'Partial Effects on Outcome - {freq}'); plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.savefig(os.path.join(VIZ_SAVE_DIR, f'{freq}_coxph_partial_effects.png')); plt.close(); print(f\" -> Saved CoxPH partial effects plot for {freq}.\")\n",
        "                  else: print(f\" -> No significant features (p<0.05) for partial effects plot for {freq}.\")\n",
        "             except Exception as e: print(f\"Error plotting CoxPH partial effects for {freq}: {e}\"); plt.close()\n",
        "        print(\"Survival analysis visualization generation finished.\")\n",
        "\n",
        "\n",
        "    # --- Standard Methods: save_models, load_models ---\n",
        "    def save_models(self, model_dir=MODEL_SAVE_DIR, scaler_dir=SCALER_SAVE_DIR):\n",
        "        \"\"\"Saves trained models and scalers.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Saving Models and Scalers ---\")\n",
        "        os.makedirs(model_dir, exist_ok=True); os.makedirs(scaler_dir, exist_ok=True)\n",
        "        for name, scaler in self.scalers.items():\n",
        "             if scaler: joblib.dump(scaler, f\"{scaler_dir}/{name}_scaler.joblib\"); print(f\"Scaler '{name}' saved.\")\n",
        "        for stage, stage_models in self.models.items():\n",
        "            for name, model in stage_models.items():\n",
        "                save_path = f\"{model_dir}/{name}.model\"\n",
        "                try:\n",
        "                    if isinstance(model, (RandomForestClassifier, RandomForestRegressor, IsolationForest, OneClassSVM, xgb.XGBClassifier, xgb.XGBRegressor, CoxPHFitter)): # Added CoxPHFitter\n",
        "                        joblib.dump(model, save_path + '.joblib'); print(f\"Model '{name}' saved as joblib.\")\n",
        "                    elif isinstance(model, (Model, Sequential)):\n",
        "                        model.save(save_path + '.keras'); print(f\"Model '{name}' saved as Keras format.\")\n",
        "                    else: print(f\"Warning: Model type for '{name}' not recognized for saving.\")\n",
        "                except Exception as e: print(f\"Error saving model {name}: {e}\")\n",
        "        print(\"Model and scaler saving process finished.\")\n",
        "\n",
        "\n",
        "    def load_models(self, model_dir=MODEL_SAVE_DIR, scaler_dir=SCALER_SAVE_DIR):\n",
        "        \"\"\"Loads previously saved models and scalers.\"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        print(\"\\n--- Loading Models and Scalers ---\")\n",
        "        try: # Load Scalers\n",
        "            if os.path.isdir(scaler_dir):\n",
        "                for f in os.listdir(scaler_dir):\n",
        "                     if f.endswith('_scaler.joblib'): name = f.replace('_scaler.joblib', ''); self.scalers[name] = joblib.load(f\"{scaler_dir}/{f}\"); print(f\"Scaler '{name}' loaded.\")\n",
        "            else: print(f\"Scaler directory not found: {scaler_dir}\")\n",
        "        except Exception as e: print(f\"Error loading scalers: {e}\")\n",
        "        try: # Load Models\n",
        "            if os.path.isdir(model_dir):\n",
        "                 for f in os.listdir(model_dir):\n",
        "                      load_path = os.path.join(model_dir, f); model_name = f.replace('.model.joblib', '').replace('.model.keras', '')\n",
        "                      stage = None # Determine stage\n",
        "                      if 'isolation_forest' in model_name or 'ocsvm' in model_name or 'autoencoder' in model_name: stage = 'anomaly_detector'\n",
        "                      elif 'random_forest' in model_name or 'xgboost' in model_name or 'neural_network' in model_name or 'coxph' in model_name: stage = 'lifetime_predictor' if 'lifetime_predictor' in self.models and (f'{model_name}' in self.models['lifetime_predictor'] or 'coxph' in model_name) else 'crack_classifier'\n",
        "                      elif 'lstm' in model_name: stage = 'lifetime_predictor' if 'lifetime_predictor' in self.models and f'{model_name}' in self.models['lifetime_predictor'] else 'anomaly_detector'\n",
        "                      if stage:\n",
        "                           try:\n",
        "                                if f.endswith('.joblib'):\n",
        "                                     loaded_model = joblib.load(load_path); self.models[stage][model_name] = loaded_model\n",
        "                                     model_type = \"CoxPH\" if isinstance(loaded_model, CoxPHFitter) else \"sklearn/xgb/joblib\"\n",
        "                                     print(f\"Model '{model_name}' ({model_type}) loaded for stage '{stage}'.\")\n",
        "                                elif f.endswith('.keras'): self.models[stage][model_name] = load_model(load_path); print(f\"Model '{model_name}' (Keras) loaded for stage '{stage}'.\")\n",
        "                                else: print(f\"Skipping format: {f}\")\n",
        "                           except Exception as e: print(f\"Error loading model {f}: {e}\")\n",
        "            else: print(f\"Model directory not found: {model_dir}\")\n",
        "        except Exception as e: print(f\"Error loading models: {e}\")\n",
        "        print(\"Model and scaler loading process finished.\")\n",
        "\n",
        "\n",
        "    def run_pipeline(self, perform_training=True, perform_evaluation=True, run_survival_analysis=True,\n",
        "                     save_models_on_completion=True, generate_visualizations=True):\n",
        "        \"\"\"\n",
        "        Run the full analysis pipeline with integrated visualizations.\n",
        "        \"\"\"\n",
        "        # --- Function code filled in (same as before) ---\n",
        "        try:\n",
        "             self.load_data()\n",
        "             self.preprocess_features()\n",
        "             if perform_training:\n",
        "                 self.build_anomaly_detector()\n",
        "                 self.build_crack_classifier()\n",
        "                 self.build_lifetime_predictor() # RUL Regressors\n",
        "                 if run_survival_analysis: self.build_survival_model() # CoxPH\n",
        "                 if save_models_on_completion: self.save_models()\n",
        "             else: print(\"\\n--- Skipping Training - Loading Models ---\"); self.load_models()\n",
        "             if perform_evaluation:\n",
        "                 print(\"\\n--- Running Evaluation ---\")\n",
        "                 self.evaluate_anomaly_detector()\n",
        "                 self.evaluate_crack_classifier()\n",
        "                 self.evaluate_lifetime_predictor() # RUL Regressors\n",
        "                 if run_survival_analysis and any('coxph' in k for k in self.models.get('lifetime_predictor',{})): self.evaluate_survival_model() # CoxPH\n",
        "                 print(\"\\n--- Evaluation Summary ---\") # Optional summary\n",
        "             if generate_visualizations:\n",
        "                 print(\"\\n--- Generating All Visualizations ---\")\n",
        "                 self.visualize_data() # Feature/Split Dist\n",
        "                 self.visualize_feature_importance() # RF Importance\n",
        "                 self.visualize_anomalies() # Anomaly Points/Error\n",
        "                 self.visualize_classification_results() # Confusion Matrices\n",
        "                 self.visualize_rul_predictions() # RUL Plot\n",
        "                 if run_survival_analysis and any('coxph' in k for k in self.models.get('lifetime_predictor', {})): self.visualize_survival_analysis() # Cox Plots\n",
        "             print(\"\\n--- Pipeline Run Finished ---\")\n",
        "        except FileNotFoundError as e: print(f\"Pipeline stopped - Missing file: {e}\")\n",
        "        except Exception as e: print(f\"Pipeline stopped - Error: {e}\"); raise\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    frequencies_to_run = ['Frequency_1', 'Frequency_2','Frequency_3', 'Frequency_4','Frequency_5'] # Adjust as needed\n",
        "    pipeline = PipelineCrackAnalysisPipeline( data_dir='./processed_data/', frequencies=frequencies_to_run )\n",
        "    pipeline.run_pipeline( perform_training=True, perform_evaluation=True, run_survival_analysis=True, save_models_on_completion=True, generate_visualizations=True )"
      ],
      "metadata": {
        "id": "zB3X3gDIeiIG",
        "outputId": "cd9d0a05-0032-475d-c100-8f0867c43473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data ---\n",
            "Loading data for Frequency_1...\n",
            " -> Loaded windowed NPY data.\n",
            " -> Loaded feature CSV data.\n",
            " -> Windowed data shapes (Train X/y, Val X/y, Test X/y):\n",
            "    (196, 5, 17), (196,)\n",
            "Loading data for Frequency_2...\n",
            " -> Loaded windowed NPY data.\n",
            " -> Loaded feature CSV data.\n",
            " -> Windowed data shapes (Train X/y, Val X/y, Test X/y):\n",
            "    (196, 5, 17), (196,)\n",
            "Loading data for Frequency_3...\n",
            " -> Loaded windowed NPY data.\n",
            " -> Loaded feature CSV data.\n",
            " -> Windowed data shapes (Train X/y, Val X/y, Test X/y):\n",
            "    (196, 5, 17), (196,)\n",
            "Loading data for Frequency_4...\n",
            " -> Loaded windowed NPY data.\n",
            " -> Loaded feature CSV data.\n",
            " -> Windowed data shapes (Train X/y, Val X/y, Test X/y):\n",
            "    (196, 5, 17), (196,)\n",
            "Loading data for Frequency_5...\n",
            " -> Loaded windowed NPY data.\n",
            " -> Loaded feature CSV data.\n",
            " -> Windowed data shapes (Train X/y, Val X/y, Test X/y):\n",
            "    (196, 5, 17), (196,)\n",
            "Data loaded successfully.\n",
            "\n",
            "--- Feature Preprocessing ---\n",
            "Creating scaled feature set for Frequency_1...\n",
            " -> Fitted feature scaler for Frequency_1 on training data.\n",
            " -> Scaled features created for Frequency_1-train.\n",
            " -> Scaled features created for Frequency_1-val.\n",
            " -> Scaled features created for Frequency_1-test.\n",
            " -> Feature scaling preparation done for Frequency_1.\n",
            "Scaling windowed data for Frequency_1...\n",
            " -> Windowed data scaled for Frequency_1.\n",
            "Creating scaled feature set for Frequency_2...\n",
            " -> Fitted feature scaler for Frequency_2 on training data.\n",
            " -> Scaled features created for Frequency_2-train.\n",
            " -> Scaled features created for Frequency_2-val.\n",
            " -> Scaled features created for Frequency_2-test.\n",
            " -> Feature scaling preparation done for Frequency_2.\n",
            "Scaling windowed data for Frequency_2...\n",
            " -> Windowed data scaled for Frequency_2.\n",
            "Creating scaled feature set for Frequency_3...\n",
            " -> Fitted feature scaler for Frequency_3 on training data.\n",
            " -> Scaled features created for Frequency_3-train.\n",
            " -> Scaled features created for Frequency_3-val.\n",
            " -> Scaled features created for Frequency_3-test.\n",
            " -> Feature scaling preparation done for Frequency_3.\n",
            "Scaling windowed data for Frequency_3...\n",
            " -> Windowed data scaled for Frequency_3.\n",
            "Creating scaled feature set for Frequency_4...\n",
            " -> Fitted feature scaler for Frequency_4 on training data.\n",
            " -> Scaled features created for Frequency_4-train.\n",
            " -> Scaled features created for Frequency_4-val.\n",
            " -> Scaled features created for Frequency_4-test.\n",
            " -> Feature scaling preparation done for Frequency_4.\n",
            "Scaling windowed data for Frequency_4...\n",
            " -> Windowed data scaled for Frequency_4.\n",
            "Creating scaled feature set for Frequency_5...\n",
            " -> Fitted feature scaler for Frequency_5 on training data.\n",
            " -> Scaled features created for Frequency_5-train.\n",
            " -> Scaled features created for Frequency_5-val.\n",
            " -> Scaled features created for Frequency_5-test.\n",
            " -> Feature scaling preparation done for Frequency_5.\n",
            "Scaling windowed data for Frequency_5...\n",
            " -> Windowed data scaled for Frequency_5.\n",
            "Feature preprocessing completed for all frequencies.\n",
            "\n",
            "--- Building Anomaly Detection Models (Stage 1) ---\n",
            "\n",
            "Training anomaly detectors for Frequency_1...\n",
            "Training Isolation Forest...\n",
            " -> IF trained.\n",
            "Training One-Class SVM...\n",
            " -> OCSVM trained.\n",
            "Training Dense Autoencoder...\n",
            " -> Dense AE trained.\n",
            "Training LSTM Autoencoder...\n",
            " -> LSTM AE trained.\n",
            "\n",
            "Training anomaly detectors for Frequency_2...\n",
            "Training Isolation Forest...\n",
            " -> IF trained.\n",
            "Training One-Class SVM...\n",
            " -> OCSVM trained.\n",
            "Training Dense Autoencoder...\n",
            " -> Dense AE trained.\n",
            "Training LSTM Autoencoder...\n",
            " -> LSTM AE trained.\n",
            "\n",
            "Training anomaly detectors for Frequency_3...\n",
            "Training Isolation Forest...\n",
            " -> IF trained.\n",
            "Training One-Class SVM...\n",
            " -> OCSVM trained.\n",
            "Training Dense Autoencoder...\n",
            " -> Dense AE trained.\n",
            "Training LSTM Autoencoder...\n",
            " -> LSTM AE trained.\n",
            "\n",
            "Training anomaly detectors for Frequency_4...\n",
            "Training Isolation Forest...\n",
            " -> IF trained.\n",
            "Training One-Class SVM...\n",
            " -> OCSVM trained.\n",
            "Training Dense Autoencoder...\n",
            " -> Dense AE trained.\n",
            "Training LSTM Autoencoder...\n",
            " -> LSTM AE trained.\n",
            "\n",
            "Training anomaly detectors for Frequency_5...\n",
            "Training Isolation Forest...\n",
            " -> IF trained.\n",
            "Training One-Class SVM...\n",
            " -> OCSVM trained.\n",
            "Training Dense Autoencoder...\n",
            " -> Dense AE trained.\n",
            "Training LSTM Autoencoder...\n",
            " -> LSTM AE trained.\n",
            "\n",
            "Anomaly detection model building completed.\n",
            "\n",
            "--- Building Crack Classification Models (Stage 2) ---\n",
            "\n",
            "Training crack classifiers for Frequency_1...\n",
            "Training label distribution: [181   6   6   7]\n",
            "Validation label distribution: [43  1  1  2]\n",
            "Unique training labels found: [0 1 2 3]\n",
            "Training Random Forest Classifier...\n",
            " -> RF Validation Accuracy: 0.9574\n",
            "Training XGBoost Classifier...\n",
            " -> XGB Validation Accuracy: 0.9362\n",
            "Training Dense Neural Network Classifier...\n",
            " -> NN Validation Accuracy: 0.9149\n",
            "\n",
            "Training crack classifiers for Frequency_2...\n",
            "Training label distribution: [181   6   6   7]\n",
            "Validation label distribution: [43  1  1  2]\n",
            "Unique training labels found: [0 1 2 3]\n",
            "Training Random Forest Classifier...\n",
            " -> RF Validation Accuracy: 0.8936\n",
            "Training XGBoost Classifier...\n",
            " -> XGB Validation Accuracy: 0.8936\n",
            "Training Dense Neural Network Classifier...\n",
            " -> NN Validation Accuracy: 0.9149\n",
            "\n",
            "Training crack classifiers for Frequency_3...\n",
            "Training label distribution: [181   6   6   7]\n",
            "Validation label distribution: [43  1  1  2]\n",
            "Unique training labels found: [0 1 2 3]\n",
            "Training Random Forest Classifier...\n",
            " -> RF Validation Accuracy: 0.9149\n",
            "Training XGBoost Classifier...\n",
            " -> XGB Validation Accuracy: 0.9149\n",
            "Training Dense Neural Network Classifier...\n",
            " -> NN Validation Accuracy: 0.9149\n",
            "\n",
            "Training crack classifiers for Frequency_4...\n",
            "Training label distribution: [181   6   6   7]\n",
            "Validation label distribution: [43  1  1  2]\n",
            "Unique training labels found: [0 1 2 3]\n",
            "Training Random Forest Classifier...\n",
            " -> RF Validation Accuracy: 0.9149\n",
            "Training XGBoost Classifier...\n",
            " -> XGB Validation Accuracy: 0.9149\n",
            "Training Dense Neural Network Classifier...\n",
            " -> NN Validation Accuracy: 0.9149\n",
            "\n",
            "Training crack classifiers for Frequency_5...\n",
            "Training label distribution: [181   6   6   7]\n",
            "Validation label distribution: [43  1  1  2]\n",
            "Unique training labels found: [0 1 2 3]\n",
            "Training Random Forest Classifier...\n",
            " -> RF Validation Accuracy: 0.9149\n",
            "Training XGBoost Classifier...\n",
            " -> XGB Validation Accuracy: 0.9149\n",
            "Training Dense Neural Network Classifier...\n",
            " -> NN Validation Accuracy: 0.9149\n",
            "\n",
            "Crack classification model building completed.\n",
            "\n",
            "--- Building Lifetime Prediction Models (Stage 3 - RUL Regression) ---\n",
            "NOTE: Using synthetic RUL (max=1000, damage_impact=100) for demonstration.\n",
            "\n",
            "Training RUL predictors for Frequency_1...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Training Random Forest Regressor...\n",
            " -> RF Regressor Validation MAE: 25.09\n",
            "Training XGBoost Regressor...\n",
            " -> XGB Regressor Validation MAE: 30.90\n",
            "Training LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Validation MAE: 96.89\n",
            "\n",
            "Training RUL predictors for Frequency_2...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Training Random Forest Regressor...\n",
            " -> RF Regressor Validation MAE: 31.66\n",
            "Training XGBoost Regressor...\n",
            " -> XGB Regressor Validation MAE: 29.72\n",
            "Training LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Validation MAE: 93.28\n",
            "\n",
            "Training RUL predictors for Frequency_3...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Training Random Forest Regressor...\n",
            " -> RF Regressor Validation MAE: 26.22\n",
            "Training XGBoost Regressor...\n",
            " -> XGB Regressor Validation MAE: 28.75\n",
            "Training LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Validation MAE: 187.63\n",
            "\n",
            "Training RUL predictors for Frequency_4...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Training Random Forest Regressor...\n",
            " -> RF Regressor Validation MAE: 25.62\n",
            "Training XGBoost Regressor...\n",
            " -> XGB Regressor Validation MAE: 20.04\n",
            "Training LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Validation MAE: 75.14\n",
            "\n",
            "Training RUL predictors for Frequency_5...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Training Random Forest Regressor...\n",
            " -> RF Regressor Validation MAE: 28.78\n",
            "Training XGBoost Regressor...\n",
            " -> XGB Regressor Validation MAE: 27.40\n",
            "Training LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Validation MAE: 78.64\n",
            "\n",
            "Lifetime prediction model building completed (RUL Regression).\n",
            "\n",
            "--- Building Survival Analysis Model (Stage 3 - CoxPH) ---\n",
            "CAUTION: Using simplified survival data (window acquisition as time, category>0 as event).\n",
            "\n",
            "Training CoxPH model for Frequency_1...\n",
            " -> CoxPH model trained for Frequency_1.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<lifelines.CoxPHFitter: fitted with 200 total observations, 181 right-censored observations>\n",
              "             duration col = 'duration'\n",
              "                event col = 'event'\n",
              "                penalizer = 0.1\n",
              "                 l1 ratio = 0.0\n",
              "      baseline estimation = breslow\n",
              "   number of observations = 200\n",
              "number of events observed = 19\n",
              "   partial log-likelihood = -35.646\n",
              "         time fit was run = 2025-04-02 23:03:48 UTC\n",
              "\n",
              "---\n",
              "                         coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
              "covariate                                                                                                                \n",
              "torsional_mean          0.013     1.013     0.221          -0.420           0.447               0.657               1.564\n",
              "torsional_std           0.114     1.121     0.210          -0.297           0.525               0.743               1.691\n",
              "torsional_max           0.085     1.089     0.209          -0.324           0.495               0.723               1.640\n",
              "torsional_min          -0.067     0.935     0.209          -0.477           0.343               0.621               1.410\n",
              "torsional_peak_to_peak  0.076     1.079     0.209          -0.334           0.487               0.716               1.627\n",
              "torsional_rms           0.114     1.121     0.210          -0.297           0.525               0.743               1.691\n",
              "torsional_kurtosis     -0.153     0.858     0.196          -0.536           0.230               0.585               1.259\n",
              "flexural_mean           0.058     1.060     0.213          -0.360           0.475               0.698               1.609\n",
              "flexural_std            0.092     1.096     0.212          -0.324           0.507               0.723               1.661\n",
              "flexural_max            0.038     1.038     0.210          -0.374           0.450               0.688               1.568\n",
              "flexural_min           -0.029     0.971     0.209          -0.438           0.380               0.645               1.462\n",
              "flexural_peak_to_peak   0.034     1.034     0.210          -0.377           0.444               0.686               1.559\n",
              "flexural_rms            0.092     1.096     0.212          -0.324           0.507               0.723               1.661\n",
              "flexural_kurtosis      -0.135     0.873     0.184          -0.496           0.225               0.609               1.252\n",
              "torsional_energy        0.149     1.161     0.212          -0.266           0.564               0.767               1.758\n",
              "flexural_energy         0.076     1.079     0.215          -0.345           0.498               0.708               1.645\n",
              "energy_ratio           -0.001     0.999     0.200          -0.392           0.391               0.676               1.478\n",
              "\n",
              "                        cmp to      z     p  -log2(p)\n",
              "covariate                                            \n",
              "torsional_mean           0.000  0.060 0.952     0.071\n",
              "torsional_std            0.000  0.545 0.586     0.771\n",
              "torsional_max            0.000  0.408 0.683     0.550\n",
              "torsional_min            0.000 -0.320 0.749     0.416\n",
              "torsional_peak_to_peak   0.000  0.366 0.715     0.485\n",
              "torsional_rms            0.000  0.545 0.586     0.771\n",
              "torsional_kurtosis       0.000 -0.782 0.434     1.203\n",
              "flexural_mean            0.000  0.271 0.786     0.347\n",
              "flexural_std             0.000  0.433 0.665     0.588\n",
              "flexural_max             0.000  0.179 0.858     0.222\n",
              "flexural_min             0.000 -0.140 0.889     0.170\n",
              "flexural_peak_to_peak    0.000  0.160 0.873     0.196\n",
              "flexural_rms             0.000  0.433 0.665     0.588\n",
              "flexural_kurtosis        0.000 -0.737 0.461     1.116\n",
              "torsional_energy         0.000  0.704 0.481     1.055\n",
              "flexural_energy          0.000  0.356 0.722     0.470\n",
              "energy_ratio             0.000 -0.004 0.997     0.004\n",
              "---\n",
              "Concordance = 0.784\n",
              "Partial AIC = 105.291\n",
              "log-likelihood ratio test = 7.388 on 17 df\n",
              "-log2(p) of ll-ratio test = 0.032"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>lifelines.CoxPHFitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration col</th>\n",
              "      <td>'duration'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>event col</th>\n",
              "      <td>'event'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>penalizer</th>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>l1 ratio</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baseline estimation</th>\n",
              "      <td>breslow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of observations</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of events observed</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>partial log-likelihood</th>\n",
              "      <td>-35.646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time fit was run</th>\n",
              "      <td>2025-04-02 23:03:48 UTC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th style=\"min-width: 12px;\"></th>\n",
              "      <th style=\"min-width: 12px;\">coef</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">cmp to</th>\n",
              "      <th style=\"min-width: 12px;\">z</th>\n",
              "      <th style=\"min-width: 12px;\">p</th>\n",
              "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>torsional_mean</th>\n",
              "      <td>0.013</td>\n",
              "      <td>1.013</td>\n",
              "      <td>0.221</td>\n",
              "      <td>-0.420</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.657</td>\n",
              "      <td>1.564</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.952</td>\n",
              "      <td>0.071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_std</th>\n",
              "      <td>0.114</td>\n",
              "      <td>1.121</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.297</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.743</td>\n",
              "      <td>1.691</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_max</th>\n",
              "      <td>0.085</td>\n",
              "      <td>1.089</td>\n",
              "      <td>0.209</td>\n",
              "      <td>-0.324</td>\n",
              "      <td>0.495</td>\n",
              "      <td>0.723</td>\n",
              "      <td>1.640</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.408</td>\n",
              "      <td>0.683</td>\n",
              "      <td>0.550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_min</th>\n",
              "      <td>-0.067</td>\n",
              "      <td>0.935</td>\n",
              "      <td>0.209</td>\n",
              "      <td>-0.477</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.621</td>\n",
              "      <td>1.410</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_peak_to_peak</th>\n",
              "      <td>0.076</td>\n",
              "      <td>1.079</td>\n",
              "      <td>0.209</td>\n",
              "      <td>-0.334</td>\n",
              "      <td>0.487</td>\n",
              "      <td>0.716</td>\n",
              "      <td>1.627</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_rms</th>\n",
              "      <td>0.114</td>\n",
              "      <td>1.121</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.297</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.743</td>\n",
              "      <td>1.691</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_kurtosis</th>\n",
              "      <td>-0.153</td>\n",
              "      <td>0.858</td>\n",
              "      <td>0.196</td>\n",
              "      <td>-0.536</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.585</td>\n",
              "      <td>1.259</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.782</td>\n",
              "      <td>0.434</td>\n",
              "      <td>1.203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_mean</th>\n",
              "      <td>0.058</td>\n",
              "      <td>1.060</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-0.360</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.698</td>\n",
              "      <td>1.609</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.786</td>\n",
              "      <td>0.347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_std</th>\n",
              "      <td>0.092</td>\n",
              "      <td>1.096</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.324</td>\n",
              "      <td>0.507</td>\n",
              "      <td>0.723</td>\n",
              "      <td>1.661</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.665</td>\n",
              "      <td>0.588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_max</th>\n",
              "      <td>0.038</td>\n",
              "      <td>1.038</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.374</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.688</td>\n",
              "      <td>1.568</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.858</td>\n",
              "      <td>0.222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_min</th>\n",
              "      <td>-0.029</td>\n",
              "      <td>0.971</td>\n",
              "      <td>0.209</td>\n",
              "      <td>-0.438</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.645</td>\n",
              "      <td>1.462</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.140</td>\n",
              "      <td>0.889</td>\n",
              "      <td>0.170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_peak_to_peak</th>\n",
              "      <td>0.034</td>\n",
              "      <td>1.034</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.377</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.686</td>\n",
              "      <td>1.559</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.873</td>\n",
              "      <td>0.196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_rms</th>\n",
              "      <td>0.092</td>\n",
              "      <td>1.096</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.324</td>\n",
              "      <td>0.507</td>\n",
              "      <td>0.723</td>\n",
              "      <td>1.661</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.665</td>\n",
              "      <td>0.588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_kurtosis</th>\n",
              "      <td>-0.135</td>\n",
              "      <td>0.873</td>\n",
              "      <td>0.184</td>\n",
              "      <td>-0.496</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.609</td>\n",
              "      <td>1.252</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.737</td>\n",
              "      <td>0.461</td>\n",
              "      <td>1.116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_energy</th>\n",
              "      <td>0.149</td>\n",
              "      <td>1.161</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>0.564</td>\n",
              "      <td>0.767</td>\n",
              "      <td>1.758</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.481</td>\n",
              "      <td>1.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_energy</th>\n",
              "      <td>0.076</td>\n",
              "      <td>1.079</td>\n",
              "      <td>0.215</td>\n",
              "      <td>-0.345</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.708</td>\n",
              "      <td>1.645</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.356</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>energy_ratio</th>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.200</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>0.391</td>\n",
              "      <td>0.676</td>\n",
              "      <td>1.478</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.997</td>\n",
              "      <td>0.004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Concordance</th>\n",
              "      <td>0.784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Partial AIC</th>\n",
              "      <td>105.291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>log-likelihood ratio test</th>\n",
              "      <td>7.388 on 17 df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-log2(p) of ll-ratio test</th>\n",
              "      <td>0.032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/latex": "\\begin{tabular}{lrrrrrrrrrrr}\n & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\ncovariate &  &  &  &  &  &  &  &  &  &  &  \\\\\ntorsional_mean & 0.013 & 1.013 & 0.221 & -0.420 & 0.447 & 0.657 & 1.564 & 0.000 & 0.060 & 0.952 & 0.071 \\\\\ntorsional_std & 0.114 & 1.121 & 0.210 & -0.297 & 0.525 & 0.743 & 1.691 & 0.000 & 0.545 & 0.586 & 0.771 \\\\\ntorsional_max & 0.085 & 1.089 & 0.209 & -0.324 & 0.495 & 0.723 & 1.640 & 0.000 & 0.408 & 0.683 & 0.550 \\\\\ntorsional_min & -0.067 & 0.935 & 0.209 & -0.477 & 0.343 & 0.621 & 1.410 & 0.000 & -0.320 & 0.749 & 0.416 \\\\\ntorsional_peak_to_peak & 0.076 & 1.079 & 0.209 & -0.334 & 0.487 & 0.716 & 1.627 & 0.000 & 0.366 & 0.715 & 0.485 \\\\\ntorsional_rms & 0.114 & 1.121 & 0.210 & -0.297 & 0.525 & 0.743 & 1.691 & 0.000 & 0.545 & 0.586 & 0.771 \\\\\ntorsional_kurtosis & -0.153 & 0.858 & 0.196 & -0.536 & 0.230 & 0.585 & 1.259 & 0.000 & -0.782 & 0.434 & 1.203 \\\\\nflexural_mean & 0.058 & 1.060 & 0.213 & -0.360 & 0.475 & 0.698 & 1.609 & 0.000 & 0.271 & 0.786 & 0.347 \\\\\nflexural_std & 0.092 & 1.096 & 0.212 & -0.324 & 0.507 & 0.723 & 1.661 & 0.000 & 0.433 & 0.665 & 0.588 \\\\\nflexural_max & 0.038 & 1.038 & 0.210 & -0.374 & 0.450 & 0.688 & 1.568 & 0.000 & 0.179 & 0.858 & 0.222 \\\\\nflexural_min & -0.029 & 0.971 & 0.209 & -0.438 & 0.380 & 0.645 & 1.462 & 0.000 & -0.140 & 0.889 & 0.170 \\\\\nflexural_peak_to_peak & 0.034 & 1.034 & 0.210 & -0.377 & 0.444 & 0.686 & 1.559 & 0.000 & 0.160 & 0.873 & 0.196 \\\\\nflexural_rms & 0.092 & 1.096 & 0.212 & -0.324 & 0.507 & 0.723 & 1.661 & 0.000 & 0.433 & 0.665 & 0.588 \\\\\nflexural_kurtosis & -0.135 & 0.873 & 0.184 & -0.496 & 0.225 & 0.609 & 1.252 & 0.000 & -0.737 & 0.461 & 1.116 \\\\\ntorsional_energy & 0.149 & 1.161 & 0.212 & -0.266 & 0.564 & 0.767 & 1.758 & 0.000 & 0.704 & 0.481 & 1.055 \\\\\nflexural_energy & 0.076 & 1.079 & 0.215 & -0.345 & 0.498 & 0.708 & 1.645 & 0.000 & 0.356 & 0.722 & 0.470 \\\\\nenergy_ratio & -0.001 & 0.999 & 0.200 & -0.392 & 0.391 & 0.676 & 1.478 & 0.000 & -0.004 & 0.997 & 0.004 \\\\\n\\end{tabular}\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training CoxPH model for Frequency_2...\n",
            " -> CoxPH model trained for Frequency_2.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<lifelines.CoxPHFitter: fitted with 200 total observations, 181 right-censored observations>\n",
              "             duration col = 'duration'\n",
              "                event col = 'event'\n",
              "                penalizer = 0.1\n",
              "                 l1 ratio = 0.0\n",
              "      baseline estimation = breslow\n",
              "   number of observations = 200\n",
              "number of events observed = 19\n",
              "   partial log-likelihood = -32.095\n",
              "         time fit was run = 2025-04-02 23:03:48 UTC\n",
              "\n",
              "---\n",
              "                         coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
              "covariate                                                                                                                \n",
              "torsional_mean         -0.004     0.996     0.184          -0.365           0.356               0.694               1.428\n",
              "torsional_std           0.139     1.149     0.212          -0.277           0.554               0.758               1.741\n",
              "torsional_max           0.027     1.028     0.205          -0.374           0.429               0.688               1.535\n",
              "torsional_min          -0.052     0.950     0.206          -0.455           0.352               0.634               1.422\n",
              "torsional_peak_to_peak  0.040     1.040     0.205          -0.363           0.442               0.696               1.556\n",
              "torsional_rms           0.139     1.149     0.212          -0.277           0.554               0.758               1.741\n",
              "torsional_kurtosis     -0.190     0.827     0.196          -0.574           0.194               0.563               1.214\n",
              "flexural_mean          -0.086     0.917     0.193          -0.465           0.292               0.628               1.339\n",
              "flexural_std            0.077     1.080     0.215          -0.345           0.499               0.708               1.648\n",
              "flexural_max           -0.007     0.993     0.217          -0.431           0.418               0.650               1.519\n",
              "flexural_min            0.002     1.002     0.217          -0.424           0.427               0.655               1.532\n",
              "flexural_peak_to_peak  -0.004     0.996     0.217          -0.429           0.421               0.651               1.523\n",
              "flexural_rms            0.077     1.080     0.215          -0.345           0.499               0.708               1.648\n",
              "flexural_kurtosis      -0.300     0.741     0.184          -0.662           0.061               0.516               1.063\n",
              "torsional_energy        0.163     1.177     0.213          -0.255           0.581               0.775               1.787\n",
              "flexural_energy         0.069     1.072     0.217          -0.357           0.495               0.700               1.641\n",
              "energy_ratio            0.050     1.051     0.200          -0.342           0.442               0.710               1.555\n",
              "\n",
              "                        cmp to      z     p  -log2(p)\n",
              "covariate                                            \n",
              "torsional_mean           0.000 -0.024 0.981     0.027\n",
              "torsional_std            0.000  0.654 0.513     0.962\n",
              "torsional_max            0.000  0.133 0.894     0.162\n",
              "torsional_min            0.000 -0.252 0.801     0.319\n",
              "torsional_peak_to_peak   0.000  0.193 0.847     0.240\n",
              "torsional_rms            0.000  0.654 0.513     0.962\n",
              "torsional_kurtosis       0.000 -0.971 0.331     1.593\n",
              "flexural_mean            0.000 -0.447 0.655     0.611\n",
              "flexural_std             0.000  0.358 0.720     0.474\n",
              "flexural_max             0.000 -0.030 0.976     0.035\n",
              "flexural_min             0.000  0.007 0.994     0.008\n",
              "flexural_peak_to_peak    0.000 -0.019 0.985     0.022\n",
              "flexural_rms             0.000  0.358 0.720     0.474\n",
              "flexural_kurtosis        0.000 -1.627 0.104     3.269\n",
              "torsional_energy         0.000  0.764 0.445     1.168\n",
              "flexural_energy          0.000  0.320 0.749     0.416\n",
              "energy_ratio             0.000  0.249 0.803     0.316\n",
              "---\n",
              "Concordance = 0.825\n",
              "Partial AIC = 98.189\n",
              "log-likelihood ratio test = 14.491 on 17 df\n",
              "-log2(p) of ll-ratio test = 0.662"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>lifelines.CoxPHFitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration col</th>\n",
              "      <td>'duration'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>event col</th>\n",
              "      <td>'event'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>penalizer</th>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>l1 ratio</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baseline estimation</th>\n",
              "      <td>breslow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of observations</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of events observed</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>partial log-likelihood</th>\n",
              "      <td>-32.095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time fit was run</th>\n",
              "      <td>2025-04-02 23:03:48 UTC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th style=\"min-width: 12px;\"></th>\n",
              "      <th style=\"min-width: 12px;\">coef</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">cmp to</th>\n",
              "      <th style=\"min-width: 12px;\">z</th>\n",
              "      <th style=\"min-width: 12px;\">p</th>\n",
              "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>torsional_mean</th>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.184</td>\n",
              "      <td>-0.365</td>\n",
              "      <td>0.356</td>\n",
              "      <td>0.694</td>\n",
              "      <td>1.428</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>0.981</td>\n",
              "      <td>0.027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_std</th>\n",
              "      <td>0.139</td>\n",
              "      <td>1.149</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.758</td>\n",
              "      <td>1.741</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.654</td>\n",
              "      <td>0.513</td>\n",
              "      <td>0.962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_max</th>\n",
              "      <td>0.027</td>\n",
              "      <td>1.028</td>\n",
              "      <td>0.205</td>\n",
              "      <td>-0.374</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.688</td>\n",
              "      <td>1.535</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.894</td>\n",
              "      <td>0.162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_min</th>\n",
              "      <td>-0.052</td>\n",
              "      <td>0.950</td>\n",
              "      <td>0.206</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.634</td>\n",
              "      <td>1.422</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.252</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_peak_to_peak</th>\n",
              "      <td>0.040</td>\n",
              "      <td>1.040</td>\n",
              "      <td>0.205</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.696</td>\n",
              "      <td>1.556</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_rms</th>\n",
              "      <td>0.139</td>\n",
              "      <td>1.149</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.758</td>\n",
              "      <td>1.741</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.654</td>\n",
              "      <td>0.513</td>\n",
              "      <td>0.962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_kurtosis</th>\n",
              "      <td>-0.190</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.196</td>\n",
              "      <td>-0.574</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.563</td>\n",
              "      <td>1.214</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.971</td>\n",
              "      <td>0.331</td>\n",
              "      <td>1.593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_mean</th>\n",
              "      <td>-0.086</td>\n",
              "      <td>0.917</td>\n",
              "      <td>0.193</td>\n",
              "      <td>-0.465</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.628</td>\n",
              "      <td>1.339</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.447</td>\n",
              "      <td>0.655</td>\n",
              "      <td>0.611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_std</th>\n",
              "      <td>0.077</td>\n",
              "      <td>1.080</td>\n",
              "      <td>0.215</td>\n",
              "      <td>-0.345</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.708</td>\n",
              "      <td>1.648</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_max</th>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.993</td>\n",
              "      <td>0.217</td>\n",
              "      <td>-0.431</td>\n",
              "      <td>0.418</td>\n",
              "      <td>0.650</td>\n",
              "      <td>1.519</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.030</td>\n",
              "      <td>0.976</td>\n",
              "      <td>0.035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_min</th>\n",
              "      <td>0.002</td>\n",
              "      <td>1.002</td>\n",
              "      <td>0.217</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.655</td>\n",
              "      <td>1.532</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.994</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_peak_to_peak</th>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.217</td>\n",
              "      <td>-0.429</td>\n",
              "      <td>0.421</td>\n",
              "      <td>0.651</td>\n",
              "      <td>1.523</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.019</td>\n",
              "      <td>0.985</td>\n",
              "      <td>0.022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_rms</th>\n",
              "      <td>0.077</td>\n",
              "      <td>1.080</td>\n",
              "      <td>0.215</td>\n",
              "      <td>-0.345</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.708</td>\n",
              "      <td>1.648</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_kurtosis</th>\n",
              "      <td>-0.300</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.184</td>\n",
              "      <td>-0.662</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.516</td>\n",
              "      <td>1.063</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-1.627</td>\n",
              "      <td>0.104</td>\n",
              "      <td>3.269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_energy</th>\n",
              "      <td>0.163</td>\n",
              "      <td>1.177</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>0.581</td>\n",
              "      <td>0.775</td>\n",
              "      <td>1.787</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.445</td>\n",
              "      <td>1.168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_energy</th>\n",
              "      <td>0.069</td>\n",
              "      <td>1.072</td>\n",
              "      <td>0.217</td>\n",
              "      <td>-0.357</td>\n",
              "      <td>0.495</td>\n",
              "      <td>0.700</td>\n",
              "      <td>1.641</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>energy_ratio</th>\n",
              "      <td>0.050</td>\n",
              "      <td>1.051</td>\n",
              "      <td>0.200</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.710</td>\n",
              "      <td>1.555</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.803</td>\n",
              "      <td>0.316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Concordance</th>\n",
              "      <td>0.825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Partial AIC</th>\n",
              "      <td>98.189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>log-likelihood ratio test</th>\n",
              "      <td>14.491 on 17 df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-log2(p) of ll-ratio test</th>\n",
              "      <td>0.662</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/latex": "\\begin{tabular}{lrrrrrrrrrrr}\n & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\ncovariate &  &  &  &  &  &  &  &  &  &  &  \\\\\ntorsional_mean & -0.004 & 0.996 & 0.184 & -0.365 & 0.356 & 0.694 & 1.428 & 0.000 & -0.024 & 0.981 & 0.027 \\\\\ntorsional_std & 0.139 & 1.149 & 0.212 & -0.277 & 0.554 & 0.758 & 1.741 & 0.000 & 0.654 & 0.513 & 0.962 \\\\\ntorsional_max & 0.027 & 1.028 & 0.205 & -0.374 & 0.429 & 0.688 & 1.535 & 0.000 & 0.133 & 0.894 & 0.162 \\\\\ntorsional_min & -0.052 & 0.950 & 0.206 & -0.455 & 0.352 & 0.634 & 1.422 & 0.000 & -0.252 & 0.801 & 0.319 \\\\\ntorsional_peak_to_peak & 0.040 & 1.040 & 0.205 & -0.363 & 0.442 & 0.696 & 1.556 & 0.000 & 0.193 & 0.847 & 0.240 \\\\\ntorsional_rms & 0.139 & 1.149 & 0.212 & -0.277 & 0.554 & 0.758 & 1.741 & 0.000 & 0.654 & 0.513 & 0.962 \\\\\ntorsional_kurtosis & -0.190 & 0.827 & 0.196 & -0.574 & 0.194 & 0.563 & 1.214 & 0.000 & -0.971 & 0.331 & 1.593 \\\\\nflexural_mean & -0.086 & 0.917 & 0.193 & -0.465 & 0.292 & 0.628 & 1.339 & 0.000 & -0.447 & 0.655 & 0.611 \\\\\nflexural_std & 0.077 & 1.080 & 0.215 & -0.345 & 0.499 & 0.708 & 1.648 & 0.000 & 0.358 & 0.720 & 0.474 \\\\\nflexural_max & -0.007 & 0.993 & 0.217 & -0.431 & 0.418 & 0.650 & 1.519 & 0.000 & -0.030 & 0.976 & 0.035 \\\\\nflexural_min & 0.002 & 1.002 & 0.217 & -0.424 & 0.427 & 0.655 & 1.532 & 0.000 & 0.007 & 0.994 & 0.008 \\\\\nflexural_peak_to_peak & -0.004 & 0.996 & 0.217 & -0.429 & 0.421 & 0.651 & 1.523 & 0.000 & -0.019 & 0.985 & 0.022 \\\\\nflexural_rms & 0.077 & 1.080 & 0.215 & -0.345 & 0.499 & 0.708 & 1.648 & 0.000 & 0.358 & 0.720 & 0.474 \\\\\nflexural_kurtosis & -0.300 & 0.741 & 0.184 & -0.662 & 0.061 & 0.516 & 1.063 & 0.000 & -1.627 & 0.104 & 3.269 \\\\\ntorsional_energy & 0.163 & 1.177 & 0.213 & -0.255 & 0.581 & 0.775 & 1.787 & 0.000 & 0.764 & 0.445 & 1.168 \\\\\nflexural_energy & 0.069 & 1.072 & 0.217 & -0.357 & 0.495 & 0.700 & 1.641 & 0.000 & 0.320 & 0.749 & 0.416 \\\\\nenergy_ratio & 0.050 & 1.051 & 0.200 & -0.342 & 0.442 & 0.710 & 1.555 & 0.000 & 0.249 & 0.803 & 0.316 \\\\\n\\end{tabular}\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training CoxPH model for Frequency_3...\n",
            " -> CoxPH model trained for Frequency_3.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<lifelines.CoxPHFitter: fitted with 200 total observations, 181 right-censored observations>\n",
              "             duration col = 'duration'\n",
              "                event col = 'event'\n",
              "                penalizer = 0.1\n",
              "                 l1 ratio = 0.0\n",
              "      baseline estimation = breslow\n",
              "   number of observations = 200\n",
              "number of events observed = 19\n",
              "   partial log-likelihood = -38.843\n",
              "         time fit was run = 2025-04-02 23:03:48 UTC\n",
              "\n",
              "---\n",
              "                         coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
              "covariate                                                                                                                \n",
              "torsional_mean         -0.040     0.960     0.222          -0.476           0.396               0.621               1.485\n",
              "torsional_std           0.012     1.013     0.216          -0.412           0.437               0.663               1.547\n",
              "torsional_max           0.027     1.027     0.210          -0.385           0.439               0.680               1.550\n",
              "torsional_min          -0.037     0.963     0.210          -0.450           0.375               0.638               1.455\n",
              "torsional_peak_to_peak  0.032     1.032     0.210          -0.381           0.444               0.683               1.559\n",
              "torsional_rms           0.012     1.013     0.216          -0.412           0.437               0.663               1.547\n",
              "torsional_kurtosis      0.089     1.093     0.198          -0.299           0.477               0.741               1.610\n",
              "flexural_mean           0.109     1.115     0.214          -0.311           0.529               0.733               1.697\n",
              "flexural_std            0.024     1.024     0.221          -0.410           0.457               0.664               1.580\n",
              "flexural_max            0.025     1.026     0.219          -0.404           0.455               0.668               1.576\n",
              "flexural_min           -0.028     0.972     0.219          -0.458           0.402               0.633               1.495\n",
              "flexural_peak_to_peak   0.027     1.027     0.219          -0.403           0.456               0.668               1.578\n",
              "flexural_rms            0.024     1.024     0.221          -0.410           0.457               0.664               1.580\n",
              "flexural_kurtosis       0.038     1.039     0.202          -0.357           0.434               0.700               1.543\n",
              "torsional_energy       -0.001     0.999     0.220          -0.433           0.431               0.649               1.539\n",
              "flexural_energy         0.011     1.011     0.223          -0.425           0.448               0.653               1.565\n",
              "energy_ratio           -0.008     0.992     0.207          -0.414           0.397               0.661               1.487\n",
              "\n",
              "                        cmp to      z     p  -log2(p)\n",
              "covariate                                            \n",
              "torsional_mean           0.000 -0.182 0.856     0.225\n",
              "torsional_std            0.000  0.058 0.954     0.068\n",
              "torsional_max            0.000  0.126 0.900     0.153\n",
              "torsional_min            0.000 -0.177 0.859     0.219\n",
              "torsional_peak_to_peak   0.000  0.151 0.880     0.185\n",
              "torsional_rms            0.000  0.058 0.954     0.068\n",
              "torsional_kurtosis       0.000  0.448 0.654     0.613\n",
              "flexural_mean            0.000  0.508 0.611     0.710\n",
              "flexural_std             0.000  0.107 0.914     0.129\n",
              "flexural_max             0.000  0.116 0.908     0.140\n",
              "flexural_min             0.000 -0.128 0.898     0.155\n",
              "flexural_peak_to_peak    0.000  0.122 0.903     0.147\n",
              "flexural_rms             0.000  0.107 0.914     0.129\n",
              "flexural_kurtosis        0.000  0.191 0.849     0.236\n",
              "torsional_energy         0.000 -0.004 0.997     0.005\n",
              "flexural_energy          0.000  0.051 0.959     0.060\n",
              "energy_ratio             0.000 -0.041 0.968     0.048\n",
              "---\n",
              "Concordance = 0.573\n",
              "Partial AIC = 111.687\n",
              "log-likelihood ratio test = 0.993 on 17 df\n",
              "-log2(p) of ll-ratio test = 0.000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>lifelines.CoxPHFitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration col</th>\n",
              "      <td>'duration'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>event col</th>\n",
              "      <td>'event'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>penalizer</th>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>l1 ratio</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baseline estimation</th>\n",
              "      <td>breslow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of observations</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of events observed</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>partial log-likelihood</th>\n",
              "      <td>-38.843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time fit was run</th>\n",
              "      <td>2025-04-02 23:03:48 UTC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th style=\"min-width: 12px;\"></th>\n",
              "      <th style=\"min-width: 12px;\">coef</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">cmp to</th>\n",
              "      <th style=\"min-width: 12px;\">z</th>\n",
              "      <th style=\"min-width: 12px;\">p</th>\n",
              "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>torsional_mean</th>\n",
              "      <td>-0.040</td>\n",
              "      <td>0.960</td>\n",
              "      <td>0.222</td>\n",
              "      <td>-0.476</td>\n",
              "      <td>0.396</td>\n",
              "      <td>0.621</td>\n",
              "      <td>1.485</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.182</td>\n",
              "      <td>0.856</td>\n",
              "      <td>0.225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_std</th>\n",
              "      <td>0.012</td>\n",
              "      <td>1.013</td>\n",
              "      <td>0.216</td>\n",
              "      <td>-0.412</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.663</td>\n",
              "      <td>1.547</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.954</td>\n",
              "      <td>0.068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_max</th>\n",
              "      <td>0.027</td>\n",
              "      <td>1.027</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.385</td>\n",
              "      <td>0.439</td>\n",
              "      <td>0.680</td>\n",
              "      <td>1.550</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_min</th>\n",
              "      <td>-0.037</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.450</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.638</td>\n",
              "      <td>1.455</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.177</td>\n",
              "      <td>0.859</td>\n",
              "      <td>0.219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_peak_to_peak</th>\n",
              "      <td>0.032</td>\n",
              "      <td>1.032</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.381</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.683</td>\n",
              "      <td>1.559</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_rms</th>\n",
              "      <td>0.012</td>\n",
              "      <td>1.013</td>\n",
              "      <td>0.216</td>\n",
              "      <td>-0.412</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.663</td>\n",
              "      <td>1.547</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.954</td>\n",
              "      <td>0.068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_kurtosis</th>\n",
              "      <td>0.089</td>\n",
              "      <td>1.093</td>\n",
              "      <td>0.198</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.741</td>\n",
              "      <td>1.610</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.654</td>\n",
              "      <td>0.613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_mean</th>\n",
              "      <td>0.109</td>\n",
              "      <td>1.115</td>\n",
              "      <td>0.214</td>\n",
              "      <td>-0.311</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.733</td>\n",
              "      <td>1.697</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_std</th>\n",
              "      <td>0.024</td>\n",
              "      <td>1.024</td>\n",
              "      <td>0.221</td>\n",
              "      <td>-0.410</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.664</td>\n",
              "      <td>1.580</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.914</td>\n",
              "      <td>0.129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_max</th>\n",
              "      <td>0.025</td>\n",
              "      <td>1.026</td>\n",
              "      <td>0.219</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.668</td>\n",
              "      <td>1.576</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.908</td>\n",
              "      <td>0.140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_min</th>\n",
              "      <td>-0.028</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.219</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>0.402</td>\n",
              "      <td>0.633</td>\n",
              "      <td>1.495</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_peak_to_peak</th>\n",
              "      <td>0.027</td>\n",
              "      <td>1.027</td>\n",
              "      <td>0.219</td>\n",
              "      <td>-0.403</td>\n",
              "      <td>0.456</td>\n",
              "      <td>0.668</td>\n",
              "      <td>1.578</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.903</td>\n",
              "      <td>0.147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_rms</th>\n",
              "      <td>0.024</td>\n",
              "      <td>1.024</td>\n",
              "      <td>0.221</td>\n",
              "      <td>-0.410</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.664</td>\n",
              "      <td>1.580</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.914</td>\n",
              "      <td>0.129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_kurtosis</th>\n",
              "      <td>0.038</td>\n",
              "      <td>1.039</td>\n",
              "      <td>0.202</td>\n",
              "      <td>-0.357</td>\n",
              "      <td>0.434</td>\n",
              "      <td>0.700</td>\n",
              "      <td>1.543</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.849</td>\n",
              "      <td>0.236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_energy</th>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.220</td>\n",
              "      <td>-0.433</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.649</td>\n",
              "      <td>1.539</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.004</td>\n",
              "      <td>0.997</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_energy</th>\n",
              "      <td>0.011</td>\n",
              "      <td>1.011</td>\n",
              "      <td>0.223</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.653</td>\n",
              "      <td>1.565</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.959</td>\n",
              "      <td>0.060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>energy_ratio</th>\n",
              "      <td>-0.008</td>\n",
              "      <td>0.992</td>\n",
              "      <td>0.207</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.661</td>\n",
              "      <td>1.487</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.041</td>\n",
              "      <td>0.968</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Concordance</th>\n",
              "      <td>0.573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Partial AIC</th>\n",
              "      <td>111.687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>log-likelihood ratio test</th>\n",
              "      <td>0.993 on 17 df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-log2(p) of ll-ratio test</th>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/latex": "\\begin{tabular}{lrrrrrrrrrrr}\n & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\ncovariate &  &  &  &  &  &  &  &  &  &  &  \\\\\ntorsional_mean & -0.040 & 0.960 & 0.222 & -0.476 & 0.396 & 0.621 & 1.485 & 0.000 & -0.182 & 0.856 & 0.225 \\\\\ntorsional_std & 0.012 & 1.013 & 0.216 & -0.412 & 0.437 & 0.663 & 1.547 & 0.000 & 0.058 & 0.954 & 0.068 \\\\\ntorsional_max & 0.027 & 1.027 & 0.210 & -0.385 & 0.439 & 0.680 & 1.550 & 0.000 & 0.126 & 0.900 & 0.153 \\\\\ntorsional_min & -0.037 & 0.963 & 0.210 & -0.450 & 0.375 & 0.638 & 1.455 & 0.000 & -0.177 & 0.859 & 0.219 \\\\\ntorsional_peak_to_peak & 0.032 & 1.032 & 0.210 & -0.381 & 0.444 & 0.683 & 1.559 & 0.000 & 0.151 & 0.880 & 0.185 \\\\\ntorsional_rms & 0.012 & 1.013 & 0.216 & -0.412 & 0.437 & 0.663 & 1.547 & 0.000 & 0.058 & 0.954 & 0.068 \\\\\ntorsional_kurtosis & 0.089 & 1.093 & 0.198 & -0.299 & 0.477 & 0.741 & 1.610 & 0.000 & 0.448 & 0.654 & 0.613 \\\\\nflexural_mean & 0.109 & 1.115 & 0.214 & -0.311 & 0.529 & 0.733 & 1.697 & 0.000 & 0.508 & 0.611 & 0.710 \\\\\nflexural_std & 0.024 & 1.024 & 0.221 & -0.410 & 0.457 & 0.664 & 1.580 & 0.000 & 0.107 & 0.914 & 0.129 \\\\\nflexural_max & 0.025 & 1.026 & 0.219 & -0.404 & 0.455 & 0.668 & 1.576 & 0.000 & 0.116 & 0.908 & 0.140 \\\\\nflexural_min & -0.028 & 0.972 & 0.219 & -0.458 & 0.402 & 0.633 & 1.495 & 0.000 & -0.128 & 0.898 & 0.155 \\\\\nflexural_peak_to_peak & 0.027 & 1.027 & 0.219 & -0.403 & 0.456 & 0.668 & 1.578 & 0.000 & 0.122 & 0.903 & 0.147 \\\\\nflexural_rms & 0.024 & 1.024 & 0.221 & -0.410 & 0.457 & 0.664 & 1.580 & 0.000 & 0.107 & 0.914 & 0.129 \\\\\nflexural_kurtosis & 0.038 & 1.039 & 0.202 & -0.357 & 0.434 & 0.700 & 1.543 & 0.000 & 0.191 & 0.849 & 0.236 \\\\\ntorsional_energy & -0.001 & 0.999 & 0.220 & -0.433 & 0.431 & 0.649 & 1.539 & 0.000 & -0.004 & 0.997 & 0.005 \\\\\nflexural_energy & 0.011 & 1.011 & 0.223 & -0.425 & 0.448 & 0.653 & 1.565 & 0.000 & 0.051 & 0.959 & 0.060 \\\\\nenergy_ratio & -0.008 & 0.992 & 0.207 & -0.414 & 0.397 & 0.661 & 1.487 & 0.000 & -0.041 & 0.968 & 0.048 \\\\\n\\end{tabular}\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training CoxPH model for Frequency_4...\n",
            " -> CoxPH model trained for Frequency_4.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<lifelines.CoxPHFitter: fitted with 200 total observations, 181 right-censored observations>\n",
              "             duration col = 'duration'\n",
              "                event col = 'event'\n",
              "                penalizer = 0.1\n",
              "                 l1 ratio = 0.0\n",
              "      baseline estimation = breslow\n",
              "   number of observations = 200\n",
              "number of events observed = 19\n",
              "   partial log-likelihood = -34.886\n",
              "         time fit was run = 2025-04-02 23:03:48 UTC\n",
              "\n",
              "---\n",
              "                         coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
              "covariate                                                                                                                \n",
              "torsional_mean          0.088     1.092     0.173          -0.250           0.426               0.779               1.531\n",
              "torsional_std          -0.036     0.965     0.208          -0.444           0.372               0.641               1.451\n",
              "torsional_max           0.110     1.116     0.214          -0.310           0.530               0.733               1.699\n",
              "torsional_min          -0.084     0.920     0.214          -0.503           0.336               0.605               1.399\n",
              "torsional_peak_to_peak  0.097     1.102     0.214          -0.323           0.517               0.724               1.677\n",
              "torsional_rms          -0.036     0.965     0.208          -0.444           0.372               0.641               1.451\n",
              "torsional_kurtosis      0.184     1.202     0.204          -0.215           0.583               0.807               1.792\n",
              "flexural_mean           0.053     1.055     0.150          -0.241           0.348               0.786               1.417\n",
              "flexural_std           -0.079     0.924     0.207          -0.485           0.328               0.616               1.388\n",
              "flexural_max            0.009     1.009     0.213          -0.408           0.425               0.665               1.530\n",
              "flexural_min           -0.027     0.973     0.213          -0.446           0.391               0.640               1.478\n",
              "flexural_peak_to_peak   0.018     1.018     0.213          -0.400           0.435               0.671               1.545\n",
              "flexural_rms           -0.079     0.924     0.207          -0.485           0.328               0.616               1.388\n",
              "flexural_kurtosis       0.240     1.272     0.186          -0.124           0.605               0.883               1.831\n",
              "torsional_energy       -0.085     0.919     0.198          -0.473           0.304               0.623               1.355\n",
              "flexural_energy        -0.046     0.955     0.219          -0.475           0.383               0.622               1.466\n",
              "energy_ratio            0.142     1.153     0.202          -0.254           0.538               0.776               1.713\n",
              "\n",
              "                        cmp to      z     p  -log2(p)\n",
              "covariate                                            \n",
              "torsional_mean           0.000  0.510 0.610     0.713\n",
              "torsional_std            0.000 -0.173 0.863     0.213\n",
              "torsional_max            0.000  0.512 0.609     0.716\n",
              "torsional_min            0.000 -0.391 0.696     0.523\n",
              "torsional_peak_to_peak   0.000  0.452 0.651     0.619\n",
              "torsional_rms            0.000 -0.173 0.863     0.213\n",
              "torsional_kurtosis       0.000  0.904 0.366     1.450\n",
              "flexural_mean            0.000  0.356 0.722     0.470\n",
              "flexural_std             0.000 -0.379 0.705     0.505\n",
              "flexural_max             0.000  0.042 0.966     0.049\n",
              "flexural_min             0.000 -0.128 0.898     0.155\n",
              "flexural_peak_to_peak    0.000  0.084 0.933     0.100\n",
              "flexural_rms             0.000 -0.379 0.705     0.505\n",
              "flexural_kurtosis        0.000  1.292 0.196     2.349\n",
              "torsional_energy         0.000 -0.426 0.670     0.578\n",
              "flexural_energy          0.000 -0.211 0.833     0.263\n",
              "energy_ratio             0.000  0.704 0.481     1.055\n",
              "---\n",
              "Concordance = 0.766\n",
              "Partial AIC = 103.771\n",
              "log-likelihood ratio test = 8.909 on 17 df\n",
              "-log2(p) of ll-ratio test = 0.085"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>lifelines.CoxPHFitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration col</th>\n",
              "      <td>'duration'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>event col</th>\n",
              "      <td>'event'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>penalizer</th>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>l1 ratio</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baseline estimation</th>\n",
              "      <td>breslow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of observations</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of events observed</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>partial log-likelihood</th>\n",
              "      <td>-34.886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time fit was run</th>\n",
              "      <td>2025-04-02 23:03:48 UTC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th style=\"min-width: 12px;\"></th>\n",
              "      <th style=\"min-width: 12px;\">coef</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">cmp to</th>\n",
              "      <th style=\"min-width: 12px;\">z</th>\n",
              "      <th style=\"min-width: 12px;\">p</th>\n",
              "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>torsional_mean</th>\n",
              "      <td>0.088</td>\n",
              "      <td>1.092</td>\n",
              "      <td>0.173</td>\n",
              "      <td>-0.250</td>\n",
              "      <td>0.426</td>\n",
              "      <td>0.779</td>\n",
              "      <td>1.531</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.610</td>\n",
              "      <td>0.713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_std</th>\n",
              "      <td>-0.036</td>\n",
              "      <td>0.965</td>\n",
              "      <td>0.208</td>\n",
              "      <td>-0.444</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.641</td>\n",
              "      <td>1.451</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.173</td>\n",
              "      <td>0.863</td>\n",
              "      <td>0.213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_max</th>\n",
              "      <td>0.110</td>\n",
              "      <td>1.116</td>\n",
              "      <td>0.214</td>\n",
              "      <td>-0.310</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.733</td>\n",
              "      <td>1.699</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.609</td>\n",
              "      <td>0.716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_min</th>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.920</td>\n",
              "      <td>0.214</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.605</td>\n",
              "      <td>1.399</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.391</td>\n",
              "      <td>0.696</td>\n",
              "      <td>0.523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_peak_to_peak</th>\n",
              "      <td>0.097</td>\n",
              "      <td>1.102</td>\n",
              "      <td>0.214</td>\n",
              "      <td>-0.323</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.724</td>\n",
              "      <td>1.677</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_rms</th>\n",
              "      <td>-0.036</td>\n",
              "      <td>0.965</td>\n",
              "      <td>0.208</td>\n",
              "      <td>-0.444</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.641</td>\n",
              "      <td>1.451</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.173</td>\n",
              "      <td>0.863</td>\n",
              "      <td>0.213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_kurtosis</th>\n",
              "      <td>0.184</td>\n",
              "      <td>1.202</td>\n",
              "      <td>0.204</td>\n",
              "      <td>-0.215</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.807</td>\n",
              "      <td>1.792</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.904</td>\n",
              "      <td>0.366</td>\n",
              "      <td>1.450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_mean</th>\n",
              "      <td>0.053</td>\n",
              "      <td>1.055</td>\n",
              "      <td>0.150</td>\n",
              "      <td>-0.241</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.786</td>\n",
              "      <td>1.417</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.356</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_std</th>\n",
              "      <td>-0.079</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0.207</td>\n",
              "      <td>-0.485</td>\n",
              "      <td>0.328</td>\n",
              "      <td>0.616</td>\n",
              "      <td>1.388</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.379</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_max</th>\n",
              "      <td>0.009</td>\n",
              "      <td>1.009</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-0.408</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.665</td>\n",
              "      <td>1.530</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_min</th>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>0.391</td>\n",
              "      <td>0.640</td>\n",
              "      <td>1.478</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_peak_to_peak</th>\n",
              "      <td>0.018</td>\n",
              "      <td>1.018</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-0.400</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.671</td>\n",
              "      <td>1.545</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.933</td>\n",
              "      <td>0.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_rms</th>\n",
              "      <td>-0.079</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0.207</td>\n",
              "      <td>-0.485</td>\n",
              "      <td>0.328</td>\n",
              "      <td>0.616</td>\n",
              "      <td>1.388</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.379</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_kurtosis</th>\n",
              "      <td>0.240</td>\n",
              "      <td>1.272</td>\n",
              "      <td>0.186</td>\n",
              "      <td>-0.124</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.883</td>\n",
              "      <td>1.831</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.292</td>\n",
              "      <td>0.196</td>\n",
              "      <td>2.349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_energy</th>\n",
              "      <td>-0.085</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.198</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.623</td>\n",
              "      <td>1.355</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.426</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_energy</th>\n",
              "      <td>-0.046</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.219</td>\n",
              "      <td>-0.475</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.622</td>\n",
              "      <td>1.466</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>0.833</td>\n",
              "      <td>0.263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>energy_ratio</th>\n",
              "      <td>0.142</td>\n",
              "      <td>1.153</td>\n",
              "      <td>0.202</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>0.538</td>\n",
              "      <td>0.776</td>\n",
              "      <td>1.713</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.481</td>\n",
              "      <td>1.055</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Concordance</th>\n",
              "      <td>0.766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Partial AIC</th>\n",
              "      <td>103.771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>log-likelihood ratio test</th>\n",
              "      <td>8.909 on 17 df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-log2(p) of ll-ratio test</th>\n",
              "      <td>0.085</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/latex": "\\begin{tabular}{lrrrrrrrrrrr}\n & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\ncovariate &  &  &  &  &  &  &  &  &  &  &  \\\\\ntorsional_mean & 0.088 & 1.092 & 0.173 & -0.250 & 0.426 & 0.779 & 1.531 & 0.000 & 0.510 & 0.610 & 0.713 \\\\\ntorsional_std & -0.036 & 0.965 & 0.208 & -0.444 & 0.372 & 0.641 & 1.451 & 0.000 & -0.173 & 0.863 & 0.213 \\\\\ntorsional_max & 0.110 & 1.116 & 0.214 & -0.310 & 0.530 & 0.733 & 1.699 & 0.000 & 0.512 & 0.609 & 0.716 \\\\\ntorsional_min & -0.084 & 0.920 & 0.214 & -0.503 & 0.336 & 0.605 & 1.399 & 0.000 & -0.391 & 0.696 & 0.523 \\\\\ntorsional_peak_to_peak & 0.097 & 1.102 & 0.214 & -0.323 & 0.517 & 0.724 & 1.677 & 0.000 & 0.452 & 0.651 & 0.619 \\\\\ntorsional_rms & -0.036 & 0.965 & 0.208 & -0.444 & 0.372 & 0.641 & 1.451 & 0.000 & -0.173 & 0.863 & 0.213 \\\\\ntorsional_kurtosis & 0.184 & 1.202 & 0.204 & -0.215 & 0.583 & 0.807 & 1.792 & 0.000 & 0.904 & 0.366 & 1.450 \\\\\nflexural_mean & 0.053 & 1.055 & 0.150 & -0.241 & 0.348 & 0.786 & 1.417 & 0.000 & 0.356 & 0.722 & 0.470 \\\\\nflexural_std & -0.079 & 0.924 & 0.207 & -0.485 & 0.328 & 0.616 & 1.388 & 0.000 & -0.379 & 0.705 & 0.505 \\\\\nflexural_max & 0.009 & 1.009 & 0.213 & -0.408 & 0.425 & 0.665 & 1.530 & 0.000 & 0.042 & 0.966 & 0.049 \\\\\nflexural_min & -0.027 & 0.973 & 0.213 & -0.446 & 0.391 & 0.640 & 1.478 & 0.000 & -0.128 & 0.898 & 0.155 \\\\\nflexural_peak_to_peak & 0.018 & 1.018 & 0.213 & -0.400 & 0.435 & 0.671 & 1.545 & 0.000 & 0.084 & 0.933 & 0.100 \\\\\nflexural_rms & -0.079 & 0.924 & 0.207 & -0.485 & 0.328 & 0.616 & 1.388 & 0.000 & -0.379 & 0.705 & 0.505 \\\\\nflexural_kurtosis & 0.240 & 1.272 & 0.186 & -0.124 & 0.605 & 0.883 & 1.831 & 0.000 & 1.292 & 0.196 & 2.349 \\\\\ntorsional_energy & -0.085 & 0.919 & 0.198 & -0.473 & 0.304 & 0.623 & 1.355 & 0.000 & -0.426 & 0.670 & 0.578 \\\\\nflexural_energy & -0.046 & 0.955 & 0.219 & -0.475 & 0.383 & 0.622 & 1.466 & 0.000 & -0.211 & 0.833 & 0.263 \\\\\nenergy_ratio & 0.142 & 1.153 & 0.202 & -0.254 & 0.538 & 0.776 & 1.713 & 0.000 & 0.704 & 0.481 & 1.055 \\\\\n\\end{tabular}\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training CoxPH model for Frequency_5...\n",
            " -> CoxPH model trained for Frequency_5.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<lifelines.CoxPHFitter: fitted with 200 total observations, 181 right-censored observations>\n",
              "             duration col = 'duration'\n",
              "                event col = 'event'\n",
              "                penalizer = 0.1\n",
              "                 l1 ratio = 0.0\n",
              "      baseline estimation = breslow\n",
              "   number of observations = 200\n",
              "number of events observed = 19\n",
              "   partial log-likelihood = -35.360\n",
              "         time fit was run = 2025-04-02 23:03:48 UTC\n",
              "\n",
              "---\n",
              "                         coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
              "covariate                                                                                                                \n",
              "torsional_mean          0.162     1.175     0.211          -0.251           0.574               0.778               1.776\n",
              "torsional_std          -0.050     0.951     0.213          -0.468           0.367               0.626               1.443\n",
              "torsional_max           0.020     1.020     0.211          -0.394           0.434               0.674               1.543\n",
              "torsional_min          -0.041     0.960     0.212          -0.457           0.375               0.633               1.455\n",
              "torsional_peak_to_peak  0.030     1.031     0.212          -0.385           0.445               0.681               1.561\n",
              "torsional_rms          -0.050     0.951     0.213          -0.468           0.367               0.626               1.443\n",
              "torsional_kurtosis      0.168     1.183     0.212          -0.249           0.584               0.780               1.793\n",
              "flexural_mean          -0.027     0.973     0.185          -0.390           0.336               0.677               1.399\n",
              "flexural_std           -0.145     0.865     0.212          -0.561           0.272               0.570               1.312\n",
              "flexural_max           -0.057     0.944     0.208          -0.466           0.351               0.628               1.421\n",
              "flexural_min            0.068     1.070     0.206          -0.337           0.472               0.714               1.603\n",
              "flexural_peak_to_peak  -0.062     0.939     0.207          -0.469           0.344               0.626               1.411\n",
              "flexural_rms           -0.145     0.865     0.212          -0.561           0.272               0.570               1.312\n",
              "flexural_kurtosis       0.148     1.159     0.195          -0.233           0.529               0.792               1.698\n",
              "torsional_energy       -0.057     0.944     0.208          -0.465           0.351               0.628               1.420\n",
              "flexural_energy        -0.103     0.902     0.215          -0.524           0.319               0.592               1.376\n",
              "energy_ratio            0.113     1.119     0.205          -0.289           0.514               0.749               1.673\n",
              "\n",
              "                        cmp to      z     p  -log2(p)\n",
              "covariate                                            \n",
              "torsional_mean           0.000  0.767 0.443     1.174\n",
              "torsional_std            0.000 -0.237 0.813     0.299\n",
              "torsional_max            0.000  0.094 0.925     0.112\n",
              "torsional_min            0.000 -0.193 0.847     0.240\n",
              "torsional_peak_to_peak   0.000  0.143 0.886     0.174\n",
              "torsional_rms            0.000 -0.237 0.813     0.299\n",
              "torsional_kurtosis       0.000  0.790 0.430     1.219\n",
              "flexural_mean            0.000 -0.145 0.884     0.177\n",
              "flexural_std             0.000 -0.682 0.495     1.014\n",
              "flexural_max             0.000 -0.275 0.783     0.352\n",
              "flexural_min             0.000  0.327 0.743     0.428\n",
              "flexural_peak_to_peak    0.000 -0.301 0.763     0.389\n",
              "flexural_rms             0.000 -0.682 0.495     1.014\n",
              "flexural_kurtosis        0.000  0.760 0.447     1.161\n",
              "torsional_energy         0.000 -0.275 0.783     0.352\n",
              "flexural_energy          0.000 -0.477 0.633     0.659\n",
              "energy_ratio             0.000  0.549 0.583     0.779\n",
              "---\n",
              "Concordance = 0.772\n",
              "Partial AIC = 104.719\n",
              "log-likelihood ratio test = 7.961 on 17 df\n",
              "-log2(p) of ll-ratio test = 0.048"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>lifelines.CoxPHFitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration col</th>\n",
              "      <td>'duration'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>event col</th>\n",
              "      <td>'event'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>penalizer</th>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>l1 ratio</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baseline estimation</th>\n",
              "      <td>breslow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of observations</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number of events observed</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>partial log-likelihood</th>\n",
              "      <td>-35.360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time fit was run</th>\n",
              "      <td>2025-04-02 23:03:48 UTC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th style=\"min-width: 12px;\"></th>\n",
              "      <th style=\"min-width: 12px;\">coef</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
              "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
              "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
              "      <th style=\"min-width: 12px;\">cmp to</th>\n",
              "      <th style=\"min-width: 12px;\">z</th>\n",
              "      <th style=\"min-width: 12px;\">p</th>\n",
              "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>torsional_mean</th>\n",
              "      <td>0.162</td>\n",
              "      <td>1.175</td>\n",
              "      <td>0.211</td>\n",
              "      <td>-0.251</td>\n",
              "      <td>0.574</td>\n",
              "      <td>0.778</td>\n",
              "      <td>1.776</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.767</td>\n",
              "      <td>0.443</td>\n",
              "      <td>1.174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_std</th>\n",
              "      <td>-0.050</td>\n",
              "      <td>0.951</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-0.468</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.626</td>\n",
              "      <td>1.443</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_max</th>\n",
              "      <td>0.020</td>\n",
              "      <td>1.020</td>\n",
              "      <td>0.211</td>\n",
              "      <td>-0.394</td>\n",
              "      <td>0.434</td>\n",
              "      <td>0.674</td>\n",
              "      <td>1.543</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.925</td>\n",
              "      <td>0.112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_min</th>\n",
              "      <td>-0.041</td>\n",
              "      <td>0.960</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.457</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.633</td>\n",
              "      <td>1.455</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.193</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_peak_to_peak</th>\n",
              "      <td>0.030</td>\n",
              "      <td>1.031</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.385</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.681</td>\n",
              "      <td>1.561</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.886</td>\n",
              "      <td>0.174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_rms</th>\n",
              "      <td>-0.050</td>\n",
              "      <td>0.951</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-0.468</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.626</td>\n",
              "      <td>1.443</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_kurtosis</th>\n",
              "      <td>0.168</td>\n",
              "      <td>1.183</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.249</td>\n",
              "      <td>0.584</td>\n",
              "      <td>0.780</td>\n",
              "      <td>1.793</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.430</td>\n",
              "      <td>1.219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_mean</th>\n",
              "      <td>-0.027</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.185</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.677</td>\n",
              "      <td>1.399</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.145</td>\n",
              "      <td>0.884</td>\n",
              "      <td>0.177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_std</th>\n",
              "      <td>-0.145</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.561</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.570</td>\n",
              "      <td>1.312</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.682</td>\n",
              "      <td>0.495</td>\n",
              "      <td>1.014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_max</th>\n",
              "      <td>-0.057</td>\n",
              "      <td>0.944</td>\n",
              "      <td>0.208</td>\n",
              "      <td>-0.466</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.628</td>\n",
              "      <td>1.421</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>0.783</td>\n",
              "      <td>0.352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_min</th>\n",
              "      <td>0.068</td>\n",
              "      <td>1.070</td>\n",
              "      <td>0.206</td>\n",
              "      <td>-0.337</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.714</td>\n",
              "      <td>1.603</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.327</td>\n",
              "      <td>0.743</td>\n",
              "      <td>0.428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_peak_to_peak</th>\n",
              "      <td>-0.062</td>\n",
              "      <td>0.939</td>\n",
              "      <td>0.207</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>0.344</td>\n",
              "      <td>0.626</td>\n",
              "      <td>1.411</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.301</td>\n",
              "      <td>0.763</td>\n",
              "      <td>0.389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_rms</th>\n",
              "      <td>-0.145</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.212</td>\n",
              "      <td>-0.561</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.570</td>\n",
              "      <td>1.312</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.682</td>\n",
              "      <td>0.495</td>\n",
              "      <td>1.014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_kurtosis</th>\n",
              "      <td>0.148</td>\n",
              "      <td>1.159</td>\n",
              "      <td>0.195</td>\n",
              "      <td>-0.233</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.792</td>\n",
              "      <td>1.698</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.447</td>\n",
              "      <td>1.161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>torsional_energy</th>\n",
              "      <td>-0.057</td>\n",
              "      <td>0.944</td>\n",
              "      <td>0.208</td>\n",
              "      <td>-0.465</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.628</td>\n",
              "      <td>1.420</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>0.783</td>\n",
              "      <td>0.352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flexural_energy</th>\n",
              "      <td>-0.103</td>\n",
              "      <td>0.902</td>\n",
              "      <td>0.215</td>\n",
              "      <td>-0.524</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.592</td>\n",
              "      <td>1.376</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.477</td>\n",
              "      <td>0.633</td>\n",
              "      <td>0.659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>energy_ratio</th>\n",
              "      <td>0.113</td>\n",
              "      <td>1.119</td>\n",
              "      <td>0.205</td>\n",
              "      <td>-0.289</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.749</td>\n",
              "      <td>1.673</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.549</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.779</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Concordance</th>\n",
              "      <td>0.772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Partial AIC</th>\n",
              "      <td>104.719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>log-likelihood ratio test</th>\n",
              "      <td>7.961 on 17 df</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-log2(p) of ll-ratio test</th>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/latex": "\\begin{tabular}{lrrrrrrrrrrr}\n & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\ncovariate &  &  &  &  &  &  &  &  &  &  &  \\\\\ntorsional_mean & 0.162 & 1.175 & 0.211 & -0.251 & 0.574 & 0.778 & 1.776 & 0.000 & 0.767 & 0.443 & 1.174 \\\\\ntorsional_std & -0.050 & 0.951 & 0.213 & -0.468 & 0.367 & 0.626 & 1.443 & 0.000 & -0.237 & 0.813 & 0.299 \\\\\ntorsional_max & 0.020 & 1.020 & 0.211 & -0.394 & 0.434 & 0.674 & 1.543 & 0.000 & 0.094 & 0.925 & 0.112 \\\\\ntorsional_min & -0.041 & 0.960 & 0.212 & -0.457 & 0.375 & 0.633 & 1.455 & 0.000 & -0.193 & 0.847 & 0.240 \\\\\ntorsional_peak_to_peak & 0.030 & 1.031 & 0.212 & -0.385 & 0.445 & 0.681 & 1.561 & 0.000 & 0.143 & 0.886 & 0.174 \\\\\ntorsional_rms & -0.050 & 0.951 & 0.213 & -0.468 & 0.367 & 0.626 & 1.443 & 0.000 & -0.237 & 0.813 & 0.299 \\\\\ntorsional_kurtosis & 0.168 & 1.183 & 0.212 & -0.249 & 0.584 & 0.780 & 1.793 & 0.000 & 0.790 & 0.430 & 1.219 \\\\\nflexural_mean & -0.027 & 0.973 & 0.185 & -0.390 & 0.336 & 0.677 & 1.399 & 0.000 & -0.145 & 0.884 & 0.177 \\\\\nflexural_std & -0.145 & 0.865 & 0.212 & -0.561 & 0.272 & 0.570 & 1.312 & 0.000 & -0.682 & 0.495 & 1.014 \\\\\nflexural_max & -0.057 & 0.944 & 0.208 & -0.466 & 0.351 & 0.628 & 1.421 & 0.000 & -0.275 & 0.783 & 0.352 \\\\\nflexural_min & 0.068 & 1.070 & 0.206 & -0.337 & 0.472 & 0.714 & 1.603 & 0.000 & 0.327 & 0.743 & 0.428 \\\\\nflexural_peak_to_peak & -0.062 & 0.939 & 0.207 & -0.469 & 0.344 & 0.626 & 1.411 & 0.000 & -0.301 & 0.763 & 0.389 \\\\\nflexural_rms & -0.145 & 0.865 & 0.212 & -0.561 & 0.272 & 0.570 & 1.312 & 0.000 & -0.682 & 0.495 & 1.014 \\\\\nflexural_kurtosis & 0.148 & 1.159 & 0.195 & -0.233 & 0.529 & 0.792 & 1.698 & 0.000 & 0.760 & 0.447 & 1.161 \\\\\ntorsional_energy & -0.057 & 0.944 & 0.208 & -0.465 & 0.351 & 0.628 & 1.420 & 0.000 & -0.275 & 0.783 & 0.352 \\\\\nflexural_energy & -0.103 & 0.902 & 0.215 & -0.524 & 0.319 & 0.592 & 1.376 & 0.000 & -0.477 & 0.633 & 0.659 \\\\\nenergy_ratio & 0.113 & 1.119 & 0.205 & -0.289 & 0.514 & 0.749 & 1.673 & 0.000 & 0.549 & 0.583 & 0.779 \\\\\n\\end{tabular}\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Survival Analysis model building completed.\n",
            "\n",
            "--- Saving Models and Scalers ---\n",
            "Scaler 'Frequency_1_features' saved.\n",
            "Scaler 'Frequency_1_windows' saved.\n",
            "Scaler 'Frequency_2_features' saved.\n",
            "Scaler 'Frequency_2_windows' saved.\n",
            "Scaler 'Frequency_3_features' saved.\n",
            "Scaler 'Frequency_3_windows' saved.\n",
            "Scaler 'Frequency_4_features' saved.\n",
            "Scaler 'Frequency_4_windows' saved.\n",
            "Scaler 'Frequency_5_features' saved.\n",
            "Scaler 'Frequency_5_windows' saved.\n",
            "Model 'Frequency_1_isolation_forest' saved as joblib.\n",
            "Model 'Frequency_1_ocsvm' saved as joblib.\n",
            "Model 'Frequency_1_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_1_lstm_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_2_isolation_forest' saved as joblib.\n",
            "Model 'Frequency_2_ocsvm' saved as joblib.\n",
            "Model 'Frequency_2_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_2_lstm_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_3_isolation_forest' saved as joblib.\n",
            "Model 'Frequency_3_ocsvm' saved as joblib.\n",
            "Model 'Frequency_3_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_3_lstm_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_4_isolation_forest' saved as joblib.\n",
            "Model 'Frequency_4_ocsvm' saved as joblib.\n",
            "Model 'Frequency_4_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_4_lstm_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_5_isolation_forest' saved as joblib.\n",
            "Model 'Frequency_5_ocsvm' saved as joblib.\n",
            "Model 'Frequency_5_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_5_lstm_autoencoder' saved as Keras format.\n",
            "Model 'Frequency_1_random_forest' saved as joblib.\n",
            "Model 'Frequency_1_xgboost' saved as joblib.\n",
            "Model 'Frequency_1_neural_network' saved as Keras format.\n",
            "Model 'Frequency_2_random_forest' saved as joblib.\n",
            "Model 'Frequency_2_xgboost' saved as joblib.\n",
            "Model 'Frequency_2_neural_network' saved as Keras format.\n",
            "Model 'Frequency_3_random_forest' saved as joblib.\n",
            "Model 'Frequency_3_xgboost' saved as joblib.\n",
            "Model 'Frequency_3_neural_network' saved as Keras format.\n",
            "Model 'Frequency_4_random_forest' saved as joblib.\n",
            "Model 'Frequency_4_xgboost' saved as joblib.\n",
            "Model 'Frequency_4_neural_network' saved as Keras format.\n",
            "Model 'Frequency_5_random_forest' saved as joblib.\n",
            "Model 'Frequency_5_xgboost' saved as joblib.\n",
            "Model 'Frequency_5_neural_network' saved as Keras format.\n",
            "Model 'Frequency_1_random_forest' saved as joblib.\n",
            "Model 'Frequency_1_xgboost' saved as joblib.\n",
            "Model 'Frequency_1_lstm' saved as Keras format.\n",
            "Model 'Frequency_2_random_forest' saved as joblib.\n",
            "Model 'Frequency_2_xgboost' saved as joblib.\n",
            "Model 'Frequency_2_lstm' saved as Keras format.\n",
            "Model 'Frequency_3_random_forest' saved as joblib.\n",
            "Model 'Frequency_3_xgboost' saved as joblib.\n",
            "Model 'Frequency_3_lstm' saved as Keras format.\n",
            "Model 'Frequency_4_random_forest' saved as joblib.\n",
            "Model 'Frequency_4_xgboost' saved as joblib.\n",
            "Model 'Frequency_4_lstm' saved as Keras format.\n",
            "Model 'Frequency_5_random_forest' saved as joblib.\n",
            "Model 'Frequency_5_xgboost' saved as joblib.\n",
            "Model 'Frequency_5_lstm' saved as Keras format.\n",
            "Model 'Frequency_1_coxph' saved as joblib.\n",
            "Model 'Frequency_2_coxph' saved as joblib.\n",
            "Model 'Frequency_3_coxph' saved as joblib.\n",
            "Model 'Frequency_4_coxph' saved as joblib.\n",
            "Model 'Frequency_5_coxph' saved as joblib.\n",
            "Model and scaler saving process finished.\n",
            "\n",
            "--- Running Evaluation ---\n",
            "\n",
            "--- Evaluating Anomaly Detection Models ---\n",
            "\n",
            "Evaluating anomaly detectors for Frequency_1...\n",
            "Evaluating Isolation Forest...\n",
            " -> IF Accuracy: 0.7258, AUC: 0.6042\n",
            "Evaluating One-Class SVM...\n",
            " -> OCSVM Accuracy: 0.7903, AUC: 0.5893\n",
            "Evaluating Dense Autoencoder...\n",
            "Determining threshold using Functional on data shape (43, 17)\n",
            " -> Dense AE Accuracy: 0.8871, AUC: 0.4524 (Thresh: 2.2661)\n",
            "Evaluating LSTM Autoencoder...\n",
            "Determining threshold using Sequential on data shape (39, 5, 17)\n",
            " -> LSTM AE Accuracy: 0.8966, AUC: 0.5673 (Thresh: 1.5758)\n",
            "\n",
            "Evaluating anomaly detectors for Frequency_2...\n",
            "Evaluating Isolation Forest...\n",
            " -> IF Accuracy: 0.7581, AUC: 0.6964\n",
            "Evaluating One-Class SVM...\n",
            " -> OCSVM Accuracy: 0.6935, AUC: 0.6548\n",
            "Evaluating Dense Autoencoder...\n",
            "Determining threshold using Functional on data shape (43, 17)\n",
            " -> Dense AE Accuracy: 0.9032, AUC: 0.7470 (Thresh: 0.9121)\n",
            "Evaluating LSTM Autoencoder...\n",
            "Determining threshold using Sequential on data shape (39, 5, 17)\n",
            " -> LSTM AE Accuracy: 0.8966, AUC: 0.2917 (Thresh: 2.6975)\n",
            "\n",
            "Evaluating anomaly detectors for Frequency_3...\n",
            "Evaluating Isolation Forest...\n",
            " -> IF Accuracy: 0.7258, AUC: 0.5000\n",
            "Evaluating One-Class SVM...\n",
            " -> OCSVM Accuracy: 0.7419, AUC: 0.4435\n",
            "Evaluating Dense Autoencoder...\n",
            "Determining threshold using Functional on data shape (43, 17)\n",
            " -> Dense AE Accuracy: 0.9032, AUC: 0.3393 (Thresh: 10.1509)\n",
            "Evaluating LSTM Autoencoder...\n",
            "Determining threshold using Sequential on data shape (39, 5, 17)\n",
            " -> LSTM AE Accuracy: 0.8966, AUC: 0.4423 (Thresh: 3.6559)\n",
            "\n",
            "Evaluating anomaly detectors for Frequency_4...\n",
            "Evaluating Isolation Forest...\n",
            " -> IF Accuracy: 0.8226, AUC: 0.7292\n",
            "Evaluating One-Class SVM...\n",
            " -> OCSVM Accuracy: 0.8548, AUC: 0.7619\n",
            "Evaluating Dense Autoencoder...\n",
            "Determining threshold using Functional on data shape (43, 17)\n",
            " -> Dense AE Accuracy: 0.8387, AUC: 0.7054 (Thresh: 1.5233)\n",
            "Evaluating LSTM Autoencoder...\n",
            "Determining threshold using Sequential on data shape (39, 5, 17)\n",
            " -> LSTM AE Accuracy: 0.7414, AUC: 0.5737 (Thresh: 1.4344)\n",
            "\n",
            "Evaluating anomaly detectors for Frequency_5...\n",
            "Evaluating Isolation Forest...\n",
            " -> IF Accuracy: 0.7419, AUC: 0.5625\n",
            "Evaluating One-Class SVM...\n",
            " -> OCSVM Accuracy: 0.8710, AUC: 0.4881\n",
            "Evaluating Dense Autoencoder...\n",
            "Determining threshold using Functional on data shape (43, 17)\n",
            " -> Dense AE Accuracy: 0.8871, AUC: 0.3274 (Thresh: 2.1930)\n",
            "Evaluating LSTM Autoencoder...\n",
            "Determining threshold using Sequential on data shape (39, 5, 17)\n",
            " -> LSTM AE Accuracy: 0.8793, AUC: 0.5897 (Thresh: 1.6640)\n",
            "\n",
            "Anomaly detection evaluation completed.\n",
            "\n",
            "--- Evaluating Crack Classification Models ---\n",
            "\n",
            "Evaluating crack classifiers for Frequency_1...\n",
            "Test label distribution: [56  2  2  2]\n",
            "Evaluating Random Forest Classifier...\n",
            " -> RF Test Accuracy: 0.9194\n",
            "Evaluating XGBoost Classifier...\n",
            " -> XGB Test Accuracy: 0.9355\n",
            "Evaluating Neural Network Classifier...\n",
            " -> NN Test Accuracy: 0.8387\n",
            "\n",
            "Evaluating crack classifiers for Frequency_2...\n",
            "Test label distribution: [56  2  2  2]\n",
            "Evaluating Random Forest Classifier...\n",
            " -> RF Test Accuracy: 0.9355\n",
            "Evaluating XGBoost Classifier...\n",
            " -> XGB Test Accuracy: 0.9355\n",
            "Evaluating Neural Network Classifier...\n",
            " -> NN Test Accuracy: 0.9032\n",
            "\n",
            "Evaluating crack classifiers for Frequency_3...\n",
            "Test label distribution: [56  2  2  2]\n",
            "Evaluating Random Forest Classifier...\n",
            " -> RF Test Accuracy: 0.9032\n",
            "Evaluating XGBoost Classifier...\n",
            " -> XGB Test Accuracy: 0.9194\n",
            "Evaluating Neural Network Classifier...\n",
            " -> NN Test Accuracy: 0.9032\n",
            "\n",
            "Evaluating crack classifiers for Frequency_4...\n",
            "Test label distribution: [56  2  2  2]\n",
            "Evaluating Random Forest Classifier...\n",
            " -> RF Test Accuracy: 0.9032\n",
            "Evaluating XGBoost Classifier...\n",
            " -> XGB Test Accuracy: 0.8871\n",
            "Evaluating Neural Network Classifier...\n",
            " -> NN Test Accuracy: 0.9032\n",
            "\n",
            "Evaluating crack classifiers for Frequency_5...\n",
            "Test label distribution: [56  2  2  2]\n",
            "Evaluating Random Forest Classifier...\n",
            " -> RF Test Accuracy: 0.9194\n",
            "Evaluating XGBoost Classifier...\n",
            " -> XGB Test Accuracy: 0.9355\n",
            "Evaluating Neural Network Classifier...\n",
            " -> NN Test Accuracy: 0.9032\n",
            "\n",
            "Crack classification evaluation completed.\n",
            "\n",
            "--- Evaluating Lifetime Prediction Models (RUL Regression) ---\n",
            "\n",
            "Evaluating RUL predictors for Frequency_1...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Evaluating Random Forest Regressor...\n",
            " -> RF Regressor Test MAE: 26.20, R2: 0.2236\n",
            "Evaluating XGBoost Regressor...\n",
            " -> XGB Regressor Test MAE: 30.17, R2: 0.0852\n",
            "Evaluating LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Test MAE: 90.53, R2: -2.3774\n",
            "\n",
            "Evaluating RUL predictors for Frequency_2...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Evaluating Random Forest Regressor...\n",
            " -> RF Regressor Test MAE: 20.02, R2: 0.6252\n",
            "Evaluating XGBoost Regressor...\n",
            " -> XGB Regressor Test MAE: 17.00, R2: 0.6333\n",
            "Evaluating LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Test MAE: 87.96, R2: -2.0006\n",
            "\n",
            "Evaluating RUL predictors for Frequency_3...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Evaluating Random Forest Regressor...\n",
            " -> RF Regressor Test MAE: 20.71, R2: 0.5775\n",
            "Evaluating XGBoost Regressor...\n",
            " -> XGB Regressor Test MAE: 23.27, R2: 0.5898\n",
            "Evaluating LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Test MAE: 102.97, R2: -3.0541\n",
            "\n",
            "Evaluating RUL predictors for Frequency_4...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Evaluating Random Forest Regressor...\n",
            " -> RF Regressor Test MAE: 22.78, R2: 0.4358\n",
            "Evaluating XGBoost Regressor...\n",
            " -> XGB Regressor Test MAE: 21.10, R2: 0.5638\n",
            "Evaluating LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Test MAE: 70.17, R2: -1.0171\n",
            "\n",
            "Evaluating RUL predictors for Frequency_5...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            "Evaluating Random Forest Regressor...\n",
            " -> RF Regressor Test MAE: 23.79, R2: 0.4747\n",
            "Evaluating XGBoost Regressor...\n",
            " -> XGB Regressor Test MAE: 21.14, R2: 0.6402\n",
            "Evaluating LSTM Regressor...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> LSTM Regressor Test MAE: 83.44, R2: -1.4004\n",
            "\n",
            "Lifetime prediction evaluation completed (RUL Regression).\n",
            "\n",
            "--- Evaluating Survival Analysis Model (CoxPH) ---\n",
            "\n",
            "Evaluating CoxPH model for Frequency_1...\n",
            " -> CoxPH Concordance Index for Frequency_1: 0.6000\n",
            "\n",
            "Evaluating CoxPH model for Frequency_2...\n",
            " -> CoxPH Concordance Index for Frequency_2: 0.8667\n",
            "\n",
            "Evaluating CoxPH model for Frequency_3...\n",
            " -> CoxPH Concordance Index for Frequency_3: 0.3333\n",
            "\n",
            "Evaluating CoxPH model for Frequency_4...\n",
            " -> CoxPH Concordance Index for Frequency_4: 0.6000\n",
            "\n",
            "Evaluating CoxPH model for Frequency_5...\n",
            " -> CoxPH Concordance Index for Frequency_5: 0.7333\n",
            "\n",
            "Survival Analysis model evaluation completed.\n",
            "\n",
            "--- Evaluation Summary ---\n",
            "\n",
            "--- Generating All Visualizations ---\n",
            "\n",
            "--- Creating Visualizations ---\n",
            "\n",
            "Generating standard plots for Frequency_1...\n",
            " -> Saved feature dist plot for Frequency_1.\n",
            " -> Saved class dist plot for Frequency_1.\n",
            "\n",
            "Generating standard plots for Frequency_2...\n",
            " -> Saved feature dist plot for Frequency_2.\n",
            " -> Saved class dist plot for Frequency_2.\n",
            "\n",
            "Generating standard plots for Frequency_3...\n",
            " -> Saved feature dist plot for Frequency_3.\n",
            " -> Saved class dist plot for Frequency_3.\n",
            "\n",
            "Generating standard plots for Frequency_4...\n",
            " -> Saved feature dist plot for Frequency_4.\n",
            " -> Saved class dist plot for Frequency_4.\n",
            "\n",
            "Generating standard plots for Frequency_5...\n",
            " -> Saved feature dist plot for Frequency_5.\n",
            " -> Saved class dist plot for Frequency_5.\n",
            "Standard visualization generation finished.\n",
            "\n",
            "--- Generating Feature Importance Plots ---\n",
            "Plotting feature importance for Frequency_1...\n",
            " -> Saved feature importance plot for Frequency_1.\n",
            "Plotting feature importance for Frequency_2...\n",
            " -> Saved feature importance plot for Frequency_2.\n",
            "Plotting feature importance for Frequency_3...\n",
            " -> Saved feature importance plot for Frequency_3.\n",
            "Plotting feature importance for Frequency_4...\n",
            " -> Saved feature importance plot for Frequency_4.\n",
            "Plotting feature importance for Frequency_5...\n",
            " -> Saved feature importance plot for Frequency_5.\n",
            "\n",
            "--- Generating Anomaly Detection Plots ---\n",
            "Plotting anomalies for Frequency_1...\n",
            " -> Skipping IF anomaly plot for Frequency_1: Predictions not found.\n",
            " -> Saved AE error plot for Frequency_1.\n",
            "Plotting anomalies for Frequency_2...\n",
            " -> Skipping IF anomaly plot for Frequency_2: Predictions not found.\n",
            " -> Saved AE error plot for Frequency_2.\n",
            "Plotting anomalies for Frequency_3...\n",
            " -> Skipping IF anomaly plot for Frequency_3: Predictions not found.\n",
            " -> Saved AE error plot for Frequency_3.\n",
            "Plotting anomalies for Frequency_4...\n",
            " -> Skipping IF anomaly plot for Frequency_4: Predictions not found.\n",
            " -> Saved AE error plot for Frequency_4.\n",
            "Plotting anomalies for Frequency_5...\n",
            " -> Skipping IF anomaly plot for Frequency_5: Predictions not found.\n",
            " -> Saved AE error plot for Frequency_5.\n",
            "\n",
            "--- Generating Classification Result Plots ---\n",
            "Plotting classification results for Frequency_1...\n",
            " -> Saved CM plot (Random Forest) for Frequency_1.\n",
            " -> Saved CM plot (XGBoost) for Frequency_1.\n",
            " -> Saved CM plot (Neural Network) for Frequency_1.\n",
            "Plotting classification results for Frequency_2...\n",
            " -> Saved CM plot (Random Forest) for Frequency_2.\n",
            " -> Saved CM plot (XGBoost) for Frequency_2.\n",
            " -> Saved CM plot (Neural Network) for Frequency_2.\n",
            "Plotting classification results for Frequency_3...\n",
            " -> Saved CM plot (Random Forest) for Frequency_3.\n",
            " -> Saved CM plot (XGBoost) for Frequency_3.\n",
            " -> Saved CM plot (Neural Network) for Frequency_3.\n",
            "Plotting classification results for Frequency_4...\n",
            " -> Saved CM plot (Random Forest) for Frequency_4.\n",
            " -> Saved CM plot (XGBoost) for Frequency_4.\n",
            " -> Saved CM plot (Neural Network) for Frequency_4.\n",
            "Plotting classification results for Frequency_5...\n",
            " -> Saved CM plot (Random Forest) for Frequency_5.\n",
            " -> Saved CM plot (XGBoost) for Frequency_5.\n",
            " -> Saved CM plot (Neural Network) for Frequency_5.\n",
            "\n",
            "--- Generating RUL Prediction Plots ---\n",
            "Plotting RUL predictions for Frequency_1...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> Warning: RUL prediction length mismatch for LSTM - Frequency_1.\n",
            " -> Saved RUL prediction plot for Frequency_1.\n",
            "Plotting RUL predictions for Frequency_2...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> Warning: RUL prediction length mismatch for LSTM - Frequency_2.\n",
            " -> Saved RUL prediction plot for Frequency_2.\n",
            "Plotting RUL predictions for Frequency_3...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> Warning: RUL prediction length mismatch for LSTM - Frequency_3.\n",
            " -> Saved RUL prediction plot for Frequency_3.\n",
            "Plotting RUL predictions for Frequency_4...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> Warning: RUL prediction length mismatch for LSTM - Frequency_4.\n",
            " -> Saved RUL prediction plot for Frequency_4.\n",
            "Plotting RUL predictions for Frequency_5...\n",
            "Generating synthetic RUL (higher category -> lower RUL)...\n",
            " -> Warning: RUL prediction length mismatch for LSTM - Frequency_5.\n",
            " -> Saved RUL prediction plot for Frequency_5.\n",
            "\n",
            "--- Creating Survival Analysis Visualizations ---\n",
            "Generating survival plots for Frequency_1...\n",
            " -> Saved CoxPH coefficients plot for Frequency_1.\n",
            " -> No significant features (p<0.05) for partial effects plot for Frequency_1.\n",
            "Generating survival plots for Frequency_2...\n",
            " -> Saved CoxPH coefficients plot for Frequency_2.\n",
            " -> No significant features (p<0.05) for partial effects plot for Frequency_2.\n",
            "Generating survival plots for Frequency_3...\n",
            " -> Saved CoxPH coefficients plot for Frequency_3.\n",
            " -> No significant features (p<0.05) for partial effects plot for Frequency_3.\n",
            "Generating survival plots for Frequency_4...\n",
            " -> Saved CoxPH coefficients plot for Frequency_4.\n",
            " -> No significant features (p<0.05) for partial effects plot for Frequency_4.\n",
            "Generating survival plots for Frequency_5...\n",
            " -> Saved CoxPH coefficients plot for Frequency_5.\n",
            " -> No significant features (p<0.05) for partial effects plot for Frequency_5.\n",
            "Survival analysis visualization generation finished.\n",
            "\n",
            "--- Pipeline Run Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QF5WIg7geAns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}